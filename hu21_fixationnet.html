<!DOCTYPE html>
<html lang="en">
    <meta http-equiv="content-type" content="text/html;charset=utf-8" />
    <head>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
        <title>FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments</title>
        <link rel="stylesheet" media="all" href="./index/css/main_v2.css" />
        <link rel="stylesheet" media="all" href="./hu21_fixationnet/css/owl.carousel.min.css" />
        <link rel="stylesheet" media="all" href="./hu21_fixationnet/css/owl.theme.default.min.css" />
        <link rel="stylesheet" media="all" href="./hu21_fixationnet/css/jquery-ui.min.css" />
        <script src="./hu21_fixationnet/js/jquery-3.4.1.min.js"></script>
        <script src="./hu21_fixationnet/js/jquery-ui.min.js"></script>
        <script src="./hu21_fixationnet/js/fontawesome-5.11.2.js"></script>
        <script src="./hu21_fixationnet/js/main.js"></script>
        <script src="./hu21_fixationnet/js/owl.carousel.min.js"></script>
        <script src="./hu21_fixationnet/js/rot13.js"></script>
		<script src="https://www.w3counter.com/tracker.js?id=150008"></script>
    </head>
    <body class="header header-location">
        <div class="wrapper">
            <!-- header -->
            <div id="unstickyheader" class="unstickyheader">
                
            </div>
        </div>
        

        <!-- content -->
        <div class="content">
        <div class="section margin-top-20 margin-bottom-40">
    <div class="container">

        <h3>FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments</h3>

        <h4>
        
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>,
            Sheng Li, Guoping Wang 
        </h4>

        <h4>
			<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics (TVCG, IEEE VR 2021 Journal-track), 2021, 27(5): 2681-2690.
			</span>
        </h4>        
            <h4><span class="pub_award"><i class="fa fa-award"></i> Best Journal Paper Nominees</span></h4>                                        

		<hr>
		<img src="./hu21_fixationnet/image/teaser.png" style="height:auto; width:1024px;" class="centerContent"><br>
		<i class="centerContent"></i>            
        <hr>
            
        <h4>Abstract</h4>

        Human visual attention in immersive virtual reality (VR) is key for many important applications, such as content design, gaze-contingent rendering, or gaze-based interaction. However, prior works typically focused on free-viewing conditions that have limited relevance for practical applications. We first collect eye tracking data of 27 participants performing a visual search task in four immersive VR environments. Based on this dataset, we provide a comprehensive analysis of the collected data and reveal correlations between users’ eye fixations and other factors, i.e. users’ historical gaze positions, task-related objects, saliency information of the VR content, and users’ head rotation velocities. Based on this analysis, we propose FixationNet – a novel learning-based model to forecast users’ eye fixations in the near future in VR. We evaluate the performance of our model for free-viewing and task-oriented settings and show that it outperforms the state of the art by a large margin of 19.8% (from a mean error of 2.93° to 2.35°) in free-viewing and of 15.1% (from 2.05° to 1.74°) in task-oriented situations. As such, our work provides new insights into task-oriented attention in virtual environments and guides future work on this important topic in VR research.

        <hr>

        <h4>Presentation Video</h4>
        <object type='application/x-shockwave-flash' style='width:1024px; height:576px;' data='https://www.youtube.com/v/kQGqXLJwM54' class="centerContent">
        <param name='movie' value='https://www.youtube.com/v/kQGqXLJwM54' />
        </object>                
        <br>

        <h4>Demo Video</h4>
        <object type='application/x-shockwave-flash' style='width:1024px; height:576px;' data='https://www.youtube.com/v/ephYjXvUyGo' class="centerContent">
        <param name='movie' value='https://www.youtube.com/v/ephYjXvUyGo'/>
        </object>
           

        <hr>

        <h4>Links</h4>
        <div class="pub_links">
            
            <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2021.3067779" rel="nofollow" target="_blank">doi</a></p>
            
            
            <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./hu21_fixationnet/pdf/hu21_fixationnet.pdf">paper.pdf</a></p>
          
          
          
            <i class="fa fa-link"></i><p>Code: <a class="a-text-ext" href="https://github.com/CraneHzm/FixationNet" rel="nofollow" target="_blank">code</a></p>            
          
            <i class="fa fa-link"></i><p>Dataset: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/:f:/g/personal/1701111311_pku_edu_cn/EnIq21YTs2lFqIlTDS1JPXQBj8acW2PGfutjHzGaLqMkVA?e=9IFekh" rel="nofollow" target="_blank">dataset</a></p>
          
            <i class="fa fa-file-pdf"></i>&nbsp;Slides: <a class="pub_list" href="./hu21_fixationnet/ppt/hu21_fixationnet.pdf">slides.pdf</a></p>
          
            <i class="fa fa-link"></i><p>Experimental scenes: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/:f:/g/personal/1701111311_pku_edu_cn/EkFr_JPZMslBmEXADxAehTQB-AwGsqEeOiv4OXG6_5wGhA?e=KJHFVt" rel="nofollow" target="_blank">experimental senes</a></p>
          
            <i class="fa fa-link"></i><p>Supplementary materials: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/:u:/g/personal/1701111311_pku_edu_cn/EdljgqDO4P1Jk65pTDSOwtUBH61Uy-2PObzVjqU-NhX6ng?e=6DLdLZ" rel="nofollow" target="_blank">supplementary materials</a></p>
          </div>

        <hr>

        <h4>BibTeX</h4>

<div class="pub_bibtex bg_grey">@article{hu21fixationnet,
	author = {Hu, Zhiming and Bulling, Andreas and Li, Sheng and Wang, Guoping},
	title = {FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	year = {2021},
	doi = {10.1109/TVCG.2021.3067779},
	pages = {2681--2690},
	volume = {27},
	number = {5},
	url = {https://cranehzm.github.io/FixationNet.html}}
</div>

        
    </div>
</div>

        </div>

        <!-- footer -->
        <div id="footer" class="footer-v1">
            
            <div class="copyright custom-copyright">
                <div class="container">
                    <div class="row">
                        <div class="col-md-6">
                            <p>
                                
                                <span class="custom-copyright-container">
                                    
                                    Last modified: 28/06/2023
                                </span>
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        
    </body>
</html>
