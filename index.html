<!DOCTYPE html>
<html lang="en" class="fontawesome-i2svg-active fontawesome-i2svg-complete"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">        
<title>Zhiming Hu - Homepage</title>        
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" media="all" href="./index/css/main_v2.css">
<link rel="stylesheet" media="all" href="./index/css/owl.carousel.min.css">
<link rel="stylesheet" media="all" href="./index/css/owl.theme.default.min.css">
<link rel="stylesheet" media="all" href="./index/css/jquery-ui.min.css">
<link rel="stylesheet" href="./index/css/swipe.css">
<script src="./index/js/jquery-3.4.1.min.js"></script>
<script src="./index/js/jquery-ui.min.js"></script>
<script src="./index/js/fontawesome-5.11.2.js"></script>
<script src="./index/js/main.js"></script>
<script src="./index/js/owl.carousel.min.js"></script>
<script src="./index/js/rot13.js"></script>
<script src="./index/js/script.js"></script>
<script src="https://www.w3counter.com/tracker.js?id=150008"></script>
</head>

<body class="header header-location">
<div class="wrapper">
<!-- header -->
<div id="unstickyheader" class="unstickyheader">
<div class="topbar">
<div class="container">
<div class="header-image-container">
	<!--<a href=""><img class="header-image" src="./index/" alt=""></a>-->
</div>
<div class="header-image-container-right">
	<!--a href=""><img class="header-image" src="./index/image/" alt=""></a>-->
</div>
</div>
</div>
</div>
</div>

<!-- content -->
<div class="content">
<div class="section margin-top-20 margin-bottom-20">
<div class="container">

<div class="person">
  <img class="personpic" title="Zhiming Hu" src="./index/image/hu.png" alt="Zhiming Hu">

  
<div class="personinfo">
<h3>Zhiming Hu</h3>

<p><a class="a-text-ext" href="https://www.simtech.uni-stuttgart.de/" title="">Stuttgart Center for Simulation Science (SimTech)</a>, <a class="a-text-ext" href="https://www.uni-stuttgart.de/" target="_blank" title="University of Stuttgart" rel="nofollow">University of Stuttgart</a></p>
<p>
	<svg class="svg-inline--fa fa-envelope fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="envelope" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg><!-- <i class="fa fa-envelope"></i> --><span>zhiming.hu (at) simtech.uni-stuttgart.de</span>
	<!--<svg class="svg-inline--fa fa-phone fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="phone" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M493.4 24.6l-104-24c-11.3-2.6-22.9 3.3-27.5 13.9l-48 112c-4.2 9.8-1.4 21.3 6.9 28l60.6 49.6c-36 76.7-98.9 140.5-177.2 177.2l-49.6-60.6c-6.8-8.3-18.2-11.1-28-6.9l-112 48C3.9 366.5-2 378.1.6 389.4l24 104C27.1 504.2 36.7 512 48 512c256.1 0 464-207.5 464-464 0-11.2-7.7-20.9-18.6-23.4z"></path></svg><span><a class="a-text-ext" href="https://scholar.google.com/citations?user=OLB_xBEAAAAJ" title="Google Scholar">Google Scholar</a>, <a class="a-text-ext" href="https://github.com/cranehzm" title="Github">Github</a>, <a class="a-text-ext" href="./CurriculumVitae/Curriculum_Vitae_Zhiming_Hu.pdf" title="Curriculum Vitae">Curriculum Vitae</a>, <a class="a-text-ext" href="https://mp.weixin.qq.com/s/7uPr0SxNhLA58K97osda-Q" title="WeChat Official Account">WeChat Official Account</a></span>-->
	<svg class="svg-inline--fa fa-map-marker-alt fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="map-marker-alt" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M172.268 501.67C26.97 291.031 0 269.413 0 192 0 85.961 85.961 0 192 0s192 85.961 192 192c0 77.413-26.97 99.031-172.268 309.67-9.535 13.774-29.93 13.773-39.464 0zM192 272c44.183 0 80-35.817 80-80s-35.817-80-80-80-80 35.817-80 80 35.817 80 80 80z"></path></svg><!-- <i class="fa fa-map-marker-alt"></i> --><span><a href="https://osm.org/go/0Dk7mAUtE?node=1568673162" rel="nofollow" target="_blank" class="a-text-ext">Pfaffenwaldring 5a, 70569 Stuttgart, Germany</a></span>
	<svg class="svg-inline--fa fa-door-open fa-w-20" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="door-open" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" data-fa-i2svg=""><path fill="currentColor" d="M624 448h-80V113.45C544 86.19 522.47 64 496 64H384v64h96v384h144c8.84 0 16-7.16 16-16v-32c0-8.84-7.16-16-16-16zM312.24 1.01l-192 49.74C105.99 54.44 96 67.7 96 82.92V448H16c-8.84 0-16 7.16-16 16v32c0 8.84 7.16 16 16 16h336V33.18c0-21.58-19.56-37.41-39.76-32.17zM264 288c-13.25 0-24-14.33-24-32s10.75-32 24-32 24 14.33 24 32-10.75 32-24 32z"></path></svg><!-- <i class="fa fa-door-open"></i> --><span>University of Stuttgart, SimTech Building</span>		
	<svg class="svg-inline--fa fa-graduation-cap fa-w-20" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="graduation-cap" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" data-fa-i2svg=""><path fill="currentColor" d="M622.34 153.2L343.4 67.5c-15.2-4.67-31.6-4.67-46.79 0L17.66 153.2c-23.54 7.23-23.54 38.36 0 45.59l48.63 14.94c-10.67 13.19-17.23 29.28-17.88 46.9C38.78 266.15 32 276.11 32 288c0 10.78 5.68 19.85 13.86 25.65L20.33 428.53C18.11 438.52 25.71 448 35.94 448h56.11c10.24 0 17.84-9.48 15.62-19.47L82.14 313.65C90.32 307.85 96 298.78 96 288c0-11.57-6.47-21.25-15.66-26.87.76-15.02 8.44-28.3 20.69-36.72L296.6 284.5c9.06 2.78 26.44 6.25 46.79 0l278.95-85.7c23.55-7.24 23.55-38.36 0-45.6zM352.79 315.09c-28.53 8.76-52.84 3.92-65.59 0l-145.02-44.55L128 384c0 35.35 85.96 64 192 64s192-28.65 192-64l-14.18-113.47-145.03 44.56z"></path></svg><!-- <i class="fa fa-graduation-cap"></i> --><span><a class="a-text-ext" href="https://scholar.google.com/citations?user=OLB_xBEAAAAJ" title="google scholar">google scholar</a>, <a class="a-text-ext" href="https://github.com/cranehzm" title="github">github</a>, <a class="a-text-ext" href="./curriculum vitae/curriculum_vitae_Zhiming_Hu.pdf" title="curriculum vitae">curriculum vitae</a><!--, <a class="a-text-ext" href="./index/image/weChat_official_account.jpg" title="essays">essays</a>--></span>
</p>
</div>
</div>
<div class="clearfix"></div>

<hr>
<h3>Short Bio</h3>
<p>I am a post-doctoral researcher in the <a class="a-text-ext" href="https://www.perceptualui.org/" title="">Perceptual User Interfaces Group</a> led by <a class="a-text-ext" href="https://www.perceptualui.org/people/bulling/" title="">Prof. Andreas Bulling</a> and <a class="a-text-ext" href="https://www.imsb.uni-stuttgart.de/" title="">Computational Biophysics and Biorobotics Group</a> led by <a class="a-text-ext" href="https://www.imsb.uni-stuttgart.de/team/Schmitt-00006/" title="">Prof. Syn Schmitt</a>, in the <b>University of Stuttgart</b>, Germany since August 2022. I obtained my Ph.D. degree in Computer Software and Theory from <b>Peking University</b>, China in 2022, supervised by <a class="a-text-ext" href="https://www.graphics.pku.edu.cn/xztd/jgfaculty/wgp2/index.htm#EN_intro" title="">Prof. Guoping Wang</a>. I received my Bachelor's degree in Optical Engineering from <b>Beijing Institute of Technology</b>, China in 2017.
</p>


<hr>
<h3>Latest News</h3>
<ul>
<li>[08.2023] I will serve as the Virtualization Chair for ETRA 2024.</li>
<li>[08.2023] One paper is accepted at UIST 2023.</li>
<li>[06.2023] I will serve as an Associate Chair for MuC 2023.</li>
<li>[06.2023] One paper is accepted at INTERACT 2023.</li>
<li>[05.2023] I will serve as a Technical Program Committee member for iWOAR 2023.</li>
<li>[10.2022] One paper is accepted at NeurIPS 2022 Workshop Gaze Meets ML.</li>
<li>[08.2022] I joined the University of Stuttgart as a post-doctoral researcher.</li>
<li>[07.2022] I successfully defended my Ph.D.!</li>
</ul>


<hr>
<h3>Research Interests</h3>
<p>My research interests include virtual reality, human-computer interaction, eye tracking, and human behaviour modelling. The long-term research goal is to build a human-aware intelligent interactive system that can accurately model human behaviours in activities of daily living. My previous research mostly focused on the modelling of human eye gaze behaviour. I am now extending my research to other human behaviours, e.g. human body motions, as well as scene-aware/context-aware behaviour modelling for daily activities.
</p>

<hr>
<h3>Awards & Honors</h3>
<ul>
<li>SimTech Research Fellowship, 2022</li>
<!--<li>Outstanding Graduates Prize in Peking University, 2022</li>-->
<li> National Scholarship (top 2%), 2021</li>
<!--<li> Merit Student Prize in Peking University, 2021</li>-->
<li> TVCG Best Journal Nominees Award (IEEE VR 2021, top 2%)</li>
<li> CSC (China Scholarship Council) Scholarship, 2020</li>
<!--<li> Second-Class Scholarship of Peking University, 2020</li>-->
<!--<li> Merit Student Prize in Peking University, 2020</li>-->
<li> Chancellor's Scholarship (top 2%), 2020</li>
<li> Leo KoGuan Scholarship (top 5%), 2019</li>
<!--<li> Hongcai Scholarship, 2019</li>-->
<!--<li> Merit Student Prize in Peking University, 2019</li>-->
<!--<li> Hongcai Scholarship, 2017</li>-->
<li> Leader Scholarship (top 0.2%, only 7 in over 3800 students), 2017</li>
<!--<li> Outstanding Graduates Prize in Beijing Institute of Technology, 2017</li>-->
<li> National Scholarship (top 2%), 2016</li>
<!--<li> Honorable Mention of Mathematical Contest in Modeling, 2016</li>-->
<!--<li> Merit Student Prize in Beijing Institute of Technology, 2016</li>-->
<!--<li> National Encouragement Scholarship, 2015</li>-->
<!--<li> Merit Student Prize in Beijing Institute of Technology, 2015</li>-->
<!--<li> Third Prize of the National College Students Composition Competition, 2015</li>-->
<!--<li> Pacemaker to Merit Student Prize in Beijing Institute of Technology, 2014</li>-->
<!--<li> Third Prize of Beijing Humanistic Knowledge Competition, 2014</li>-->
<li> National Scholarship (top 2%), 2014</li>
</ul>


<hr>
<h3>Professional Activities & Talks</h3>
<b>Reviewing</b>
<ul>
<li> Journals: IMWUT, TiiS, T-MM, TVCG, IJHCI, MTAP</li>
<li> Conferences: CVPR, ICCV, ECCV, UIST, IEEE VR, ISMAR</li>
</ul>

<b>Organizing Committee</b>
<ul>
<li> Virtualization Chair for ETRA 2024</li>
<li> Associate Chair for MuC 2023</li>
<li> Technical Program Committee member for iWOAR 2023</li>
</ul>

<b>Invited Talks</b>
<ul>
<li> Analysis and Prediction of Human Visual Attention in Virtual Reality. Southeast University, China, Hosted by <a class="a-text-ext" href="https://dingdingseu.mystrikingly.com/" title="">Prof. Ding Ding</a>, June, 2022.</li>
<li> Recognizing User Tasks from Eye and Head Movements in Immersive Virtual Reality. IEEE VR 2022, Hosted by <a class="a-text-ext" href="https://carelab.info/en/kiyoshi-kiyokawa/" title="">Prof. Kiyoshi Kiyokawa</a>, March, 2022.</li>
<li> Forecasting Eye Fixations in Task-Oriented Virtual Environments. GAMES Webinar 2021, Hosted by <a class="a-text-ext" href="http://dalab.se.sjtu.edu.cn/www/home/?page_id=143" title="">Prof. Xubo Yang</a>, September, 2021.</li>
<li> Eye-Head Coordination Model for Real-time Gaze Prediction. 2019 International Conference on VR/AR and 3D Display, Hosted by <a class="a-text-ext" href="http://xufeng.site/" title="">Prof. Feng Xu</a>, June 2019.</li>
</ul>


<hr>
<h3>Teaching</h3>
<ul>
<li> Machine Perception and Learning, University of Stuttgart, 2022, Lecturer</li>
<li> Computer Graphics, Peking University, 2018, Teaching Assistant</li>
<li> Image and Video-Based 3D Reconstruction, Peking University, 2018, Teaching Assistant</li>
<li> Programming Basics, Peking University, 2018, Teaching Assistant</li>
</ul>


<hr>
<h3>Selected Publications</h3>
* Corresponding author
<!--paper-->
<ol class="bibliography"><li>
<div id="zhang23_exploring" class="margin-bottom-30">    
        <a href="./zhang23_exploring.html">
        <img class="thumbnail" src="./zhang23_exploring/image/thumb.png" title="Exploring Natural Language Processing Methods for Interactive Behaviour Modelling" alt="Exploring Natural Language Processing Methods for Interactive Behaviour Modelling">
        </a>
    

    <p class="pub_title">Exploring Natural Language Processing Methods for Interactive Behaviour Modelling</p>

    <p class="pub_author">
        
            <a class="a-int" href="https://www.perceptualui.org/people/gzhang/">Guanhua Zhang</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bortoletto/">Matteo Bortoletto</a>,
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/shi/">Lei Shi</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bace/">Mihai Bâce</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
    </p>

    <p class="pub_additional">
        
            <span class="pub_additional_journal">Proc. IFIP TC13 Conference on Human-Computer Interaction, </span>
        

        
            <span class="pub_additional_pages">pp. 1–22, </span>
        
        <span class="pub_additional_year">2023</span>.
    </p>

    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_zhang23_exploring" class="pub_show" onclick="pub_showhide('zhang23_exploring','pub_abstract')">Abstract</a></span>
        

        
        <span class="button bg_greydark"><a id="pub_links_sh_zhang23_exploring" class="pub_show" onclick="pub_showhide('zhang23_exploring','pub_links')">Links</a></span>
        

        <span class="button bg_greydark"><a id="pub_bibtex_sh_zhang23_exploring" class="pub_show" onclick="pub_showhide('zhang23_exploring','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./zhang23_exploring.html">Project</a></span>
        
    </p>

    <div class="pub_abstract bg_grey" id="pub_abstract_zhang23_exploring" style="display:none;">
        Analysing and modelling interactive behaviour is an important topic in human-computer interaction (HCI) and a key requirement for the development of intelligent interactive systems. Interactive behaviour has a sequential (actions happen one after another) and hierarchical (a sequence of actions forms an activity driven by interaction goals) structure, which may be similar to the structure of natural language. Designed based on such a structure, natural language processing (NLP) methods have achieved groundbreaking success in various downstream tasks. However, few works linked interactive behaviour with natural language. In this paper, we explore the similarity between interactive behaviour and natural language by applying an NLP method, byte pair encoding (BPE), to encode mouse and keyboard behaviour. We then analyse the vocabulary, i.e., the set of action sequences, learnt by BPE, as well as use the vocabulary to encode the input behaviour for interactive task recognition. An existing dataset collected in constrained lab settings and our novel out-of-the-lab dataset were used for evaluation. Results show that this natural language-inspired approach not only learns action sequences that reflect specific interaction goals, but also achieves higher F1 scores on task recognition than other methods. Our work reveals the similarity between interactive behaviour and natural language, and presents the potential of applying the new pack of methods that leverage insights from NLP to model interactive behaviour in HCI.
    </div>

    <div class="pub_links bg_grey" id="pub_links_zhang23_exploring" style="display:none;">
        <div class="pub_links">
                
                
                    <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./zhang23_exploring/pdf/zhang23_exploring.pdf">paper.pdf</a></p>
                
                
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_zhang23_exploring" style="display:none;">
@inproceedings{zhang23_exploring,
  title = {Exploring Natural Language Processing Methods for Interactive Behaviour Modelling},
  author = {Zhang, Guanhua and Bortoletto, Matteo and Hu, Zhiming and Shi, Lei and B{\^a}ce, Mihai and Bulling, Andreas},
  booktitle = {Proc. IFIP TC13 Conference on Human-Computer Interaction (INTERACT)},
  pages = {1--22},
  year = {2023},
  publisher = {Springer}
}
</div>

</div>
<a class="details" href="/publications/zhang23_exploring/"></a>
</li></ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu22_ehtask" class="margin-bottom-30">
<a href="./hu22_ehtask.html">
<img class="thumbnail" src="./hu22_ehtask/image/thumb.png" title="EHTask: Recognizing User Tasks from Eye and Head Movements in Immersive Virtual Reality" alt="EHTask: Recognizing User Tasks from Eye and Head Movements in Immersive Virtual Reality">
</a>	
<p class="pub_title">EHTask: Recognizing User Tasks from Eye and Head Movements in Immersive Virtual Reality</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
<a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>,
Sheng Li, Guoping Wang
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics, 2023, 29(4): 1992-2004.
</span>
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu22_ehtask" class="pub_show" onclick="pub_showhide(&#39;hu22_ehtask&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu22_ehtask" class="pub_show" onclick="pub_showhide(&#39;hu22_ehtask&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu22_ehtask" class="pub_show" onclick="pub_showhide(&#39;hu22_ehtask&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu22_ehtask.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu22_ehtask" style="display:none;">
Understanding human visual attention in immersive virtual reality (VR) is crucial for many important applications, including gaze prediction, gaze guidance, and gaze-contingent rendering.
However, previous works on visual attention analysis typically only explored one specific VR task and paid less attention to the differences between different tasks.
Moreover, existing task recognition methods typically focused on 2D viewing conditions and only explored the effectiveness of human eye movements.
We first collect eye and head movements of 30 participants performing four tasks, i.e. Free viewing, Visual search, Saliency, and Track, in 15 360-degree VR videos.
Using this dataset, we analyze the patterns of human eye and head movements and reveal significant differences across different tasks in terms of fixation duration, saccade amplitude, head rotation velocity, and eye-head coordination.
We then propose EHTask -- a novel learning-based method that employs eye and head movements to recognize user tasks in VR.
We show that our method significantly outperforms the state-of-the-art methods derived from 2D viewing conditions both on our dataset (accuracy of 84.4% vs. 62.8%) and on a real-world dataset (61.9% vs. 44.1%). 
As such, our work provides meaningful insights into human visual attention under different VR tasks and guides future work on recognizing user tasks in VR.
</div>
<div class="pub_links bg_grey" id="pub_links_hu22_ehtask" style="display:none;">
<div class="pub_links">
  <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2021.3138902" rel="nofollow" target="_blank">doi</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu22_ehtask/pdf/hu22_ehtask.pdf">paper.pdf</a></p>
  <i class="fa fa-database"></i>&nbsp;&nbsp;Dataset: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FEHTaskDataset&ga=1">dataset</a></p>
  <i class="fa fa-file-powerpoint"></i>&nbsp; Slides: <a class="pub_list" href="./hu22_ehtask/ppt/hu22_ehtask.pdf">slides.pdf</a></p>
  <i class="fa fa-code"></i>&nbsp;Code: <a class="a-text-ext" href="https://github.com/CraneHzm/EHTask">code</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu22_ehtask" style="display:none;">
  @ARTICLE{hu22_ehtask,
    author={Hu, Zhiming and Bulling, Andreas and Li, Sheng and Wang, Guoping},
    journal={IEEE Transactions on Visualization and Computer Graphics}, 
    title={EHTask: Recognizing User Tasks From Eye and Head Movements in Immersive Virtual Reality}, 
    year={2023},
    volume={29},
    number={4},
    pages={1992-2004},
    doi={10.1109/TVCG.2021.3138902}}
  
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
  <div id="lin22_intentional" class="margin-bottom-30">
  
      
      
          <a href="./lin22_intentional.html">
          <img class="thumbnail" src="./lin22_intentional/image/thumb.png" title="Intentional Head-Motion Assisted Locomotion for Reducing Cybersickness" alt="Intentional Head-Motion Assisted Locomotion for Reducing Cybersickness">
          </a>
      
  
      <p class="pub_title">Intentional Head-Motion Assisted Locomotion for Reducing Cybersickness</p>
  
      <p class="pub_author">
          
              Zehui Lin,
              Xiang Gu,
              Sheng Li,
              <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
              Guoping Wang
      </p>
  
      <p class="pub_additional">
          
              <span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics,
          </span>
          
  
          
          <span class="pub_additional_year">2022,</span>
          <span class="pub_additional_page"> 29(8): 3458-3471.</span>
      </p>
  
      <p class="pub_tags">
          
          <span class="button bg_greydark"><a id="pub_abstract_sh_lin22_intentional" class="pub_show" onclick="pub_showhide('lin22_intentional','pub_abstract')">Abstract</a></span>
          
  
          
          <span class="button bg_greydark"><a id="pub_links_sh_lin22_intentional" class="pub_show" onclick="pub_showhide('lin22_intentional','pub_links')">Links</a></span>
          
  
          <span class="button bg_greydark"><a id="pub_bibtex_sh_lin22_intentional" class="pub_show" onclick="pub_showhide('lin22_intentional','pub_bibtex')">BibTeX</a></span>
  
          <span class="button bg_greydark margin-left-10"><a href="./lin22_intentional.html">Project</a></span>
  
          
      </p>
  
      <div class="pub_abstract bg_grey" id="pub_abstract_lin22_intentional" style="display:none;">
          We present an efficient locomotion technique that can reduce cybersickness through aligning the visual and vestibular induced self-motion illusion. Our locomotion technique stimulates proprioception consistent with the visual sense by intentional head motion, which includes both the head’s translational movement and yaw rotation. A locomotion event is triggered by the hand-held controller together with an intended physical head motion simultaneously. Based on our method, we further explore the connections between the level of cybersickness and the velocity of self motion through a series of experiments. We first conduct Experiment 1 to investigate the cybersickness induced by different translation velocities using our method and then conduct Experiment 2 to investigate the cybersickness induced by different angular velocities. Our user studies from these two experiments reveal a new finding on the correlation between translation/angular velocities and the level of cybersickness. The cybersickness is greatest at the lowest velocity using our method, and the statistical analysis also indicates a possible U-shaped relation between the translation/angular velocity and cybersickness degree. Finally, we conduct Experiment 3 to evaluate the performances of our method and other commonly-used locomotion approaches, i.e., joystick-based steering and teleportation. The results show that our method can significantly reduce cybersickness compared with the joystick-based steering and obtain a higher presence compared with the teleportation. These advantages demonstrate that our method can be an optional locomotion solution for immersive VR applications using commercially available HMD suites only.
      </div>
  
      <div class="pub_links bg_grey" id="pub_links_lin22_intentional" style="display:none;">
          <div class="pub_links">
                  
                      <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="https://ieeexplore.ieee.org/document/9737429" rel="nofollow" target="_blank">doi</a></p>
                  
                  
                      <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./lin22_intentional/pdf/lin22_intentional.pdf">paper.pdf</a></p>
                  
                 
          </div>
      </div>
  <div class="pub_bibtex bg_grey" id="pub_bibtex_lin22_intentional" style="display:none;">@ARTICLE{lin22_intentional,
    author={Lin, Zehui and Gu, Xiang and Li, Sheng and Hu, Zhiming and Wang, Guoping},
    journal={IEEE Transactions on Visualization and Computer Graphics}, 
    title={Intentional Head-Motion Assisted Locomotion for Reducing Cybersickness}, 
    year={2023},
    volume={29},
    number={8},
    pages={3458-3471},
    doi={10.1109/TVCG.2022.3160232}}
  </div>
  <a class="details" href="/publications/lin22_intentional/"></a>
  </li></ol>

<!--paper-->
<ol class="bibliography"><li>
<div id="elfares22_federated" class="margin-bottom-30">

    
    
        <a href="./elfares22_federated.html">
        <img class="thumbnail" src="./elfares22_federated/image/thumb.png" title="Federated Learning for Appearance-based Gaze Estimation in the Wild" alt="Federated Learning for Appearance-based Gaze Estimation in the Wild">
        </a>
    

    <p class="pub_title">Federated Learning for Appearance-based Gaze Estimation in the Wild</p>

    <p class="pub_author">
        
            <a class="a-int" href="https://www.perceptualui.org/people/elfares/">Mayar Elfares</a>,
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
            Pascal Reisert,
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>,
            Ralf Küsters
    </p>

    <p class="pub_additional">
        
            <span class="pub_additional_journal">Proceedings of the NeurIPS Workshop Gaze Meets ML, </span>
        

        
            <span class="pub_additional_pages">pp. 1–17, </span>
        
        <span class="pub_additional_year">2022</span>.
    </p>

    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_elfares22_federated" class="pub_show" onclick="pub_showhide('elfares22_federated','pub_abstract')">Abstract</a></span>
        

        
        <span class="button bg_greydark"><a id="pub_links_sh_elfares22_federated" class="pub_show" onclick="pub_showhide('elfares22_federated','pub_links')">Links</a></span>
        

        <span class="button bg_greydark"><a id="pub_bibtex_sh_elfares22_federated" class="pub_show" onclick="pub_showhide('elfares22_federated','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./elfares22_federated.html">Project</a></span>

        
    </p>

    <div class="pub_abstract bg_grey" id="pub_abstract_elfares22_federated" style="display:none;">
        Gaze estimation methods have significantly matured in recent years but the large number of eye images required to train deep learning models poses significant privacy risks. In addition, the heterogeneous data distribution across different users can significantly hinder the training process. In this work, we propose the first federated learning approach for gaze estimation to preserve the privacy of gaze data. We further employ pseudo-gradients optimisation to adapt our federated learning approach to the divergent model updates to address the heterogeneous nature of in-the-wild gaze data in collaborative setups. We evaluate our approach on a real-world dataset (MPIIGaze dataset) and show that our work enhances the privacy guarantees of conventional appearance-based gaze estimation methods, handles the convergence issues of gaze estimators, and significantly outperforms vanilla federated learning by 15.8% (from a mean error of 10.63 degrees to 8.95 degrees). As such, our work paves the way to develop privacy-aware collaborative 14 learning setups for gaze estimation while maintaining the model’s performance.
    </div>

    <div class="pub_links bg_grey" id="pub_links_elfares22_federated" style="display:none;">
        <div class="pub_links">
              
                
                    <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.48550/arXiv.2211.07330" rel="nofollow" target="_blank">doi</a></p>

                    <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./elfares22_federated/pdf/elfares22_federated.pdf">paper.pdf</a></p>
                
                
                
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_elfares22_federated" style="display:none;">@inproceedings{elfares22_federated,
  title = {Federated Learning for Appearance-based Gaze Estimation in the Wild},
  author = {Elfares, Mayar and Hu, Zhiming and Reisert, Pascal and Bulling, Andreas and Küsters, Ralf},
  year = {2022},
  booktitle = {Proceedings of the NeurIPS Workshop Gaze Meets ML (GMML)},
  doi = {10.48550/arXiv.2211.07330},
  pages = {1--17}
}
</div>

</div>
<a class="details" href="/publications/elfares22_federated/"></a>
</li></ol>


<!--paper
<ol class="bibliography"><li>
<div id="hu21_user" class="margin-bottom-30">
<a href="./hu21_user.html">
<img class="thumbnail" src="./hu21_user/image/thumb.png" title="Research progress of user task prediction and algorithm analysis (in Chinese)" alt="Research progress of user task prediction and algorithm analysis (in Chinese)">
</a>	
<p class="pub_title">Research progress of user task prediction and algorithm analysis (in Chinese)</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
Sheng Li, Meng Gai
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">Journal of Graphics, 2021, 42(3): 367-375.
</span>
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu21_user" class="pub_show" onclick="pub_showhide(&#39;hu21_user&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu21_user" class="pub_show" onclick="pub_showhide(&#39;hu21_user&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu21_user" class="pub_show" onclick="pub_showhide(&#39;hu21_user&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu21_user.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu21_user" style="display:none;">
Users’ cognitive behaviors are dramatically influenced by the specific tasks assigned to them. 
Information on users’ tasks can be applied to many areas, such as human behavior analysis and intelligent human-computer interfaces. 
It can be used as the input of intelligent systems and enable the systems to automatically adjust their functions according to different tasks. 
User task prediction refers to the prediction of users’ tasks at hand based on the characteristics of his or her eye movements, the characteristics of scene content, and other related information. 
User task prediction is a popular research topic in vision research, and researchers have proposed many successful task prediction algorithms. 
However, the algorithms proposed in prior works mainly focus on a particular scene, and comparison and analysis are absent for these algorithms. 
This paper presented a review of prior works on task prediction in scenes of images, videos, and real world, and detailed existing task prediction algorithms. 
Based on a real-world task dataset, this paper evaluated the performances of existing algorithms and conducted the corresponding analysis and discussion. 
As such, this work can provide meaningful insights for future works on this important topic.
</div>
<div class="pub_links bg_grey" id="pub_links_hu21_user" style="display:none;">
<div class="pub_links">
  <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://www.txxb.com.cn/CN/10.11996/JG.j.2095-302X.2021030367" rel="nofollow" target="_blank">doi</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu21_user/pdf/hu21_user.pdf">paper.pdf</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu21_user" style="display:none;">
@article{hu21_user,
  title = {Research progress of user task prediction and algorithm analysis (in Chinese)},
  author = {Hu, Zhiming and Li, Sheng and Gai, Meng},
  year = {2021},
  journal={Journal of Graphics},
  doi = {http://www.txxb.com.cn/CN/10.11996/JG.j.2095-302X.2021030367},
  volume = {42},
  number = {3},
  pages = {367-375}
}
</div>
</ol>-->


<!--paper
<ol class="bibliography"><li>
<div id="hu21_eye" class="margin-bottom-30">
<a href="./hu21_eye.html">
<img class="thumbnail" src="./hu21_eye/image/thumb.png" title="Eye Fixation Forecasting in Task-Oriented Virtual Reality" alt="Eye Fixation Forecasting in Task-Oriented Virtual Reality">
</a>
<p class="pub_title">Eye Fixation Forecasting in Task-Oriented Virtual Reality</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">Proceedings of the 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, 2021: 707-708.
</span>
</p>
<p class="pub_tags">        
<span class="button bg_greydark"><a id="pub_abstract_sh_hu21_eye" class="pub_show" onclick="pub_showhide(&#39;hu21_eye&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
<span class="button bg_greydark"><a id="pub_links_sh_hu21_eye" class="pub_show" onclick="pub_showhide(&#39;hu21_eye&#39;,&#39;pub_links&#39;)">Links</a></span>
<span class="button bg_greydark"><a id="pub_bibtex_sh_hu21_eye" class="pub_show" onclick="pub_showhide(&#39;hu21_eye&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
<span class="button bg_greydark margin-left-10"><a href="./hu21_eye.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu21_eye" style="display:none;">
In immersive virtual reality (VR), users' visual attention is crucial for many important applications, including VR content design, gaze-based interaction, and gaze-contingent rendering. 
Especially, information on users' future eye fixations is key for intelligent user interfaces and has significant relevance for many areas, such as visual attention enhancement, dynamic event triggering, and human-computer interaction. 
However, previous works typically focused on free-viewing conditions and paid less attention to task-oriented attention. 
This paper aims at forecasting users' eye fixations in task-oriented virtual reality. 
To this end, a VR eye tracking dataset that corresponds to different users performing a visual search task in immersive virtual environments is built. 
A comprehensive analysis of users' eye fixations is performed based on the collected data. 
The analysis reveals that eye fixations are correlated with users' historical gaze positions, task-related objects, saliency information of the VR content, and head rotation velocities. 
Based on this analysis, a novel learning-based model is proposed to forecast users' eye fixations in the near future in immersive virtual environments.</div>
<div class="pub_links bg_grey" id="pub_links_hu21_eye" style="display:none;">
<div class="pub_links">            
	<svg class="svg-inline--fa fa-fingerprint fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="fingerprint" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M256.12 245.96c-13.25 0-24 10.74-24 24 1.14 72.25-8.14 141.9-27.7 211.55-2.73 9.72 2.15 30.49 23.12 30.49 10.48 0 20.11-6.92 23.09-17.52 13.53-47.91 31.04-125.41 29.48-224.52.01-13.25-10.73-24-23.99-24zm-.86-81.73C194 164.16 151.25 211.3 152.1 265.32c.75 47.94-3.75 95.91-13.37 142.55-2.69 12.98 5.67 25.69 18.64 28.36 13.05 2.67 25.67-5.66 28.36-18.64 10.34-50.09 15.17-101.58 14.37-153.02-.41-25.95 19.92-52.49 54.45-52.34 31.31.47 57.15 25.34 57.62 55.47.77 48.05-2.81 96.33-10.61 143.55-2.17 13.06 6.69 25.42 19.76 27.58 19.97 3.33 26.81-15.1 27.58-19.77 8.28-50.03 12.06-101.21 11.27-152.11-.88-55.8-47.94-101.88-104.91-102.72zm-110.69-19.78c-10.3-8.34-25.37-6.8-33.76 3.48-25.62 31.5-39.39 71.28-38.75 112 .59 37.58-2.47 75.27-9.11 112.05-2.34 13.05 6.31 25.53 19.36 27.89 20.11 3.5 27.07-14.81 27.89-19.36 7.19-39.84 10.5-80.66 9.86-121.33-.47-29.88 9.2-57.88 28-80.97 8.35-10.28 6.79-25.39-3.49-33.76zm109.47-62.33c-15.41-.41-30.87 1.44-45.78 4.97-12.89 3.06-20.87 15.98-17.83 28.89 3.06 12.89 16 20.83 28.89 17.83 11.05-2.61 22.47-3.77 34-3.69 75.43 1.13 137.73 61.5 138.88 134.58.59 37.88-1.28 76.11-5.58 113.63-1.5 13.17 7.95 25.08 21.11 26.58 16.72 1.95 25.51-11.88 26.58-21.11a929.06 929.06 0 0 0 5.89-119.85c-1.56-98.75-85.07-180.33-186.16-181.83zm252.07 121.45c-2.86-12.92-15.51-21.2-28.61-18.27-12.94 2.86-21.12 15.66-18.26 28.61 4.71 21.41 4.91 37.41 4.7 61.6-.11 13.27 10.55 24.09 23.8 24.2h.2c13.17 0 23.89-10.61 24-23.8.18-22.18.4-44.11-5.83-72.34zm-40.12-90.72C417.29 43.46 337.6 1.29 252.81.02 183.02-.82 118.47 24.91 70.46 72.94 24.09 119.37-.9 181.04.14 246.65l-.12 21.47c-.39 13.25 10.03 24.31 23.28 24.69.23.02.48.02.72.02 12.92 0 23.59-10.3 23.97-23.3l.16-23.64c-.83-52.5 19.16-101.86 56.28-139 38.76-38.8 91.34-59.67 147.68-58.86 69.45 1.03 134.73 35.56 174.62 92.39 7.61 10.86 22.56 13.45 33.42 5.86 10.84-7.62 13.46-22.59 5.84-33.43z"></path></svg><p>Doi: <a class="a-text-ext" href="https://doi.org/10.1109/VRW52623.2021.00236" rel="nofollow" target="_blank">doi</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu21_eye/pdf/hu21_eye.pdf">paper.pdf</a></p>                
  <i class="fa fa-file-powerpoint"></i>&nbsp;&nbsp; Slides: <a class="pub_list" href="./hu21_eye/ppt/hu21_eye.pdf">slides.pdf</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu21_eye" style="display:none;">
@inproceedings{hu21_eye, 
  author={Hu, Zhiming},
  title = {Eye Fixation Forecasting in Task-Oriented Virtual Reality}, 
  booktitle={Proceedings of the 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops},
  year = {2021},
  pages={707-708},
  organization={IEEE}
} 
</div>
</ol>-->


<!--paper-->
<ol class="bibliography"><li>
<div id="hu21_fixationnet" class="margin-bottom-30">
<a href="./hu21_fixationnet.html">
<img class="thumbnail" src="./hu21_fixationnet/image/thumb.png" title="FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments" alt="FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments">
</a>
<p class="pub_title">FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
<a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>,
Sheng Li, Guoping Wang
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics, 2021, 27(5): 2681-2690.
</span>
</p>
<p class="pub_tags">        
<span class="button bg_greydark"><a id="pub_abstract_sh_hu21_fixationnet" class="pub_show" onclick="pub_showhide(&#39;hu21_fixationnet&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
<span class="button bg_greydark"><a id="pub_links_sh_hu21_fixationnet" class="pub_show" onclick="pub_showhide(&#39;hu21_fixationnet&#39;,&#39;pub_links&#39;)">Links</a></span>
<span class="button bg_greydark"><a id="pub_bibtex_sh_hu21_fixationnet" class="pub_show" onclick="pub_showhide(&#39;hu21_fixationnet&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
<span class="button bg_greydark margin-left-10"><a href="./hu21_fixationnet.html">Project</a></span>
<span class="pub_award"><svg class="svg-inline--fa fa-award fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="award" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M97.12 362.63c-8.69-8.69-4.16-6.24-25.12-11.85-9.51-2.55-17.87-7.45-25.43-13.32L1.2 448.7c-4.39 10.77 3.81 22.47 15.43 22.03l52.69-2.01L105.56 507c8 8.44 22.04 5.81 26.43-4.96l52.05-127.62c-10.84 6.04-22.87 9.58-35.31 9.58-19.5 0-37.82-7.59-51.61-21.37zM382.8 448.7l-45.37-111.24c-7.56 5.88-15.92 10.77-25.43 13.32-21.07 5.64-16.45 3.18-25.12 11.85-13.79 13.78-32.12 21.37-51.62 21.37-12.44 0-24.47-3.55-35.31-9.58L252 502.04c4.39 10.77 18.44 13.4 26.43 4.96l36.25-38.28 52.69 2.01c11.62.44 19.82-11.27 15.43-22.03zM263 340c15.28-15.55 17.03-14.21 38.79-20.14 13.89-3.79 24.75-14.84 28.47-28.98 7.48-28.4 5.54-24.97 25.95-45.75 10.17-10.35 14.14-25.44 10.42-39.58-7.47-28.38-7.48-24.42 0-52.83 3.72-14.14-.25-29.23-10.42-39.58-20.41-20.78-18.47-17.36-25.95-45.75-3.72-14.14-14.58-25.19-28.47-28.98-27.88-7.61-24.52-5.62-44.95-26.41-10.17-10.35-25-14.4-38.89-10.61-27.87 7.6-23.98 7.61-51.9 0-13.89-3.79-28.72.25-38.89 10.61-20.41 20.78-17.05 18.8-44.94 26.41-13.89 3.79-24.75 14.84-28.47 28.98-7.47 28.39-5.54 24.97-25.95 45.75-10.17 10.35-14.15 25.44-10.42 39.58 7.47 28.36 7.48 24.4 0 52.82-3.72 14.14.25 29.23 10.42 39.59 20.41 20.78 18.47 17.35 25.95 45.75 3.72 14.14 14.58 25.19 28.47 28.98C104.6 325.96 106.27 325 121 340c13.23 13.47 33.84 15.88 49.74 5.82a39.676 39.676 0 0 1 42.53 0c15.89 10.06 36.5 7.65 49.73-5.82zM97.66 175.96c0-53.03 42.24-96.02 94.34-96.02s94.34 42.99 94.34 96.02-42.24 96.02-94.34 96.02-94.34-42.99-94.34-96.02z"></path></svg> TVCG Best Journal Nominees Award</span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu21_fixationnet" style="display:none;">
Human visual attention in immersive virtual reality (VR) is key for many important applications, such as content design, gaze-contingent rendering, or gaze-based interaction.
However, prior works typically focused on free-viewing conditions that have limited relevance for practical applications.
We first collect eye tracking data of 27 participants performing a visual search task in four immersive VR environments.
Based on this dataset, we provide a comprehensive analysis of the collected data and reveal correlations between users' eye fixations and other factors, i.e. users' historical gaze positions, task-related objects, saliency information of the VR content, and users' head rotation velocities.
Based on this analysis, we propose <i>FixationNet</i> -- a novel learning-based model to forecast users' eye fixations in the near future in VR.
We evaluate the performance of our model for free-viewing and task-oriented settings and show that it outperforms the state of the art by a large margin of 19.8% (from a mean error of 2.93&deg to 2.35&deg) in free-viewing and of 15.1% (from 2.05&deg to 1.74&deg) in task-oriented situations.
As such, our work provides new insights into task-oriented attention in virtual environments and guides future work on this important topic in VR research.</div>
<div class="pub_links bg_grey" id="pub_links_hu21_fixationnet" style="display:none;">
<div class="pub_links">            
  <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2021.3067779" rel="nofollow" target="_blank">doi</a></p>
            
            
  <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./hu21_fixationnet/pdf/hu21_fixationnet.pdf">paper.pdf</a></p>



  <i class="fa fa-link"></i><p>Code: <a class="a-text-ext" href="https://github.com/CraneHzm/FixationNet" rel="nofollow" target="_blank">code</a></p>            

  <i class="fa fa-link"></i><p>Dataset: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FFixationNetDataset&ga=1" rel="nofollow" target="_blank">dataset</a></p>

  <i class="fa fa-file-pdf"></i>&nbsp;Slides: <a class="pub_list" href="./hu21_fixationnet/ppt/hu21_fixationnet.pdf">slides.pdf</a></p>

  <i class="fa fa-link"></i><p>Experimental Scenes: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FFixationNetScenes&ga=1" rel="nofollow" target="_blank">experimental senes</a></p>

  <i class="fa fa-link"></i><p>Supplemental materials: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FFixationNet%5FSupplementalMaterial%2Ezip&parent=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments&ga=1" rel="nofollow" target="_blank">supplemental materials</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu21_fixationnet" style="display:none;">
@article{hu21_fixationnet,
  title={FixationNet: Forecasting eye fixations in task-oriented virtual environments},
  author={Hu, Zhiming and Bulling, Andreas and Li, Sheng and Wang, Guoping},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  volume={27},
  number={5},
  pages={2681--2690},
  year={2021},
  publisher={IEEE}
}
</div>
</ol>


<!--paper
<ol class="bibliography"><li>
<div id="hu20_gaze" class="margin-bottom-30">
<a href="./hu20_gaze.html">
<img class="thumbnail" src="./hu20_gaze/image/thumb.png" title="Gaze Analysis and Prediction in Virtual Reality" alt="Gaze Analysis and Prediction in Virtual Reality">
</a>
<p class="pub_title">Gaze Analysis and Prediction in Virtual Reality</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">Proceedings of the 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, 2020: 543-544.
</span>
</p>
<p class="pub_tags">        
<span class="button bg_greydark"><a id="pub_abstract_sh_hu20_gaze" class="pub_show" onclick="pub_showhide(&#39;hu20_gaze&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
<span class="button bg_greydark"><a id="pub_links_sh_hu20_gaze" class="pub_show" onclick="pub_showhide(&#39;hu20_gaze&#39;,&#39;pub_links&#39;)">Links</a></span>
<span class="button bg_greydark"><a id="pub_bibtex_sh_hu20_gaze" class="pub_show" onclick="pub_showhide(&#39;hu20_gaze&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
<span class="button bg_greydark margin-left-10"><a href="./hu20_gaze.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu20_gaze" style="display:none;">
In virtual reality (VR) systems, users’ gaze information has gained importance in recent years. 
It can be applied to many aspects, including VR content design, eye-movement based interaction, gaze-contingent rendering, etc. 
In this context, it becomes increasingly important to understand users’ gaze behaviors in virtual reality and to predict users’ gaze positions. 
This paper presents research in gaze behavior analysis and gaze position prediction in virtual reality. 
Specifically, this paper focuses on static virtual scenes and dynamic virtual scenes under free-viewing conditions. 
Users’ gaze data in virtual scenes are collected and statistical analysis is performed on the recorded data. 
The analysis reveals that users’ gaze positions are correlated with their head rotation velocities and the salient regions of the content. 
In dynamic scenes, users’ gaze positions also have strong correlations with the positions of dynamic objects. 
A data-driven eye-head coordination model is proposed for realtime gaze prediction in static scenes and a CNN-based model is derived for predicting gaze positions in dynamic scenes.
</div>
<div class="pub_links bg_grey" id="pub_links_hu20_gaze" style="display:none;">
<div class="pub_links">            
	<svg class="svg-inline--fa fa-fingerprint fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="fingerprint" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M256.12 245.96c-13.25 0-24 10.74-24 24 1.14 72.25-8.14 141.9-27.7 211.55-2.73 9.72 2.15 30.49 23.12 30.49 10.48 0 20.11-6.92 23.09-17.52 13.53-47.91 31.04-125.41 29.48-224.52.01-13.25-10.73-24-23.99-24zm-.86-81.73C194 164.16 151.25 211.3 152.1 265.32c.75 47.94-3.75 95.91-13.37 142.55-2.69 12.98 5.67 25.69 18.64 28.36 13.05 2.67 25.67-5.66 28.36-18.64 10.34-50.09 15.17-101.58 14.37-153.02-.41-25.95 19.92-52.49 54.45-52.34 31.31.47 57.15 25.34 57.62 55.47.77 48.05-2.81 96.33-10.61 143.55-2.17 13.06 6.69 25.42 19.76 27.58 19.97 3.33 26.81-15.1 27.58-19.77 8.28-50.03 12.06-101.21 11.27-152.11-.88-55.8-47.94-101.88-104.91-102.72zm-110.69-19.78c-10.3-8.34-25.37-6.8-33.76 3.48-25.62 31.5-39.39 71.28-38.75 112 .59 37.58-2.47 75.27-9.11 112.05-2.34 13.05 6.31 25.53 19.36 27.89 20.11 3.5 27.07-14.81 27.89-19.36 7.19-39.84 10.5-80.66 9.86-121.33-.47-29.88 9.2-57.88 28-80.97 8.35-10.28 6.79-25.39-3.49-33.76zm109.47-62.33c-15.41-.41-30.87 1.44-45.78 4.97-12.89 3.06-20.87 15.98-17.83 28.89 3.06 12.89 16 20.83 28.89 17.83 11.05-2.61 22.47-3.77 34-3.69 75.43 1.13 137.73 61.5 138.88 134.58.59 37.88-1.28 76.11-5.58 113.63-1.5 13.17 7.95 25.08 21.11 26.58 16.72 1.95 25.51-11.88 26.58-21.11a929.06 929.06 0 0 0 5.89-119.85c-1.56-98.75-85.07-180.33-186.16-181.83zm252.07 121.45c-2.86-12.92-15.51-21.2-28.61-18.27-12.94 2.86-21.12 15.66-18.26 28.61 4.71 21.41 4.91 37.41 4.7 61.6-.11 13.27 10.55 24.09 23.8 24.2h.2c13.17 0 23.89-10.61 24-23.8.18-22.18.4-44.11-5.83-72.34zm-40.12-90.72C417.29 43.46 337.6 1.29 252.81.02 183.02-.82 118.47 24.91 70.46 72.94 24.09 119.37-.9 181.04.14 246.65l-.12 21.47c-.39 13.25 10.03 24.31 23.28 24.69.23.02.48.02.72.02 12.92 0 23.59-10.3 23.97-23.3l.16-23.64c-.83-52.5 19.16-101.86 56.28-139 38.76-38.8 91.34-59.67 147.68-58.86 69.45 1.03 134.73 35.56 174.62 92.39 7.61 10.86 22.56 13.45 33.42 5.86 10.84-7.62 13.46-22.59 5.84-33.43z"></path></svg><p>Doi: <a class="a-text-ext" href="https://doi.org/10.1109/vrw50115.2020.00123" rel="nofollow" target="_blank">doi</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu20_gaze/pdf/hu20_gaze.pdf">paper.pdf</a></p>                
  <i class="fa fa-file-powerpoint"></i>&nbsp; Slides: <a class="pub_list" href="./hu20_gaze/ppt/hu20_gaze.pdf">slides.pdf</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu20_gaze" style="display:none;">
@inproceedings{hu20_gaze, 
  author={Hu, Zhiming},
  title = {Gaze Analysis and Prediction in Virtual Reality}, 
  booktitle={Proceedings of the 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops},
  year = {2020},
  pages={543--544},
  organization={IEEE}
} 
</div>
</ol>-->


<!--paper-->
<ol class="bibliography"><li>
<div id="hu20_dgaze" class="margin-bottom-30">
<a href="./hu20_dgaze.html">
<img class="thumbnail" src="./hu20_dgaze/image/thumb.png" title="DGaze: CNN-Based Gaze Prediction in Dynamic Scenes" alt="DGaze: CNN-Based Gaze Prediction in Dynamic Scenes">
</a>	
<p class="pub_title">DGaze: CNN-Based Gaze Prediction in Dynamic Scenes</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>, 
Sheng Li,
<a class="a-int" href="https://cong-yi.github.io/">Congyi Zhang</a>,
Kangrui Yi, Guoping Wang,
<a class="a-int" href="https://www.cs.umd.edu/people/dmanocha">Dinesh Manocha</a>
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics, 2020, 26(5): 1902-1911.
</span>
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu20_dgaze" class="pub_show" onclick="pub_showhide(&#39;hu20_dgaze&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu20_dgaze" class="pub_show" onclick="pub_showhide(&#39;hu20_dgaze&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu20_dgaze" class="pub_show" onclick="pub_showhide(&#39;hu20_dgaze&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu20_dgaze.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu20_dgaze" style="display:none;">
We conduct novel analyses of users' gaze behaviors in dynamic virtual scenes and, based on our analyses, we present a novel CNN-based model called DGaze for gaze prediction in HMD-based applications. 
We first collect 43 users' eye tracking data in 5 dynamic scenes under free-viewing conditions. 
Next, we perform statistical analysis of our data and observe that dynamic object positions, head rotation velocities, and salient regions are correlated with users' gaze positions. 
Based on our analysis, we present a CNN-based model (DGaze) that combines object position sequence, head velocity sequence, and saliency features to predict users' gaze positions. 
Our model can be applied to predict not only realtime gaze positions but also gaze positions in the near future and can achieve better performance than prior method. 
In terms of realtime prediction, DGaze achieves a 22.0% improvement over prior method in dynamic scenes and obtains an improvement of 9.5% in static scenes, based on using the angular distance as the evaluation metric. 
We also propose a variant of our model called DGaze_ET that can be used to predict future gaze positions with higher precision by combining accurate past gaze data gathered using an eye tracker. 
We further analyze our CNN architecture and verify the effectiveness of each component in our model. 
We apply DGaze to gaze-contingent rendering and a game, and also present the evaluation results from a user study.
</div>
<div class="pub_links bg_grey" id="pub_links_hu20_dgaze" style="display:none;">
<div class="pub_links">
  <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2020.2973473" rel="nofollow" target="_blank">doi</a></p>
            
            
  <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./hu20_dgaze/pdf/hu20_dgaze.pdf">paper.pdf</a></p>

  <i class="fa fa-link"></i><p>Code: <a class="a-text-ext" href="https://github.com/CraneHzm/DGaze" rel="nofollow" target="_blank">code</a></p>            

  <i class="fa fa-link"></i><p>Dataset: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FDGazeDataset&ga=1" rel="nofollow" target="_blank">dataset</a></p>

  <i class="fa fa-file-pdf"></i>&nbsp;Slides: <a class="pub_list" href="./hu20_dgaze/ppt/hu20_dgaze.pdf">slides.pdf</a></p>

  <i class="fa fa-link"></i><p>Experimental Scenes: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FDGazeScenes&ga=1" rel="nofollow" target="_blank">experimental scenes</a></p>

  <i class="fa fa-link"></i><p>Supplemental materials: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FDGaze%5FSupplementalMaterial%2Ezip&parent=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments&ga=1" rel="nofollow" target="_blank">supplemental materials</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu20_dgaze" style="display:none;">
@article{hu20_dgaze,
  title={DGaze: CNN-Based Gaze Prediction in Dynamic Scenes},
  author={Hu, Zhiming and Li, Sheng and Zhang, Congyi and Yi, Kangrui and Wang, Guoping and Manocha, Dinesh},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  volume={26},
  number={5},
  pages={1902--1911},
  year={2020},
  publisher={IEEE}
}
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu20_temporal" class="margin-bottom-30">
<a href="./hu20_temporal.html">
<img class="thumbnail" src="./hu20_temporal/image/thumb.png" title="Temporal continuity of visual attention for future gaze prediction in immersive virtual reality" alt="Temporal continuity of visual attention for future gaze prediction in immersive virtual reality">
</a>	
<p class="pub_title">Temporal continuity of visual attention for future gaze prediction in immersive virtual reality</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
Sheng Li, Meng Gai
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">Virtual Reality & Intelligent Hardware, 2020, 2(2): 142-152.
</span>
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu20_temporal" class="pub_show" onclick="pub_showhide(&#39;hu20_temporal&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu20_temporal" class="pub_show" onclick="pub_showhide(&#39;hu20_temporal&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu20_temporal" class="pub_show" onclick="pub_showhide(&#39;hu20_temporal&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu20_temporal.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu20_temporal" style="display:none;">
<b>Background</b> Eye tracking technology is receiving increased attention in the field of virtual reality. 
Specifically, future gaze prediction is crucial in pre-computation for many applications such as gaze-contingent rendering, advertisement placement, and content-based design. 
To explore future gaze prediction, it is necessary to analyze the temporal continuity of visual attention in immersive virtual reality. 
<b>Methods</b> In this paper, the concept of temporal continuity of visual attention is presented. 
Subsequently, an autocorrelation function method is proposed to evaluate the temporal continuity. 
Thereafter, the temporal continuity is analyzed in both free-viewing and task-oriented conditions. 
<b>Results</b> Specifically, in free-viewing conditions, the analysis of a free-viewing gaze dataset indicates that the temporal continuity performs well only within a short time interval. 
A task-oriented game scene condition was created and conducted to collect users' gaze data. 
An analysis of the collected gaze data finds the temporal continuity has a similar performance with that of the free-viewing conditions. 
Temporal continuity can be applied to future gaze prediction and if it is good, users' current gaze positions can be directly utilized to predict their gaze positions in the future. 
<b>Conclusions</b> The current gaze's future prediction performances are further evaluated in both free-viewing and task-oriented conditions and discover that the current gaze can be efficiently applied to the task of short-term future gaze prediction. 
The task of long-term gaze prediction still remains to be explored.
</div>
<div class="pub_links bg_grey" id="pub_links_hu20_temporal" style="display:none;">
<div class="pub_links">
	<svg class="svg-inline--fa fa-fingerprint fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="fingerprint" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M256.12 245.96c-13.25 0-24 10.74-24 24 1.14 72.25-8.14 141.9-27.7 211.55-2.73 9.72 2.15 30.49 23.12 30.49 10.48 0 20.11-6.92 23.09-17.52 13.53-47.91 31.04-125.41 29.48-224.52.01-13.25-10.73-24-23.99-24zm-.86-81.73C194 164.16 151.25 211.3 152.1 265.32c.75 47.94-3.75 95.91-13.37 142.55-2.69 12.98 5.67 25.69 18.64 28.36 13.05 2.67 25.67-5.66 28.36-18.64 10.34-50.09 15.17-101.58 14.37-153.02-.41-25.95 19.92-52.49 54.45-52.34 31.31.47 57.15 25.34 57.62 55.47.77 48.05-2.81 96.33-10.61 143.55-2.17 13.06 6.69 25.42 19.76 27.58 19.97 3.33 26.81-15.1 27.58-19.77 8.28-50.03 12.06-101.21 11.27-152.11-.88-55.8-47.94-101.88-104.91-102.72zm-110.69-19.78c-10.3-8.34-25.37-6.8-33.76 3.48-25.62 31.5-39.39 71.28-38.75 112 .59 37.58-2.47 75.27-9.11 112.05-2.34 13.05 6.31 25.53 19.36 27.89 20.11 3.5 27.07-14.81 27.89-19.36 7.19-39.84 10.5-80.66 9.86-121.33-.47-29.88 9.2-57.88 28-80.97 8.35-10.28 6.79-25.39-3.49-33.76zm109.47-62.33c-15.41-.41-30.87 1.44-45.78 4.97-12.89 3.06-20.87 15.98-17.83 28.89 3.06 12.89 16 20.83 28.89 17.83 11.05-2.61 22.47-3.77 34-3.69 75.43 1.13 137.73 61.5 138.88 134.58.59 37.88-1.28 76.11-5.58 113.63-1.5 13.17 7.95 25.08 21.11 26.58 16.72 1.95 25.51-11.88 26.58-21.11a929.06 929.06 0 0 0 5.89-119.85c-1.56-98.75-85.07-180.33-186.16-181.83zm252.07 121.45c-2.86-12.92-15.51-21.2-28.61-18.27-12.94 2.86-21.12 15.66-18.26 28.61 4.71 21.41 4.91 37.41 4.7 61.6-.11 13.27 10.55 24.09 23.8 24.2h.2c13.17 0 23.89-10.61 24-23.8.18-22.18.4-44.11-5.83-72.34zm-40.12-90.72C417.29 43.46 337.6 1.29 252.81.02 183.02-.82 118.47 24.91 70.46 72.94 24.09 119.37-.9 181.04.14 246.65l-.12 21.47c-.39 13.25 10.03 24.31 23.28 24.69.23.02.48.02.72.02 12.92 0 23.59-10.3 23.97-23.3l.16-23.64c-.83-52.5 19.16-101.86 56.28-139 38.76-38.8 91.34-59.67 147.68-58.86 69.45 1.03 134.73 35.56 174.62 92.39 7.61 10.86 22.56 13.45 33.42 5.86 10.84-7.62 13.46-22.59 5.84-33.43z"></path></svg><!-- <i class="fa fa-fingerprint"></i> --><p>Doi: <a class="a-text-ext" href="https://doi.org/10.1016/j.vrih.2020.01.002" rel="nofollow" target="_blank">doi</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><!-- <i class="fa fa-file-pdf"></i> --><p>Paper: <a class="pub_list" href="./hu20_temporal/pdf/hu20_temporal.pdf">paper.pdf</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu20_temporal" style="display:none;">
@article{hu20_temporal,
  title={Temporal continuity of visual attention for future gaze prediction in immersive virtual reality},
  author={Hu, Zhiming and Li, Sheng and Gai, Meng},
  journal={Virtual Reality and Intelligent Hardware},
  volume={2},
  number={2},
  pages={142--152},
  year={2020},
  publisher={Elsevier}
}
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu19_sgaze" class="margin-bottom-30">
<a href="./hu19_sgaze.html">
<img class="thumbnail" src="./hu19_sgaze/image/thumb.png" title="SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction" alt="SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction">
</a>	
<p class="pub_title">SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
<a class="a-int" href="https://cong-yi.github.io/">Congyi Zhang</a>,
Sheng Li, Guoping Wang,
<a class="a-int" href="https://www.cs.umd.edu/people/dmanocha">Dinesh Manocha</a>
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics, 2019, 25(5): 2002-2010.
</span>
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu19_sgaze" class="pub_show" onclick="pub_showhide(&#39;hu19_sgaze&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu19_sgaze" class="pub_show" onclick="pub_showhide(&#39;hu19_sgaze&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu19_sgaze" class="pub_show" onclick="pub_showhide(&#39;hu19_sgaze&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu19_sgaze.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu19_sgaze" style="display:none;">
We present a novel, data-driven eye-head coordination model that can be used for realtime gaze prediction for immersive HMD-based applications without any external hardware or eye tracker. 
Our model (SGaze) is computed by generating a large dataset that corresponds to different users navigating in virtual worlds with different lighting conditions. 
We perform statistical analysis on the recorded data and observe a linear correlation between gaze positions and head rotation angular velocities. 
We also find that there exists a latency between eye movements and head movements. 
SGaze can work as a software-based realtime gaze predictor and we formulate a time related function between head movement and eye movement and use that for realtime gaze position prediction. 
We demonstrate the benefits of SGaze for gaze-contingent rendering and evaluate the results with a user study.
</div>
<div class="pub_links bg_grey" id="pub_links_hu19_sgaze" style="display:none;">
<div class="pub_links">
  <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2019.2899187" rel="nofollow" target="_blank">doi</a></p>
            
            
  <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./hu19_sgaze/pdf/hu19_sgaze.pdf">paper.pdf</a></p>

  <i class="fa fa-database"></i>&nbsp;&nbsp;Dataset: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FSGaze%5FDataset&ga=1">dataset</a></p>

  <i class="fa fa-code"></i>&nbsp;Code: <a class="a-text-ext" href="https://github.com/CraneHzm/SGaze">code</a></p>

  <i class="fa fa-link"></i><p>Supplemental materials: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FSGaze%5FSupplementalMaterial%2Ezip&parent=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments&ga=1" rel="nofollow" target="_blank">supplemental materials</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu19_sgaze" style="display:none;">
@article{hu19_sgaze,
  title={SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction},
  author={Hu, Zhiming and Zhang, Congyi and Li, Sheng and Wang, Guoping and Manocha, Dinesh},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  volume={25},
  number={5},
  pages={2002--2010},
  year={2019},
  publisher={IEEE}
}
</div>
</ol>


<hr>
<h3>Personal Blogs</h3>
All blogs are written in Chinese.
<ul>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483824&idx=1&sn=5f2b32fca7e147844de53301ac197f61&chksm=ebbedf73dcc956654ac7fb23046e7731568bd92f513495110f0fe37a708fe443324e86b190b8&token=781182399&lang=zh_CN#rd" title=""> 聊一聊“卷”和“同辈压力”
</a>, 2023</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483818&idx=1&sn=8fc606f7a13ee74c3fbea2c0399d5cbd&chksm=ebbedf69dcc9567fc80a8edb0ee86509c1d3aeba6ab13f065023ac2c5078a5c4b7e80f171ce5&token=781182399&lang=zh_CN#rd" title=""> 从博士到博士后</a>, 2022</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483762&idx=1&sn=11b9a80092adfc95abb31755b6e91d4a&chksm=ebbedfb1dcc956a7a3fa8d427978280816689802f28f6e0492fdde811bf8c0663a12afa96127&token=781182399&lang=zh_CN#rd" title=""> 写在博士生涯的最后</a>, 2022</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483752&idx=1&sn=9eea860e76a73516e5ee694e909c9142&chksm=ebbedfabdcc956bd473ce16ec52f29235d62334192a583358f7c330d544038a3926512256b26&token=781182399&lang=zh_CN#rd" title=""> 学了三个专业之后</a>, 2021</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483736&idx=1&sn=a6f827edf5156e82bcb571198372772b&chksm=ebbedf9bdcc9568d8e026b752d0e1f140c5803f67f9b94b74a879c0207371e366e0840a6a599&token=781182399&lang=zh_CN#rd" title=""> Research，还真是有趣呢~</a>, 2020</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483720&idx=1&sn=267485316bb6990035348ea618b6ec26&chksm=ebbedf8bdcc9569d14cbf7ddf665e887151e25128b96a099b692f0bd4240fe9dac3d3d0b2a0c&token=781182399&lang=zh_CN#rd" title=""> 科研路上的一颗糖</a>, 2019</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483701&idx=1&sn=bf87eda79bb28dcf6680196a18e14c24&chksm=ebbedff6dcc956e0e7921946011b0a2acfc34e357889d97e5b62aa6cfbc0743e31c09c285173&token=781182399&lang=zh_CN#rd" title=""> 博士第一年：有发堪学直须学</a>, 2018</li>
</ul>

</dl>
</dl>
</dl>
</dl>
</dl>
</div>
</div>

</div>

<!-- footer -->
<div id="footer" class="footer-v1">
<div class="copyright custom-copyright">
<div class="container">
<div class="row">
<div class="col-md-6">
<p>
<span class="custom-copyright-container">
	Last modified: 13/08/2023
</span>
</p>
</div>
</div>
</div>
</div>
</div>

</body></html>