<!DOCTYPE html>
<html lang="en" class="fontawesome-i2svg-active fontawesome-i2svg-complete"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">        
<title>Zhiming Hu - Homepage</title>        
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" media="all" href="./index/css/main_v2.css">
<link rel="stylesheet" media="all" href="./index/css/owl.carousel.min.css">
<link rel="stylesheet" media="all" href="./index/css/owl.theme.default.min.css">
<link rel="stylesheet" media="all" href="./index/css/jquery-ui.min.css">
<link rel="stylesheet" href="./index/css/swipe.css">
<script src="./index/js/jquery-3.4.1.min.js"></script>
<script src="./index/js/jquery-ui.min.js"></script>
<script src="./index/js/fontawesome-5.11.2.js"></script>
<script src="./index/js/main.js"></script>
<script src="./index/js/owl.carousel.min.js"></script>
<script src="./index/js/rot13.js"></script>
<script src="./index/js/script.js"></script>
<script src="https://www.w3counter.com/tracker.js?id=150008"></script>
</head>

<body class="header header-location">
<div class="wrapper">
<!-- header -->
<div id="unstickyheader" class="unstickyheader">
<div class="topbar">
<div class="container">
<div class="header-image-container">
	<!--<a href=""><img class="header-image" src="./index/" alt=""></a>-->
</div>
<div class="header-image-container-right">
	<!--a href=""><img class="header-image" src="./index/image/" alt=""></a>-->
</div>
</div>
</div>
</div>
</div>

<!-- content -->
<div class="content">
<div class="section margin-top-20 margin-bottom-20">
<div class="container">

<div class="person">
  <img class="personpic" title="Zhiming Hu" src="./index/image/hu.png" alt="Zhiming Hu">

  
<div class="personinfo">
<h3>Zhiming Hu</h3>

<p><a class="a-text-ext" href="https://www.simtech.uni-stuttgart.de/" title="">Stuttgart Center for Simulation Science (SimTech)</a>, <a class="a-text-ext" href="https://www.uni-stuttgart.de/" target="_blank" title="University of Stuttgart" rel="nofollow">University of Stuttgart</a></p>
<p>
	<svg class="svg-inline--fa fa-envelope fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="envelope" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg><!-- <i class="fa fa-envelope"></i> --><span>zhiming.hu (at) simtech.uni-stuttgart.de</span>
	<!--<svg class="svg-inline--fa fa-phone fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="phone" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M493.4 24.6l-104-24c-11.3-2.6-22.9 3.3-27.5 13.9l-48 112c-4.2 9.8-1.4 21.3 6.9 28l60.6 49.6c-36 76.7-98.9 140.5-177.2 177.2l-49.6-60.6c-6.8-8.3-18.2-11.1-28-6.9l-112 48C3.9 366.5-2 378.1.6 389.4l24 104C27.1 504.2 36.7 512 48 512c256.1 0 464-207.5 464-464 0-11.2-7.7-20.9-18.6-23.4z"></path></svg><span><a class="a-text-ext" href="https://scholar.google.com/citations?user=OLB_xBEAAAAJ" title="Google Scholar">Google Scholar</a>, <a class="a-text-ext" href="https://github.com/cranehzm" title="Github">Github</a>, <a class="a-text-ext" href="./CurriculumVitae/Curriculum_Vitae_Zhiming_Hu.pdf" title="Curriculum Vitae">Curriculum Vitae</a>, <a class="a-text-ext" href="https://mp.weixin.qq.com/s/7uPr0SxNhLA58K97osda-Q" title="WeChat Official Account">WeChat Official Account</a></span>-->
	<svg class="svg-inline--fa fa-map-marker-alt fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="map-marker-alt" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M172.268 501.67C26.97 291.031 0 269.413 0 192 0 85.961 85.961 0 192 0s192 85.961 192 192c0 77.413-26.97 99.031-172.268 309.67-9.535 13.774-29.93 13.773-39.464 0zM192 272c44.183 0 80-35.817 80-80s-35.817-80-80-80-80 35.817-80 80 35.817 80 80 80z"></path></svg><!-- <i class="fa fa-map-marker-alt"></i> --><span><a href="https://osm.org/go/0Dk7mAUtE?node=1568673162" rel="nofollow" target="_blank" class="a-text-ext">Pfaffenwaldring 5a, 70569 Stuttgart, Germany</a></span>
	<svg class="svg-inline--fa fa-door-open fa-w-20" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="door-open" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" data-fa-i2svg=""><path fill="currentColor" d="M624 448h-80V113.45C544 86.19 522.47 64 496 64H384v64h96v384h144c8.84 0 16-7.16 16-16v-32c0-8.84-7.16-16-16-16zM312.24 1.01l-192 49.74C105.99 54.44 96 67.7 96 82.92V448H16c-8.84 0-16 7.16-16 16v32c0 8.84 7.16 16 16 16h336V33.18c0-21.58-19.56-37.41-39.76-32.17zM264 288c-13.25 0-24-14.33-24-32s10.75-32 24-32 24 14.33 24 32-10.75 32-24 32z"></path></svg><!-- <i class="fa fa-door-open"></i> --><span>University of Stuttgart, SimTech Building</span>		
	<svg class="svg-inline--fa fa-graduation-cap fa-w-20" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="graduation-cap" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" data-fa-i2svg=""><path fill="currentColor" d="M622.34 153.2L343.4 67.5c-15.2-4.67-31.6-4.67-46.79 0L17.66 153.2c-23.54 7.23-23.54 38.36 0 45.59l48.63 14.94c-10.67 13.19-17.23 29.28-17.88 46.9C38.78 266.15 32 276.11 32 288c0 10.78 5.68 19.85 13.86 25.65L20.33 428.53C18.11 438.52 25.71 448 35.94 448h56.11c10.24 0 17.84-9.48 15.62-19.47L82.14 313.65C90.32 307.85 96 298.78 96 288c0-11.57-6.47-21.25-15.66-26.87.76-15.02 8.44-28.3 20.69-36.72L296.6 284.5c9.06 2.78 26.44 6.25 46.79 0l278.95-85.7c23.55-7.24 23.55-38.36 0-45.6zM352.79 315.09c-28.53 8.76-52.84 3.92-65.59 0l-145.02-44.55L128 384c0 35.35 85.96 64 192 64s192-28.65 192-64l-14.18-113.47-145.03 44.56z"></path></svg><!-- <i class="fa fa-graduation-cap"></i> --><span><a class="a-text-ext" href="https://scholar.google.com/citations?user=OLB_xBEAAAAJ" title="google scholar">google scholar</a>, <a class="a-text-ext" href="https://github.com/cranehzm" title="github">github</a>, <a class="a-text-ext" href="./curriculum vitae/curriculum_vitae_English.pdf" title="curriculum vitae">curriculum vitae (English)</a>,<a class="a-text-ext" href="./curriculum vitae/curriculum_vitae_Chinese.pdf" title="curriculum vitae">curriculum vitae (Chinese)</a></span>
</p>
</div>
</div>
<div class="clearfix"></div>

<hr>
<h3>Short Bio</h3>
<p>Zhiming Hu is a post-doctoral researcher in the <a class="a-text-ext" href="https://www.perceptualui.org/" title="">Perceptual User Interfaces Group</a> led by <a class="a-text-ext" href="https://www.perceptualui.org/people/bulling/" title="">Prof. Andreas Bulling</a> and the <a class="a-text-ext" href="https://www.imsb.uni-stuttgart.de/" title="">Computational Biophysics and Biorobotics Group</a> led by <a class="a-text-ext" href="https://www.imsb.uni-stuttgart.de/team/Schmitt-00006/" title="">Prof. Syn Schmitt</a>, in the <b>University of Stuttgart</b>, Germany since August 2022. He obtained his Ph.D. degree in Computer Software and Theory from <b>Peking University</b>, China in 2022, supervised by <a class="a-text-ext" href="https://www.graphics.pku.edu.cn/xztd/jgfaculty/wgp2/index.htm#EN_intro" title="">Prof. Guoping Wang</a>. He received his Bachelor's degree in Optical Engineering from <b>Beijing Institute of Technology</b>, China in 2017. He has published more than 10 papers at top venues in VR/AR and HCI, including IEEE VR, ISMAR, TVCG, CHI, UIST, and IROS. His work has been nominated for TVCG best journal award at IEEE VR 2021 as well as for best doctoral student paper award at INTERACT 2023. He served as a reviewer for many top venues, including SIGGRAPH, CVPR, ICCV, ECCV, CHI, UIST, IEEE VR, ISMAR, IMWUT, TMM, TVCG, and IJHCI.
</p>


<hr>
<h3>Latest News</h3>
<ul>
<li>[06.2024] <b>One paper is accepted at IROS 2024 as an oral presentation. </b></li>
<li>[06.2024] <b>One paper is conditionally accepted at UIST 2024.</b></li>
<li>[05.2024] I will serve as an Associate Chair for MuC 2024.</li>
<li>[05.2024] <b>One paper is conditionally accepted at ISMAR 2024 TVCG-track.</b></li>
<li>[05.2024] <b>One paper is accepted at TVCG 2024.</b></li>
<li>[03.2024] <b>Two papers are accepted at ETRA 2024.</b></li>
<li>[01.2024] <b>Two papers are accepted at CHI 2024.</b></li>
<li>[12.2023] I was invited to attend the 11th Chengyao Youth Forum organised by Nanjing University.</li>
<li>[12.2023] I was invited to be an international program committee for PETMEI 2024. </li>
<li>[12.2023] I was invited to attend the Fifth Youth Forum on the Next Generation Computer Sciences organised by Peking University. </li>
<li>[11.2023] I was invited to attend the 10th Teli Forum organised by Beijing Institute of Technology. </li>
<li>[09.2023] <b>Our paper was nominated for IFIP TC13 Pioneers’ Award for Best Doctoral Student Paper at INTERACT 2023.</b></li>
<li>[08.2023] I will serve as the Virtualization Chair for ETRA 2024.</li>
<li>[08.2023] <b>One paper is accepted at UIST 2023.</b></li>
<li>[06.2023] I will serve as an Associate Chair for MuC 2023.</li>
<li>[06.2023] One paper is accepted at INTERACT 2023.</li>
<li>[05.2023] I will serve as a Technical Program Committee member for iWOAR 2023.</li>
<li>[10.2022] One paper is accepted at NeurIPS 2022 Workshop Gaze Meets ML.</li>
<li>[08.2022] I joined the University of Stuttgart as a post-doctoral researcher.</li>
<li>[07.2022] <b>I successfully defended my Ph.D.!</b></li>
</ul>


<hr>
<h3>Research Interests</h3>
<p>My research interests include virtual reality, human-computer interaction, eye tracking, and human-centred artificial intelligence.
The long-term research goal is to develop a human-centred intelligent interactive system that can accurately model human behaviours, e.g. human eye movements and human body movements, in activities of daily living.
</p>

<hr>
<h3>Awards & Honours</h3>
<ul>
<li> Best Doctoral Student Paper Award Nominees at INTERACT 2023</li>
<li> SimTech Postdoctoral Fellowship, 2022</li>
<!--<li>Outstanding Graduates Prize in Peking University, 2022</li>-->
<li> National Scholarship (top 2%), 2021</li>
<!--<li> Merit Student Prize in Peking University, 2021</li>-->
<li> TVCG Best Journal Award Nominees at IEEE VR 2021 (top 2%, first time for Chinese researchers)</li>
<li> CSC (China Scholarship Council) Scholarship, 2020</li>
<!--<li> Second-Class Scholarship of Peking University, 2020</li>-->
<!--<li> Merit Student Prize in Peking University, 2020</li>-->
<li> Chancellor's Scholarship (top 2%), 2020</li>
<li> Leo KoGuan Scholarship (top 5%), 2019</li>
<!--<li> Hongcai Scholarship, 2019</li>-->
<!--<li> Merit Student Prize in Peking University, 2019</li>-->
<!--<li> Hongcai Scholarship, 2017</li>-->
<li> Leader Scholarship (top 0.2%, 7 out of over 3800 students), 2017</li>
<!--<li> Outstanding Graduates Prize in Beijing Institute of Technology, 2017</li>-->
<li> National Scholarship (top 2%), 2016</li>
<!--<li> Honorable Mention of Mathematical Contest in Modeling, 2016</li>-->
<!--<li> Merit Student Prize in Beijing Institute of Technology, 2016</li>-->
<!--<li> National Encouragement Scholarship, 2015</li>-->
<!--<li> Merit Student Prize in Beijing Institute of Technology, 2015</li>-->
<!--<li> Third Prize of the National College Students Composition Competition, 2015</li>-->
<!--<li> Pacemaker to Merit Student Prize in Beijing Institute of Technology, 2014</li>-->
<!--<li> Third Prize of Beijing Humanistic Knowledge Competition, 2014</li>-->
<li> National Scholarship (top 2%), 2014</li>
</ul>


<hr>
<h3>Professional Activities & Talks</h3>
<b>Reviewing</b>
<ul>
<li> Journals: IMWUT, TiiS, TMM, TVCG, IJHCI, MTAP, VR, BRM</li>
<li> Conferences: SIGGRAPH, CVPR, ICCV, ECCV, CHI, UIST, IEEE VR, ISMAR, ETRA</li>
</ul>

<b>Organizing Committee</b>
<ul>
<li> Associate Chair for MuC 2024</li>
<li> International Program Committee for PETMEI 2024</li>
<li> Virtualization Chair for ETRA 2024</li>
<li> Associate Chair for MuC 2023</li>
<li> Technical Program Committee member for iWOAR 2023</li>
</ul>


<b>Invited Talks</b>
<ul>
<li> <a class="a-text-ext" href="./talks/Towards_Human-centred_Artificial_Intelligence.pdf" title="">Towards Human-centred Artificial Intelligence.</a> Nanjing University 11th Chengyao Youth Forum, China, December, 2023.</li>
<li> <a class="a-text-ext" href="./talks/Towards_Human-aware_Intelligent_User_Interfaces.pdf" title="">Towards Human-aware Intelligent User Interfaces.</a> Peking University Fifth Youth Forum on the Next Generation Computer Sciences, China, December, 2023.</li>
<li> <a class="a-text-ext" href="./talks/Towards_the_Coordination_of_Eye_Body_and_Context_in_Daily_Activities.pdf" title="">Towards the Coordination of Eye, Body and Context in Daily Activities.</a> Beijing Institute of Technology 10th Teli Forum, China, Hosted by <a class="a-text-ext" href="https://cs.bit.edu.cn/szdw/jsml/gjjgccrc/wgr_9222aafdaa7c4daf94463e4136277e5b/index.htm" title="">Prof. Guoren Wang</a>, November, 2023.</li>
<li> <a class="a-text-ext" href="./talks/The_Coordination_of_Digital_Humans.pdf" title="">The Coordination of Digital Humans.</a> Peking University Career Talk on Computer Science, China, November, 2022.</li>
<li> <a class="a-text-ext" href="./talks/Analysis_and_Prediction_of_Human_Visual_Attention_in_Virtual_Reality.pdf" title="">Analysis and Prediction of Human Visual Attention in Virtual Reality.</a> Southeast University, China, Hosted by <a class="a-text-ext" href="https://dingdingseu.mystrikingly.com/" title="">Prof. Ding Ding</a>, June, 2022.</li>
<li> <a class="a-text-ext" href="./talks/Recognizing_User_Tasks_from_Eye_and_Head_Movements_in_Immersive_Virtual_Reality.pdf" title="">Recognizing User Tasks from Eye and Head Movements in Immersive Virtual Reality.</a> IEEE VR 2022, Hosted by <a class="a-text-ext" href="https://carelab.info/en/kiyoshi-kiyokawa/" title="">Prof. Kiyoshi Kiyokawa</a>, March, 2022.</li>
<li> <a class="a-text-ext" href="./talks/Forecasting_Eye_Fixations_in_Task-Oriented_Virtual_Environments.pdf" title="">Forecasting Eye Fixations in Task-Oriented Virtual Environments.</a> GAMES Webinar 2021, Hosted by <a class="a-text-ext" href="http://dalab.se.sjtu.edu.cn/www/home/?page_id=143" title="">Prof. Xubo Yang</a>, September, 2021.</li>
<li> <a class="a-text-ext" href="./talks/Eye-Head_Coordination_Model_for_Real-time_Gaze_Prediction.pdf" title="">Eye-Head Coordination Model for Real-time Gaze Prediction.</a> 2019 International Conference on VR/AR and 3D Display, Hosted by <a class="a-text-ext" href="http://xufeng.site/" title="">Prof. Feng Xu</a>, June 2019.</li>
</ul>


<hr>
<h3>Teaching</h3>
<ul>
<li> Machine Perception and Learning, University of Stuttgart, 2022, Lecturer</li>
<li> Computer Graphics, Peking University, 2018, Teaching Assistant</li>
<li> Image and Video-Based 3D Reconstruction, Peking University, 2018, Teaching Assistant</li>
<li> Programming Basics, Peking University, 2018, Teaching Assistant</li>
</ul>


<hr>
<h3>Publications</h3>
* Corresponding author


<!--paper-->
<ol class="bibliography"><li>
<div id="hu24_gazemotion" class="margin-bottom-30">
<a href="./hu24_gazemotion.html">
<img class="thumbnail" src="./hu24_gazemotion/image/thumb.png" title="GazeMotion: Gaze-guided Human Motion Forecasting" alt="GazeMotion: Gaze-guided Human Motion Forecasting">
</a>	
<p class="pub_title">GazeMotion: Gaze-guided Human Motion Forecasting</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>, <a class="a-int" href="https://www.imsb.uni-stuttgart.de/team/Schmitt-00006/">Syn Schmitt</a>, <a class="a-int" href="https://www.hih-tuebingen.de/forschung/unabhaengige-forschungsgruppen/mocom/">Daniel Haeufle</a>, 
<a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
</p>
<p class="pub_additional">
<span class="pub_additional_journal">Proceedings of the 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2024.</span>

</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu24_gazemotion" class="pub_show" onclick="pub_showhide(&#39;hu24_gazemotion&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu24_gazemotion" class="pub_show" onclick="pub_showhide(&#39;hu24_gazemotion&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu24_gazemotion" class="pub_show" onclick="pub_showhide(&#39;hu24_gazemotion&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu24_gazemotion.html">Project</a></span>
	<span class="pub_award"><svg class="svg-inline--fa fa-award fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="award" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M97.12 362.63c-8.69-8.69-4.16-6.24-25.12-11.85-9.51-2.55-17.87-7.45-25.43-13.32L1.2 448.7c-4.39 10.77 3.81 22.47 15.43 22.03l52.69-2.01L105.56 507c8 8.44 22.04 5.81 26.43-4.96l52.05-127.62c-10.84 6.04-22.87 9.58-35.31 9.58-19.5 0-37.82-7.59-51.61-21.37zM382.8 448.7l-45.37-111.24c-7.56 5.88-15.92 10.77-25.43 13.32-21.07 5.64-16.45 3.18-25.12 11.85-13.79 13.78-32.12 21.37-51.62 21.37-12.44 0-24.47-3.55-35.31-9.58L252 502.04c4.39 10.77 18.44 13.4 26.43 4.96l36.25-38.28 52.69 2.01c11.62.44 19.82-11.27 15.43-22.03zM263 340c15.28-15.55 17.03-14.21 38.79-20.14 13.89-3.79 24.75-14.84 28.47-28.98 7.48-28.4 5.54-24.97 25.95-45.75 10.17-10.35 14.14-25.44 10.42-39.58-7.47-28.38-7.48-24.42 0-52.83 3.72-14.14-.25-29.23-10.42-39.58-20.41-20.78-18.47-17.36-25.95-45.75-3.72-14.14-14.58-25.19-28.47-28.98-27.88-7.61-24.52-5.62-44.95-26.41-10.17-10.35-25-14.4-38.89-10.61-27.87 7.6-23.98 7.61-51.9 0-13.89-3.79-28.72.25-38.89 10.61-20.41 20.78-17.05 18.8-44.94 26.41-13.89 3.79-24.75 14.84-28.47 28.98-7.47 28.39-5.54 24.97-25.95 45.75-10.17 10.35-14.15 25.44-10.42 39.58 7.47 28.36 7.48 24.4 0 52.82-3.72 14.14.25 29.23 10.42 39.59 20.41 20.78 18.47 17.35 25.95 45.75 3.72 14.14 14.58 25.19 28.47 28.98C104.6 325.96 106.27 325 121 340c13.23 13.47 33.84 15.88 49.74 5.82a39.676 39.676 0 0 1 42.53 0c15.89 10.06 36.5 7.65 49.73-5.82zM97.66 175.96c0-53.03 42.24-96.02 94.34-96.02s94.34 42.99 94.34 96.02-42.24 96.02-94.34 96.02-94.34-42.99-94.34-96.02z"></path></svg> Oral Presentation</span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu24_gazemotion" style="display:none;">
We present GazeMotion – a novel method for human motion forecasting that combines information on past human poses with human eye gaze. Inspired by evidence from behavioural sciences showing that human eye and body movements are closely coordinated, GazeMotion first predicts future eye gaze from past gaze, then fuses predicted future gaze and past poses into a gaze-pose graph, and finally uses a residual graph convolutional network to forecast body motion. We extensively evaluate our method on the MoGaze, ADT, and GIMO benchmark datasets and show that it outperforms state-of-the-art methods by up to 7.4% improvement in mean per joint position error. Using head direction as a proxy to gaze, our method still achieves an average improvement of 5.5%. We finally report an online user study showing that our method also outperforms prior methods in terms of perceived realism. These results show the significant information content available in eye gaze for human motion forecasting as well as the effectiveness of our method in exploiting this information.
</div>
<div class="pub_links bg_grey" id="pub_links_hu24_gazemotion" style="display:none;">
<div class="pub_links">
  <!--<i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2021.3138902" rel="nofollow" target="_blank">doi</a></p>-->
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu24_gazemotion/pdf/hu24_gazemotion.pdf">paper.pdf</a></p>
	
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu24_gazemotion" style="display:none;">@inproceedings{hu24_gazemotion,
	title={GazeMotion: Gaze-guided Human Motion Forecasting},
	author={Hu, Zhiming and Schmitt, Syn and Haeufle, Daniel and Bulling, Andreas},
	booktitle={Proceedings of the 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems},	
	year={2024}}
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu24_hoimotion" class="margin-bottom-30">
<a href="./hu24_hoimotion.html">
<img class="thumbnail" src="./hu24_hoimotion/image/thumb.png" title="HOIMotion: Forecasting Human Motion During Human-Object Interactions Using Egocentric 3D Object Bounding Boxes" alt="HOIMotion: Forecasting Human Motion During Human-Object
Interactions Using Egocentric 3D Object Bounding Boxes">
</a>	
<p class="pub_title">HOIMotion: Forecasting Human Motion During Human-Object
Interactions Using Egocentric 3D Object Bounding Boxes</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>, Zheming Yin, <a class="a-int" href="https://www.hih-tuebingen.de/forschung/unabhaengige-forschungsgruppen/mocom/">Daniel Haeufle</a>, 
<a class="a-int" href="https://www.imsb.uni-stuttgart.de/team/Schmitt-00006/">Syn Schmitt</a>,
<a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
</p>
<p class="pub_additional">
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics (TVCG, ISMAR 2024 Journal-track), 2024. </span>

</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu24_hoimotion" class="pub_show" onclick="pub_showhide(&#39;hu24_hoimotion&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu24_hoimotion" class="pub_show" onclick="pub_showhide(&#39;hu24_hoimotion&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu24_hoimotion" class="pub_show" onclick="pub_showhide(&#39;hu24_hoimotion&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu24_hoimotion.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu24_hoimotion" style="display:none;">
We present HOIMotion – a novel approach for human motion forecasting during human-object interactions that integrates information about past body poses and egocentric 3D object bounding boxes. Human motion forecasting is important in many augmented reality applications but most existing methods have only used past body poses to predict future motion. HOIMotion first uses an encoder-residual graph convolutional network (GCN) and multi-layer perceptrons to extract features from body poses and egocentric 3D object bounding boxes, respectively. Our method then fuses pose and object features into a novel pose-object graph and uses a residual-decoder GCN to forecast future body motion. We extensively evaluate our method on the Aria digital twin (ADT) and MoGaze datasets and show that HOIMotion consistently outperforms state-of-the-art methods by a large margin of up to 8.7% on ADT and 7.2% on MoGaze in terms of mean per joint position error. Complementing these evaluations, we report a human study (N=20) that shows that the improvements achieved by our method result in forecasted poses being perceived as both more precise and more realistic than those of existing methods. Taken together, these results reveal the significant information content available in egocentric 3D object bounding boxes for human motion forecasting and the effectiveness of our method in exploiting this information.
</div>
<div class="pub_links bg_grey" id="pub_links_hu24_hoimotion" style="display:none;">
<div class="pub_links">
  <!--<i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2021.3138902" rel="nofollow" target="_blank">doi</a></p>-->
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu24_hoimotion/pdf/hu24_hoimotion.pdf">paper.pdf</a></p>
	
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu24_hoimotion" style="display:none;">@article{hu24_hoimotion,
	author={Hu, Zhiming and Yin, Zheming and Haeufle, Daniel and Schmitt, Syn and Bulling, Andreas},
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={HOIMotion: Forecasting Human Motion During Human-Object Interactions Using Egocentric 3D Object Bounding Boxes}, 
	year={2024}}
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu24_pose2gaze" class="margin-bottom-30">
<a href="./hu24_pose2gaze.html">
<img class="thumbnail" src="./hu24_pose2gaze/image/thumb.png" title="Pose2Gaze: Eye-body Coordination during Daily Activities for Gaze Prediction from Full-body Poses" alt="Pose2Gaze: Eye-body Coordination during Daily Activities for Gaze Prediction from Full-body Poses">
</a>	
<p class="pub_title">Pose2Gaze: Eye-body Coordination during Daily Activities for Gaze Prediction from Full-body Poses</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>, Jiahui Xu, 
<a class="a-int" href="https://www.imsb.uni-stuttgart.de/team/Schmitt-00006/">Syn Schmitt</a>,
<a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics (TVCG), 2024.
</span>
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu24_pose2gaze" class="pub_show" onclick="pub_showhide(&#39;hu24_pose2gaze&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu24_pose2gaze" class="pub_show" onclick="pub_showhide(&#39;hu24_pose2gaze&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu24_pose2gaze" class="pub_show" onclick="pub_showhide(&#39;hu24_pose2gaze&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu24_pose2gaze.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu24_pose2gaze" style="display:none;">
Human eye gaze plays a significant role in many virtual and augmented reality (VR/AR) applications, such as gaze-contingent rendering, gaze-based interaction, or eye-based activity recognition. However, prior works on gaze analysis and prediction have only explored eye-head coordination and were limited to human-object interactions. We first report a comprehensive analysis of eye-body coordination in various human-object and human-human interaction activities based on four public datasets collected in real-world (MoGaze), VR (ADT), as well as AR (GIMO and EgoBody) environments. We show that in human-object interactions, e.g. pick and place, eye gaze exhibits strong correlations with full-body motion while in human-human interactions, e.g. chat and teach, a person’s gaze direction is correlated with the body orientation towards the interaction partner. Informed by these analyses we then present Pose2Gaze – a novel eye-body coordination model that uses a convolutional neural network and a spatio-temporal graph convolutional neural network to extract features from head direction and full-body poses, respectively, and then uses a convolutional neural network to predict eye gaze. We compare our method with state-of-the-art methods that predict eye gaze only from head movements and show that Pose2Gaze outperforms these baselines with an average improvement of 24.0% on MoGaze, 10.1% on ADT, 21.3% on GIMO, and 28.6% on EgoBody in mean angular error, respectively. We also show that our method significantly outperforms prior methods in the sample downstream task of eye-based activity recognition. These results underline the significant information content available in eye-body coordination during daily activities and open up a new direction for gaze prediction.
</div>
<div class="pub_links bg_grey" id="pub_links_hu24_pose2gaze" style="display:none;">
<div class="pub_links">
  <!--<i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2021.3138902" rel="nofollow" target="_blank">doi</a></p>-->
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu24_pose2gaze/pdf/hu24_pose2gaze.pdf">paper.pdf</a></p>
  
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu24_pose2gaze" style="display:none;">@article{hu24_pose2gaze,
	author={Hu, Zhiming and Xu, Jiahui and Schmitt, Syn and Bulling, Andreas},
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={Pose2Gaze: Eye-body Coordination during Daily Activities for Gaze Prediction from Full-body Poses}, 
	year={2024}}  
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="wang24_visrecall" class="margin-bottom-30">
        <a href="./wang24_visrecall.html">
        <img class="thumbnail" src="./wang24_visrecall/image/thumb.png" title="VisRecall++: Analysing and Predicting Visualisation Recallability from Gaze Behaviour" alt="VisRecall++: Analysing and Predicting Visualisation Recallability from Gaze Behaviour">
        </a>
    
    <p class="pub_title">VisRecall++: Analysing and Predicting Visualisation Recallability from Gaze Behaviour</p>

    <p class="pub_author">	
            <a class="a-int" href="https://www.perceptualui.org/people/wang/">Yao Wang</a>, 
			<a class="a-int" href="https://yuejiang-nj.github.io/">Yue Jiang</a>, 
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,    
			<a class="a-int" href="https://www.perceptualui.org/people/ruhdorfer/">Constantin Ruhdorfer</a>,
			<a class="a-int" href="https://www.perceptualui.org/people/bace/">Mihai Bâce</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
    </p>
	
    <p class="pub_additional">
        
            <span class="pub_additional_journal">Proc. ACM on Human-Computer Interaction (PACM HCI), 8 (ETRA),</span>        
            <span class="pub_additional_pages">pp. 1–18, </span>
        
        <span class="pub_additional_year">2024</span>.
    </p>

    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_wang24_visrecall" class="pub_show" onclick="pub_showhide('wang24_visrecall','pub_abstract')">Abstract</a></span>
                
        <span class="button bg_greydark"><a id="pub_links_sh_wang24_visrecall" class="pub_show" onclick="pub_showhide('wang24_visrecall','pub_links')">Links</a></span>
        
        <span class="button bg_greydark"><a id="pub_bibtex_sh_wang24_visrecall" class="pub_show" onclick="pub_showhide('wang24_visrecall','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./wang24_visrecall.html">Project</a></span>
        
    </p>
	
    <div class="pub_abstract bg_grey" id="pub_abstract_wang24_visrecall" style="display:none;">
        Question answering has recently been proposed as a promising means to assess the recallability of information visualisations. However, prior works are yet to study the link between visually encoding a visualisation in memory and recall performance. To fill this gap, we propose VisRecall++ – a novel 40-participant recallability dataset that contains gaze data on 200 visualisations and five question types, such as identifying the title, and finding extreme values.We measured recallability by asking participants questions after they observed the visualisation for 10 seconds.Our analyses reveal several insights, such as saccade amplitude, number of fixations, and fixation duration significantly differ between high and low recallability groups.Finally, we propose GazeRecallNet – a novel computational method to predict recallability from gaze behaviour that outperforms several baselines on this task.Taken together, our results shed light on assessing recallability from gaze behaviour and inform future work on recallability-based visualisation optimisation.
    </div>

    <div class="pub_links bg_grey" id="pub_links_wang24_visrecall" style="display:none;">
        <div class="pub_links">                            
	<i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./wang24_visrecall/pdf/wang24_visrecall.pdf">paper.pdf</a></p>                               
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_wang24_visrecall" style="display:none;">@article{wang24_visrecall,
	title = {VisRecall++: Analysing and Predicting Visualisation Recallability from Gaze Behaviour},
	author = {Wang, Yao and Jiang, Yue and Hu, Zhiming and Ruhdorfer, Constantin and Bâce, Mihai and Bulling, Andreas},
	year = {2024},
	journal = {Proc. ACM on Human-Computer Interaction (PACM HCI)},
	pages = {1--18},
	volume = {8},
	number = {ETRA}}
</div>

</div>
</li></ol>


<ol class="bibliography"><li>
<div id="elfares24_privateyes" class="margin-bottom-30">
        <a href="./elfares24_privateyes.html">
        <img class="thumbnail" src="./elfares24_privateyes/image/thumb.png" title="PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure Multi-Party Computation" alt="PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure Multi-Party Computation">
        </a>
    
    <p class="pub_title">PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure Multi-Party Computation</p>

    <p class="pub_author">
            <a class="a-int" href="https://www.perceptualui.org/people/elfares/">Mayar Elfares</a>, 
			Pascal Reisert, 
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,    Wenwu Tang, Ralf Küsters, 
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
    </p>
	
    <p class="pub_additional">
        
            <span class="pub_additional_journal">Proc. ACM on Human-Computer Interaction (PACM HCI), 8 (ETRA),</span>        
            <span class="pub_additional_pages">pp. 1–22, </span>
        
        <span class="pub_additional_year">2024</span>.
    </p>
	
    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_elfares24_privateyes" class="pub_show" onclick="pub_showhide('elfares24_privateyes','pub_abstract')">Abstract</a></span>
                
        <span class="button bg_greydark"><a id="pub_links_sh_elfares24_privateyes" class="pub_show" onclick="pub_showhide('elfares24_privateyes','pub_links')">Links</a></span>
        
        <span class="button bg_greydark"><a id="pub_bibtex_sh_elfares24_privateyes" class="pub_show" onclick="pub_showhide('elfares24_privateyes','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./elfares24_privateyes.html">Project</a></span>
        
    </p>
	
    <div class="pub_abstract bg_grey" id="pub_abstract_elfares24_privateyes" style="display:none;">
        Latest gaze estimation methods require large-scale training data but their collection and exchange pose significant privacy risks. We propose PrivatEyes - the first privacy-enhancing training approach for appearance-based gaze estimation based on federated learning (FL) and secure multi-party computation (MPC). PrivatEyes enables training gaze estimators on multiple local datasets across different users and server-based secure aggregation of the individual estimators’ updates. PrivatEyes guarantees that individual gaze data remains private even if a majority of the aggregating servers is malicious. We also introduce a new data leakage attack DualView that shows that PrivatEyes limits the leakage of private training data more effectively than previous approaches. Evaluations on the MPIIGaze, MPIIFaceGaze, GazeCapture, and NVGaze datasets further show that the improved privacy does not lead to a lower gaze estimation accuracy or substantially higher computational costs - both of which are on par with its non-secure counterparts.
    </div>

    <div class="pub_links bg_grey" id="pub_links_elfares24_privateyes" style="display:none;">
        <div class="pub_links">                            
	<i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./elfares24_privateyes/pdf/elfares24_privateyes.pdf">paper.pdf</a></p>                               
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_elfares24_privateyes" style="display:none;">@article{elfares24_privateyes,
	title = {PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure Multi-Party Computation},
	author = {Elfares, Mayar and Reisert, Pascal and Hu, Zhiming and Tang, Wenwu and Küsters, Ralf and Bulling, Andreas},
	year = {2024},
	journal = {Proc. ACM on Human-Computer Interaction (PACM HCI)},
	pages = {1--22},
	volume = {8},
	number = {ETRA}}
</div>

</div>
</li></ol>


<ol class="bibliography"><li>
<div id="zhang24_mouse2vec" class="margin-bottom-30">
        <a href="./zhang24_mouse2vec.html">
        <img class="thumbnail" src="./zhang24_mouse2vec/image/thumb.png" title="Mouse2Vec: Learning Reusable Semantic Representations of Mouse Behaviour" alt="Mouse2Vec: Learning Reusable Semantic Representations of Mouse Behaviour">
        </a>
    
    <p class="pub_title">Mouse2Vec: Learning Reusable Semantic Representations of Mouse Behaviour</p>

    <p class="pub_author">
            <a class="a-int" href="https://www.perceptualui.org/people/gzhang/">Guanhua Zhang</a>,
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bace/">Mihai Bâce</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
    </p>
	
    <p class="pub_additional">
        
            <span class="pub_additional_journal">Proc. ACM SIGCHI Conference on Human Factors in Computing Systems (CHI), </span>        
            <span class="pub_additional_pages">pp. 1–17, </span>
        
        <span class="pub_additional_year">2024</span>.
    </p>

    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_zhang24_mouse2vec" class="pub_show" onclick="pub_showhide('zhang24_mouse2vec','pub_abstract')">Abstract</a></span>
                
        <span class="button bg_greydark"><a id="pub_links_sh_zhang24_mouse2vec" class="pub_show" onclick="pub_showhide('zhang24_mouse2vec','pub_links')">Links</a></span>
        
        <span class="button bg_greydark"><a id="pub_bibtex_sh_zhang24_mouse2vec" class="pub_show" onclick="pub_showhide('zhang24_mouse2vec','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./zhang24_mouse2vec.html">Project</a></span>
        
    </p>
	
    <div class="pub_abstract bg_grey" id="pub_abstract_zhang24_mouse2vec" style="display:none;">
        The mouse is a pervasive input device used for a wide range of interactive applications. However, computational modelling of mouse behaviour typically requires time-consuming design and extraction of handcrafted features, or approaches that are application-specific. We instead propose Mouse2Vec – a novel self-supervised method designed to learn semantic representations of mouse behaviour that are reusable across users and applications. Mouse2Vec uses a Transformer-based encoder-decoder architecture, which is specifically geared for mouse data: During pretraining, the encoder learns an embedding of input mouse trajectories while the decoder reconstructs the input and simultaneously detects mouse click events. We show that the representations learned by our method can identify interpretable mouse behaviour clusters and retrieve similar mouse trajectories. We also demonstrate on three sample downstream tasks that the representations can be practically used to augment mouse data for training supervised methods and serve as an effective feature extractor.
    </div>

    <div class="pub_links bg_grey" id="pub_links_zhang24_mouse2vec" style="display:none;">
        <div class="pub_links">                            
	<i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./zhang24_mouse2vec/pdf/zhang24_mouse2vec.pdf">paper.pdf</a></p>                               
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_zhang24_mouse2vec" style="display:none;">@inproceedings{zhang24_mouse2vec,
	title = {Mouse2Vec: Learning Reusable Semantic Representations of Mouse Behaviour},
	author = {Zhang, Guanhua and Hu, Zhiming and B{\^a}ce, Mihai and Bulling, Andreas},
	year = {2024},
	pages = {1--17},
	booktitle = {Proc. ACM SIGCHI Conference on Human Factors in Computing Systems (CHI)},
	doi = {10.1145/3613904.3642141}}
</div>

</div>
</li></ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="wang24_salchartqa" class="margin-bottom-30">
        <a href="./wang24_salchartqa.html">
        <img class="thumbnail" src="./wang24_salchartqa/image/thumb.png" title="SalChartQA: Question-driven Saliency on Information Visualisations" alt="SalChartQA: Question-driven Saliency on Information Visualisations">
        </a>
    
    <p class="pub_title">SalChartQA: Question-driven Saliency on Information Visualisations</p>

    <p class="pub_author">
            <a class="a-int" href="https://www.perceptualui.org/people/wang/">Yao Wang</a>,
			Weitian Wang,
			Abdullah Abdelhafez,
			<a class="a-int" href="https://www.perceptualui.org/people/elfares/">Mayar Elfares</a>,
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bace/">Mihai Bâce</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
    </p>
	
    <p class="pub_additional">
        
            <span class="pub_additional_journal">Proc. ACM SIGCHI Conference on Human Factors in Computing Systems (CHI), </span>        
            <span class="pub_additional_pages">pp. 1–14, </span>
        
        <span class="pub_additional_year">2024</span>.
    </p>

    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_wang24_salchartqa" class="pub_show" onclick="pub_showhide('wang24_salchartqa','pub_abstract')">Abstract</a></span>
                
        <span class="button bg_greydark"><a id="pub_links_sh_wang24_salchartqa" class="pub_show" onclick="pub_showhide('wang24_salchartqa','pub_links')">Links</a></span>
        
        <span class="button bg_greydark"><a id="pub_bibtex_sh_wang24_salchartqa" class="pub_show" onclick="pub_showhide('wang24_salchartqa','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./wang24_salchartqa.html">Project</a></span>
        
    </p>
	
    <div class="pub_abstract bg_grey" id="pub_abstract_wang24_salchartqa" style="display:none;">
        Understanding the link between visual attention and user’s needs when visually exploring information visualisations is under-explored due to a lack of large and diverse datasets to facilitate these analyses. To fill this gap, we introduce SalChartQA – a novel crowd-sourced dataset that uses the BubbleView interface as a proxy for human gaze and a question-answering (QA) paradigm to induce different information needs in users. SalChartQA contains 74,340 answers to 6,000 questions on 3,000 visualisations. Informed by our analyses demonstrating the tight correlation between the question and visual saliency, we propose the first computational method to predict question-driven saliency on information visualisations. Our method outperforms state-of-the-art saliency models, improving several metrics, such as the correlation coefficient and the Kullback-Leibler divergence. These results show the importance of information needs for shaping attention behaviour and paving the way for new applications, such as task-driven optimisation of visualisations or explainable AI in chart question-answering.
    </div>

    <div class="pub_links bg_grey" id="pub_links_wang24_salchartqa" style="display:none;">
        <div class="pub_links">                            
	<i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./wang24_salchartqa/pdf/wang24_salchartqa.pdf">paper.pdf</a></p>                               
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_wang24_salchartqa" style="display:none;">@inproceedings{wang24_salchartqa,
	title = {SalChartQA: Question-driven Saliency on Information Visualisations},
	author = {Wang, Yao and Wang, Weitian and Abdelhafez, Abdullah and Elfares, Mayar and Hu, Zhiming and B{\^a}ce, Mihai and Bulling, Andreas},
	year = {2024},
	pages = {1--14},
	booktitle = {Proc. ACM SIGCHI Conference on Human Factors in Computing Systems (CHI)},
	doi = {10.1145/3613904.3642942}}
</div>

</div>
</li></ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="jiao23_supreyes" class="margin-bottom-30">    
        <a href="./jiao23_supreyes.html">
        <img class="thumbnail" src="./jiao23_supreyes/image/thumb.png" title="SUPREYES: SUPer Resolution for EYES Using Implicit Neural Representation Learning" alt="SUPREYES: SUPer Resolution for EYES Using Implicit Neural Representation Learning">
        </a>
    
    <p class="pub_title">SUPREYES: SUPer Resolution for EYES Using Implicit Neural Representation Learning</p>

    <p class="pub_author">
        
            <a class="a-int" href="https://www.perceptualui.org/people/jiao/">Chuhan Jiao</a>,
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bace/">Mihai Bâce</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
    </p>
	
    <p class="pub_additional">
        
            <span class="pub_additional_journal">Proc. ACM Symposium on User Interface Software and Technology (UIST), </span>        
            <span class="pub_additional_pages">pp. 1–13, </span>
        
        <span class="pub_additional_year">2023</span>.
    </p>

    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_jiao23_supreyes" class="pub_show" onclick="pub_showhide('jiao23_supreyes','pub_abstract')">Abstract</a></span>
                
        <span class="button bg_greydark"><a id="pub_links_sh_jiao23_supreyes" class="pub_show" onclick="pub_showhide('jiao23_supreyes','pub_links')">Links</a></span>
        
        <span class="button bg_greydark"><a id="pub_bibtex_sh_jiao23_supreyes" class="pub_show" onclick="pub_showhide('jiao23_supreyes','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./jiao23_supreyes.html">Project</a></span>
        
    </p>

    <div class="pub_abstract bg_grey" id="pub_abstract_jiao23_supreyes" style="display:none;">
        We introduce SUPREYES – a novel self-supervised method to increase the spatio-temporal resolution of gaze data recorded using low(er)-resolution eye trackers. Despite continuing advances in eye tracking technology, the vast majority of current eye trackers – particularly mobile ones and those integrated into mobile devices – suffer from low-resolution gaze data, thus fundamentally limiting their practical usefulness. SUPREYES learns a continuous implicit neural representation from low-resolution gaze data to up-sample the gaze data to arbitrary resolutions. We compare our method with commonly used interpolation methods on arbitrary scale super-resolution and demonstrate that SUPREYES outperforms these baselines by a significant margin. We also test on the sample downstream task of gaze-based user identification and show that our method improves the performance of original low-resolution gaze data and outperforms other baselines. These results are promising as they open up a new direction for increasing eye tracking fidelity as well as enabling new gaze-based applications without the need for new eye tracking equipment.
    </div>

    <div class="pub_links bg_grey" id="pub_links_jiao23_supreyes" style="display:none;">
        <div class="pub_links">                            
	<i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./jiao23_supreyes/pdf/jiao23_supreyes.pdf">paper.pdf</a></p>                               
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_jiao23_supreyes" style="display:none;">@inproceedings{jiao23_supreyes,
	author = {Jiao, Chuhan and Hu, Zhiming and B{\^a}ce, Mihai and Bulling, Andreas},
	title = {SUPREYES: SUPer Resolution for EYES Using Implicit Neural Representation Learning},
	booktitle = {Proc. ACM Symposium on User Interface Software and Technology (UIST)},
	year = {2023},
	pages = {1--13},
	doi = {10.1145/3586183.3606780}}
</div>

</div>
</li></ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="zhang23_exploring" class="margin-bottom-30">    
        <a href="./zhang23_exploring.html">
        <img class="thumbnail" src="./zhang23_exploring/image/thumb.png" title="Exploring Natural Language Processing Methods for Interactive Behaviour Modelling" alt="Exploring Natural Language Processing Methods for Interactive Behaviour Modelling">
        </a>
    

    <p class="pub_title">Exploring Natural Language Processing Methods for Interactive Behaviour Modelling</p>

    <p class="pub_author">
        
            <a class="a-int" href="https://www.perceptualui.org/people/gzhang/">Guanhua Zhang</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bortoletto/">Matteo Bortoletto</a>,
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/shi/">Lei Shi</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bace/">Mihai Bâce</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
    </p>

    <p class="pub_additional">
        
            <span class="pub_additional_journal">Proc. IFIP TC13 Conference on Human-Computer Interaction (INTERACT), </span>
            <span class="pub_additional_pages">pp. 1–22, </span>
        
        <span class="pub_additional_year">2023</span>.
    </p>

    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_zhang23_exploring" class="pub_show" onclick="pub_showhide('zhang23_exploring','pub_abstract')">Abstract</a></span>
        

        
        <span class="button bg_greydark"><a id="pub_links_sh_zhang23_exploring" class="pub_show" onclick="pub_showhide('zhang23_exploring','pub_links')">Links</a></span>
        

        <span class="button bg_greydark"><a id="pub_bibtex_sh_zhang23_exploring" class="pub_show" onclick="pub_showhide('zhang23_exploring','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./zhang23_exploring.html">Project</a></span>
        <span class="pub_award"><svg class="svg-inline--fa fa-award fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="award" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M97.12 362.63c-8.69-8.69-4.16-6.24-25.12-11.85-9.51-2.55-17.87-7.45-25.43-13.32L1.2 448.7c-4.39 10.77 3.81 22.47 15.43 22.03l52.69-2.01L105.56 507c8 8.44 22.04 5.81 26.43-4.96l52.05-127.62c-10.84 6.04-22.87 9.58-35.31 9.58-19.5 0-37.82-7.59-51.61-21.37zM382.8 448.7l-45.37-111.24c-7.56 5.88-15.92 10.77-25.43 13.32-21.07 5.64-16.45 3.18-25.12 11.85-13.79 13.78-32.12 21.37-51.62 21.37-12.44 0-24.47-3.55-35.31-9.58L252 502.04c4.39 10.77 18.44 13.4 26.43 4.96l36.25-38.28 52.69 2.01c11.62.44 19.82-11.27 15.43-22.03zM263 340c15.28-15.55 17.03-14.21 38.79-20.14 13.89-3.79 24.75-14.84 28.47-28.98 7.48-28.4 5.54-24.97 25.95-45.75 10.17-10.35 14.14-25.44 10.42-39.58-7.47-28.38-7.48-24.42 0-52.83 3.72-14.14-.25-29.23-10.42-39.58-20.41-20.78-18.47-17.36-25.95-45.75-3.72-14.14-14.58-25.19-28.47-28.98-27.88-7.61-24.52-5.62-44.95-26.41-10.17-10.35-25-14.4-38.89-10.61-27.87 7.6-23.98 7.61-51.9 0-13.89-3.79-28.72.25-38.89 10.61-20.41 20.78-17.05 18.8-44.94 26.41-13.89 3.79-24.75 14.84-28.47 28.98-7.47 28.39-5.54 24.97-25.95 45.75-10.17 10.35-14.15 25.44-10.42 39.58 7.47 28.36 7.48 24.4 0 52.82-3.72 14.14.25 29.23 10.42 39.59 20.41 20.78 18.47 17.35 25.95 45.75 3.72 14.14 14.58 25.19 28.47 28.98C104.6 325.96 106.27 325 121 340c13.23 13.47 33.84 15.88 49.74 5.82a39.676 39.676 0 0 1 42.53 0c15.89 10.06 36.5 7.65 49.73-5.82zM97.66 175.96c0-53.03 42.24-96.02 94.34-96.02s94.34 42.99 94.34 96.02-42.24 96.02-94.34 96.02-94.34-42.99-94.34-96.02z"></path></svg> IFIP TC13 Pioneers’ Award for Best Doctoral Student Paper Nominees</span>
    </p>

    <div class="pub_abstract bg_grey" id="pub_abstract_zhang23_exploring" style="display:none;">
        Analysing and modelling interactive behaviour is an important topic in human-computer interaction (HCI) and a key requirement for the development of intelligent interactive systems. Interactive behaviour has a sequential (actions happen one after another) and hierarchical (a sequence of actions forms an activity driven by interaction goals) structure, which may be similar to the structure of natural language. Designed based on such a structure, natural language processing (NLP) methods have achieved groundbreaking success in various downstream tasks. However, few works linked interactive behaviour with natural language. In this paper, we explore the similarity between interactive behaviour and natural language by applying an NLP method, byte pair encoding (BPE), to encode mouse and keyboard behaviour. We then analyse the vocabulary, i.e., the set of action sequences, learnt by BPE, as well as use the vocabulary to encode the input behaviour for interactive task recognition. An existing dataset collected in constrained lab settings and our novel out-of-the-lab dataset were used for evaluation. Results show that this natural language-inspired approach not only learns action sequences that reflect specific interaction goals, but also achieves higher F1 scores on task recognition than other methods. Our work reveals the similarity between interactive behaviour and natural language, and presents the potential of applying the new pack of methods that leverage insights from NLP to model interactive behaviour in HCI.
    </div>

    <div class="pub_links bg_grey" id="pub_links_zhang23_exploring" style="display:none;">
        <div class="pub_links">
                
                
                    <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./zhang23_exploring/pdf/zhang23_exploring.pdf">paper.pdf</a></p>
                
                
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_zhang23_exploring" style="display:none;">@inproceedings{zhang23_exploring,
	title = {Exploring Natural Language Processing Methods for Interactive Behaviour Modelling},
	author = {Zhang, Guanhua and Bortoletto, Matteo and Hu, Zhiming and Shi, Lei and B{\^a}ce, Mihai and Bulling, Andreas},
	booktitle = {Proc. IFIP TC13 Conference on Human-Computer Interaction (INTERACT)},
	pages = {1--22},
	year = {2023},
	publisher = {Springer}}
</div>

</div>
</li></ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu22_ehtask" class="margin-bottom-30">
<a href="./hu22_ehtask.html">
<img class="thumbnail" src="./hu22_ehtask/image/thumb.png" title="EHTask: Recognizing User Tasks from Eye and Head Movements in Immersive Virtual Reality" alt="EHTask: Recognizing User Tasks from Eye and Head Movements in Immersive Virtual Reality">
</a>	
<p class="pub_title">EHTask: Recognizing User Tasks from Eye and Head Movements in Immersive Virtual Reality</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
<a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>,
Sheng Li, Guoping Wang
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics (TVCG, oral presentation at IEEE VR 2022), 2023, 29(4): 1992-2004.
</span>
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu22_ehtask" class="pub_show" onclick="pub_showhide(&#39;hu22_ehtask&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu22_ehtask" class="pub_show" onclick="pub_showhide(&#39;hu22_ehtask&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu22_ehtask" class="pub_show" onclick="pub_showhide(&#39;hu22_ehtask&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu22_ehtask.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu22_ehtask" style="display:none;">
Understanding human visual attention in immersive virtual reality (VR) is crucial for many important applications, including gaze prediction, gaze guidance, and gaze-contingent rendering.
However, previous works on visual attention analysis typically only explored one specific VR task and paid less attention to the differences between different tasks.
Moreover, existing task recognition methods typically focused on 2D viewing conditions and only explored the effectiveness of human eye movements.
We first collect eye and head movements of 30 participants performing four tasks, i.e. Free viewing, Visual search, Saliency, and Track, in 15 360-degree VR videos.
Using this dataset, we analyze the patterns of human eye and head movements and reveal significant differences across different tasks in terms of fixation duration, saccade amplitude, head rotation velocity, and eye-head coordination.
We then propose EHTask -- a novel learning-based method that employs eye and head movements to recognize user tasks in VR.
We show that our method significantly outperforms the state-of-the-art methods derived from 2D viewing conditions both on our dataset (accuracy of 84.4% vs. 62.8%) and on a real-world dataset (61.9% vs. 44.1%). 
As such, our work provides meaningful insights into human visual attention under different VR tasks and guides future work on recognizing user tasks in VR.
</div>
<div class="pub_links bg_grey" id="pub_links_hu22_ehtask" style="display:none;">
<div class="pub_links">
  <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2021.3138902" rel="nofollow" target="_blank">doi</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu22_ehtask/pdf/hu22_ehtask.pdf">paper.pdf</a></p>
  <i class="fa fa-database"></i>&nbsp;&nbsp;Dataset: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FEHTaskDataset&ga=1">dataset</a></p>
  <i class="fa fa-file-powerpoint"></i>&nbsp; Slides: <a class="pub_list" href="./hu22_ehtask/ppt/hu22_ehtask.pdf">slides.pdf</a></p>
  <i class="fa fa-code"></i>&nbsp;Code: <a class="a-text-ext" href="https://github.com/CraneHzm/EHTask">code</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu22_ehtask" style="display:none;">@article{hu22_ehtask,
	author={Hu, Zhiming and Bulling, Andreas and Li, Sheng and Wang, Guoping},
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={EHTask: Recognizing User Tasks From Eye and Head Movements in Immersive Virtual Reality}, 
	year={2023},
	volume={29},
	number={4},
	pages={1992-2004},
	doi={10.1109/TVCG.2021.3138902}}  
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
  <div id="lin22_intentional" class="margin-bottom-30">
  
      
      
          <a href="./lin22_intentional.html">
          <img class="thumbnail" src="./lin22_intentional/image/thumb.png" title="Intentional Head-Motion Assisted Locomotion for Reducing Cybersickness" alt="Intentional Head-Motion Assisted Locomotion for Reducing Cybersickness">
          </a>
      
  
      <p class="pub_title">Intentional Head-Motion Assisted Locomotion for Reducing Cybersickness</p>
  
      <p class="pub_author">
          
              Zehui Lin,
              Xiang Gu,
              Sheng Li,
              <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
              Guoping Wang
      </p>
  
      <p class="pub_additional">
          
              <span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics (TVCG, oral presentation at IEEE VR 2022),
          </span>
          <span class="pub_additional_year">2023,</span>
          <span class="pub_additional_page"> 29(8): 3458-3471.</span>
      </p>
  
      <p class="pub_tags">
          
          <span class="button bg_greydark"><a id="pub_abstract_sh_lin22_intentional" class="pub_show" onclick="pub_showhide('lin22_intentional','pub_abstract')">Abstract</a></span>
          
  
          
          <span class="button bg_greydark"><a id="pub_links_sh_lin22_intentional" class="pub_show" onclick="pub_showhide('lin22_intentional','pub_links')">Links</a></span>
          
  
          <span class="button bg_greydark"><a id="pub_bibtex_sh_lin22_intentional" class="pub_show" onclick="pub_showhide('lin22_intentional','pub_bibtex')">BibTeX</a></span>
  
          <span class="button bg_greydark margin-left-10"><a href="./lin22_intentional.html">Project</a></span>
  
          
      </p>
  
      <div class="pub_abstract bg_grey" id="pub_abstract_lin22_intentional" style="display:none;">
          We present an efficient locomotion technique that can reduce cybersickness through aligning the visual and vestibular induced self-motion illusion. Our locomotion technique stimulates proprioception consistent with the visual sense by intentional head motion, which includes both the head’s translational movement and yaw rotation. A locomotion event is triggered by the hand-held controller together with an intended physical head motion simultaneously. Based on our method, we further explore the connections between the level of cybersickness and the velocity of self motion through a series of experiments. We first conduct Experiment 1 to investigate the cybersickness induced by different translation velocities using our method and then conduct Experiment 2 to investigate the cybersickness induced by different angular velocities. Our user studies from these two experiments reveal a new finding on the correlation between translation/angular velocities and the level of cybersickness. The cybersickness is greatest at the lowest velocity using our method, and the statistical analysis also indicates a possible U-shaped relation between the translation/angular velocity and cybersickness degree. Finally, we conduct Experiment 3 to evaluate the performances of our method and other commonly-used locomotion approaches, i.e., joystick-based steering and teleportation. The results show that our method can significantly reduce cybersickness compared with the joystick-based steering and obtain a higher presence compared with the teleportation. These advantages demonstrate that our method can be an optional locomotion solution for immersive VR applications using commercially available HMD suites only.
      </div>
  
      <div class="pub_links bg_grey" id="pub_links_lin22_intentional" style="display:none;">
          <div class="pub_links">
                  
                      <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="https://ieeexplore.ieee.org/document/9737429" rel="nofollow" target="_blank">doi</a></p>
                  
                  
                      <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./lin22_intentional/pdf/lin22_intentional.pdf">paper.pdf</a></p>
                  
                 
          </div>
      </div>
  <div class="pub_bibtex bg_grey" id="pub_bibtex_lin22_intentional" style="display:none;">@article{lin22_intentional,
	author={Lin, Zehui and Gu, Xiang and Li, Sheng and Hu, Zhiming and Wang, Guoping},
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={Intentional Head-Motion Assisted Locomotion for Reducing Cybersickness}, 
	year={2023},
	volume={29},
	number={8},
	pages={3458-3471},
	doi={10.1109/TVCG.2022.3160232}}
</div>
  </li></ol>

<!--paper-->
<ol class="bibliography"><li>
<div id="elfares22_federated" class="margin-bottom-30">

    
    
        <a href="./elfares22_federated.html">
        <img class="thumbnail" src="./elfares22_federated/image/thumb.png" title="Federated Learning for Appearance-based Gaze Estimation in the Wild" alt="Federated Learning for Appearance-based Gaze Estimation in the Wild">
        </a>
    

    <p class="pub_title">Federated Learning for Appearance-based Gaze Estimation in the Wild</p>

    <p class="pub_author">
        
            <a class="a-int" href="https://www.perceptualui.org/people/elfares/">Mayar Elfares</a>,
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
            Pascal Reisert,
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>,
            Ralf Küsters
    </p>

    <p class="pub_additional">
        
            <span class="pub_additional_journal">Proceedings of the NeurIPS Workshop Gaze Meets ML (NeurIPS GMML), </span>
        

        
            <span class="pub_additional_pages">pp. 1–17, </span>
        
        <span class="pub_additional_year">2022</span>.
    </p>

    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_elfares22_federated" class="pub_show" onclick="pub_showhide('elfares22_federated','pub_abstract')">Abstract</a></span>
        

        
        <span class="button bg_greydark"><a id="pub_links_sh_elfares22_federated" class="pub_show" onclick="pub_showhide('elfares22_federated','pub_links')">Links</a></span>
        

        <span class="button bg_greydark"><a id="pub_bibtex_sh_elfares22_federated" class="pub_show" onclick="pub_showhide('elfares22_federated','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./elfares22_federated.html">Project</a></span>

        
    </p>

    <div class="pub_abstract bg_grey" id="pub_abstract_elfares22_federated" style="display:none;">
        Gaze estimation methods have significantly matured in recent years but the large number of eye images required to train deep learning models poses significant privacy risks. In addition, the heterogeneous data distribution across different users can significantly hinder the training process. In this work, we propose the first federated learning approach for gaze estimation to preserve the privacy of gaze data. We further employ pseudo-gradients optimisation to adapt our federated learning approach to the divergent model updates to address the heterogeneous nature of in-the-wild gaze data in collaborative setups. We evaluate our approach on a real-world dataset (MPIIGaze dataset) and show that our work enhances the privacy guarantees of conventional appearance-based gaze estimation methods, handles the convergence issues of gaze estimators, and significantly outperforms vanilla federated learning by 15.8% (from a mean error of 10.63 degrees to 8.95 degrees). As such, our work paves the way to develop privacy-aware collaborative 14 learning setups for gaze estimation while maintaining the model’s performance.
    </div>

    <div class="pub_links bg_grey" id="pub_links_elfares22_federated" style="display:none;">
        <div class="pub_links">
              
                
                    <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.48550/arXiv.2211.07330" rel="nofollow" target="_blank">doi</a></p>

                    <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./elfares22_federated/pdf/elfares22_federated.pdf">paper.pdf</a></p>
                
                
                
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_elfares22_federated" style="display:none;">@inproceedings{elfares22_federated,
	title = {Federated Learning for Appearance-based Gaze Estimation in the Wild},
	author = {Elfares, Mayar and Hu, Zhiming and Reisert, Pascal and Bulling, Andreas and Küsters, Ralf},
	year = {2022},
	booktitle = {Proceedings of the NeurIPS Workshop Gaze Meets ML (GMML)},
	doi = {10.48550/arXiv.2211.07330},
	pages = {1--17}}
</div>
</div>
</li></ol>



<ol class="bibliography"><li>
<div id="hu21_user" class="margin-bottom-30">
<a href="./hu21_user.html">
<img class="thumbnail" src="./hu21_user/image/thumb.png" title="Research progress of user task prediction and algorithm analysis (in Chinese)" alt="Research progress of user task prediction and algorithm analysis (in Chinese)">
</a>	
<p class="pub_title">Research progress of user task prediction and algorithm analysis (in Chinese)</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
Sheng Li, Meng Gai
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">Journal of Graphics, 2021, 42(3): 367-375.
</span>
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu21_user" class="pub_show" onclick="pub_showhide(&#39;hu21_user&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu21_user" class="pub_show" onclick="pub_showhide(&#39;hu21_user&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu21_user" class="pub_show" onclick="pub_showhide(&#39;hu21_user&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu21_user.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu21_user" style="display:none;">
Users’ cognitive behaviors are dramatically influenced by the specific tasks assigned to them. 
Information on users’ tasks can be applied to many areas, such as human behavior analysis and intelligent human-computer interfaces. 
It can be used as the input of intelligent systems and enable the systems to automatically adjust their functions according to different tasks. 
User task prediction refers to the prediction of users’ tasks at hand based on the characteristics of his or her eye movements, the characteristics of scene content, and other related information. 
User task prediction is a popular research topic in vision research, and researchers have proposed many successful task prediction algorithms. 
However, the algorithms proposed in prior works mainly focus on a particular scene, and comparison and analysis are absent for these algorithms. 
This paper presented a review of prior works on task prediction in scenes of images, videos, and real world, and detailed existing task prediction algorithms. 
Based on a real-world task dataset, this paper evaluated the performances of existing algorithms and conducted the corresponding analysis and discussion. 
As such, this work can provide meaningful insights for future works on this important topic.
</div>
<div class="pub_links bg_grey" id="pub_links_hu21_user" style="display:none;">
<div class="pub_links">
  <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://www.txxb.com.cn/CN/10.11996/JG.j.2095-302X.2021030367" rel="nofollow" target="_blank">doi</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu21_user/pdf/hu21_user.pdf">paper.pdf</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu21_user" style="display:none;">@article{hu21_user,
	title = {Research progress of user task prediction and algorithm analysis (in Chinese)},
	author = {Hu, Zhiming and Li, Sheng and Gai, Meng},
	year = {2021},
	journal={Journal of Graphics},
	doi = {http://www.txxb.com.cn/CN/10.11996/JG.j.2095-302X.2021030367},
	volume = {42},
	number = {3},
	pages = {367-375}}
</div>
</ol>



<ol class="bibliography"><li>
<div id="hu21_eye" class="margin-bottom-30">
<a href="./hu21_eye.html">
<img class="thumbnail" src="./hu21_eye/image/thumb.png" title="Eye Fixation Forecasting in Task-Oriented Virtual Reality" alt="Eye Fixation Forecasting in Task-Oriented Virtual Reality">
</a>
<p class="pub_title">Eye Fixation Forecasting in Task-Oriented Virtual Reality</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">Proceedings of the 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), 2021: 707-708.
</span>
</p>
<p class="pub_tags">        
<span class="button bg_greydark"><a id="pub_abstract_sh_hu21_eye" class="pub_show" onclick="pub_showhide(&#39;hu21_eye&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
<span class="button bg_greydark"><a id="pub_links_sh_hu21_eye" class="pub_show" onclick="pub_showhide(&#39;hu21_eye&#39;,&#39;pub_links&#39;)">Links</a></span>
<span class="button bg_greydark"><a id="pub_bibtex_sh_hu21_eye" class="pub_show" onclick="pub_showhide(&#39;hu21_eye&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
<span class="button bg_greydark margin-left-10"><a href="./hu21_eye.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu21_eye" style="display:none;">
In immersive virtual reality (VR), users' visual attention is crucial for many important applications, including VR content design, gaze-based interaction, and gaze-contingent rendering. 
Especially, information on users' future eye fixations is key for intelligent user interfaces and has significant relevance for many areas, such as visual attention enhancement, dynamic event triggering, and human-computer interaction. 
However, previous works typically focused on free-viewing conditions and paid less attention to task-oriented attention. 
This paper aims at forecasting users' eye fixations in task-oriented virtual reality. 
To this end, a VR eye tracking dataset that corresponds to different users performing a visual search task in immersive virtual environments is built. 
A comprehensive analysis of users' eye fixations is performed based on the collected data. 
The analysis reveals that eye fixations are correlated with users' historical gaze positions, task-related objects, saliency information of the VR content, and head rotation velocities. 
Based on this analysis, a novel learning-based model is proposed to forecast users' eye fixations in the near future in immersive virtual environments.</div>
<div class="pub_links bg_grey" id="pub_links_hu21_eye" style="display:none;">
<div class="pub_links">            
	<svg class="svg-inline--fa fa-fingerprint fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="fingerprint" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M256.12 245.96c-13.25 0-24 10.74-24 24 1.14 72.25-8.14 141.9-27.7 211.55-2.73 9.72 2.15 30.49 23.12 30.49 10.48 0 20.11-6.92 23.09-17.52 13.53-47.91 31.04-125.41 29.48-224.52.01-13.25-10.73-24-23.99-24zm-.86-81.73C194 164.16 151.25 211.3 152.1 265.32c.75 47.94-3.75 95.91-13.37 142.55-2.69 12.98 5.67 25.69 18.64 28.36 13.05 2.67 25.67-5.66 28.36-18.64 10.34-50.09 15.17-101.58 14.37-153.02-.41-25.95 19.92-52.49 54.45-52.34 31.31.47 57.15 25.34 57.62 55.47.77 48.05-2.81 96.33-10.61 143.55-2.17 13.06 6.69 25.42 19.76 27.58 19.97 3.33 26.81-15.1 27.58-19.77 8.28-50.03 12.06-101.21 11.27-152.11-.88-55.8-47.94-101.88-104.91-102.72zm-110.69-19.78c-10.3-8.34-25.37-6.8-33.76 3.48-25.62 31.5-39.39 71.28-38.75 112 .59 37.58-2.47 75.27-9.11 112.05-2.34 13.05 6.31 25.53 19.36 27.89 20.11 3.5 27.07-14.81 27.89-19.36 7.19-39.84 10.5-80.66 9.86-121.33-.47-29.88 9.2-57.88 28-80.97 8.35-10.28 6.79-25.39-3.49-33.76zm109.47-62.33c-15.41-.41-30.87 1.44-45.78 4.97-12.89 3.06-20.87 15.98-17.83 28.89 3.06 12.89 16 20.83 28.89 17.83 11.05-2.61 22.47-3.77 34-3.69 75.43 1.13 137.73 61.5 138.88 134.58.59 37.88-1.28 76.11-5.58 113.63-1.5 13.17 7.95 25.08 21.11 26.58 16.72 1.95 25.51-11.88 26.58-21.11a929.06 929.06 0 0 0 5.89-119.85c-1.56-98.75-85.07-180.33-186.16-181.83zm252.07 121.45c-2.86-12.92-15.51-21.2-28.61-18.27-12.94 2.86-21.12 15.66-18.26 28.61 4.71 21.41 4.91 37.41 4.7 61.6-.11 13.27 10.55 24.09 23.8 24.2h.2c13.17 0 23.89-10.61 24-23.8.18-22.18.4-44.11-5.83-72.34zm-40.12-90.72C417.29 43.46 337.6 1.29 252.81.02 183.02-.82 118.47 24.91 70.46 72.94 24.09 119.37-.9 181.04.14 246.65l-.12 21.47c-.39 13.25 10.03 24.31 23.28 24.69.23.02.48.02.72.02 12.92 0 23.59-10.3 23.97-23.3l.16-23.64c-.83-52.5 19.16-101.86 56.28-139 38.76-38.8 91.34-59.67 147.68-58.86 69.45 1.03 134.73 35.56 174.62 92.39 7.61 10.86 22.56 13.45 33.42 5.86 10.84-7.62 13.46-22.59 5.84-33.43z"></path></svg><p>Doi: <a class="a-text-ext" href="https://doi.org/10.1109/VRW52623.2021.00236" rel="nofollow" target="_blank">doi</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu21_eye/pdf/hu21_eye.pdf">paper.pdf</a></p>                
  <i class="fa fa-file-powerpoint"></i>&nbsp;&nbsp; Slides: <a class="pub_list" href="./hu21_eye/ppt/hu21_eye.pdf">slides.pdf</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu21_eye" style="display:none;">@inproceedings{hu21_eye, 
	author={Hu, Zhiming},
	title = {Eye Fixation Forecasting in Task-Oriented Virtual Reality}, 
	booktitle={Proceedings of the 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops},
	year = {2021},
	pages={707-708},
	organization={IEEE}} 
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu21_fixationnet" class="margin-bottom-30">
<a href="./hu21_fixationnet.html">
<img class="thumbnail" src="./hu21_fixationnet/image/thumb.png" title="FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments" alt="FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments">
</a>
<p class="pub_title">FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
<a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>,
Sheng Li, Guoping Wang
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics (TVCG, IEEE VR 2021 Journal-track), 2021, 27(5): 2681-2690.
</span>
</p>
<p class="pub_tags">        
<span class="button bg_greydark"><a id="pub_abstract_sh_hu21_fixationnet" class="pub_show" onclick="pub_showhide(&#39;hu21_fixationnet&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
<span class="button bg_greydark"><a id="pub_links_sh_hu21_fixationnet" class="pub_show" onclick="pub_showhide(&#39;hu21_fixationnet&#39;,&#39;pub_links&#39;)">Links</a></span>
<span class="button bg_greydark"><a id="pub_bibtex_sh_hu21_fixationnet" class="pub_show" onclick="pub_showhide(&#39;hu21_fixationnet&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
<span class="button bg_greydark margin-left-10"><a href="./hu21_fixationnet.html">Project</a></span>
<span class="pub_award"><svg class="svg-inline--fa fa-award fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="award" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M97.12 362.63c-8.69-8.69-4.16-6.24-25.12-11.85-9.51-2.55-17.87-7.45-25.43-13.32L1.2 448.7c-4.39 10.77 3.81 22.47 15.43 22.03l52.69-2.01L105.56 507c8 8.44 22.04 5.81 26.43-4.96l52.05-127.62c-10.84 6.04-22.87 9.58-35.31 9.58-19.5 0-37.82-7.59-51.61-21.37zM382.8 448.7l-45.37-111.24c-7.56 5.88-15.92 10.77-25.43 13.32-21.07 5.64-16.45 3.18-25.12 11.85-13.79 13.78-32.12 21.37-51.62 21.37-12.44 0-24.47-3.55-35.31-9.58L252 502.04c4.39 10.77 18.44 13.4 26.43 4.96l36.25-38.28 52.69 2.01c11.62.44 19.82-11.27 15.43-22.03zM263 340c15.28-15.55 17.03-14.21 38.79-20.14 13.89-3.79 24.75-14.84 28.47-28.98 7.48-28.4 5.54-24.97 25.95-45.75 10.17-10.35 14.14-25.44 10.42-39.58-7.47-28.38-7.48-24.42 0-52.83 3.72-14.14-.25-29.23-10.42-39.58-20.41-20.78-18.47-17.36-25.95-45.75-3.72-14.14-14.58-25.19-28.47-28.98-27.88-7.61-24.52-5.62-44.95-26.41-10.17-10.35-25-14.4-38.89-10.61-27.87 7.6-23.98 7.61-51.9 0-13.89-3.79-28.72.25-38.89 10.61-20.41 20.78-17.05 18.8-44.94 26.41-13.89 3.79-24.75 14.84-28.47 28.98-7.47 28.39-5.54 24.97-25.95 45.75-10.17 10.35-14.15 25.44-10.42 39.58 7.47 28.36 7.48 24.4 0 52.82-3.72 14.14.25 29.23 10.42 39.59 20.41 20.78 18.47 17.35 25.95 45.75 3.72 14.14 14.58 25.19 28.47 28.98C104.6 325.96 106.27 325 121 340c13.23 13.47 33.84 15.88 49.74 5.82a39.676 39.676 0 0 1 42.53 0c15.89 10.06 36.5 7.65 49.73-5.82zM97.66 175.96c0-53.03 42.24-96.02 94.34-96.02s94.34 42.99 94.34 96.02-42.24 96.02-94.34 96.02-94.34-42.99-94.34-96.02z"></path></svg> TVCG Best Journal Award Nominees</span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu21_fixationnet" style="display:none;">
Human visual attention in immersive virtual reality (VR) is key for many important applications, such as content design, gaze-contingent rendering, or gaze-based interaction.
However, prior works typically focused on free-viewing conditions that have limited relevance for practical applications.
We first collect eye tracking data of 27 participants performing a visual search task in four immersive VR environments.
Based on this dataset, we provide a comprehensive analysis of the collected data and reveal correlations between users' eye fixations and other factors, i.e. users' historical gaze positions, task-related objects, saliency information of the VR content, and users' head rotation velocities.
Based on this analysis, we propose <i>FixationNet</i> -- a novel learning-based model to forecast users' eye fixations in the near future in VR.
We evaluate the performance of our model for free-viewing and task-oriented settings and show that it outperforms the state of the art by a large margin of 19.8% (from a mean error of 2.93&deg to 2.35&deg) in free-viewing and of 15.1% (from 2.05&deg to 1.74&deg) in task-oriented situations.
As such, our work provides new insights into task-oriented attention in virtual environments and guides future work on this important topic in VR research.</div>
<div class="pub_links bg_grey" id="pub_links_hu21_fixationnet" style="display:none;">
<div class="pub_links">            
  <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2021.3067779" rel="nofollow" target="_blank">doi</a></p>
            
            
  <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./hu21_fixationnet/pdf/hu21_fixationnet.pdf">paper.pdf</a></p>



  <i class="fa fa-link"></i><p>Code: <a class="a-text-ext" href="https://github.com/CraneHzm/FixationNet" rel="nofollow" target="_blank">code</a></p>            

  <i class="fa fa-link"></i><p>Dataset: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FFixationNetDataset&ga=1" rel="nofollow" target="_blank">dataset</a></p>

  <i class="fa fa-file-pdf"></i>&nbsp;Slides: <a class="pub_list" href="./hu21_fixationnet/ppt/hu21_fixationnet.pdf">slides.pdf</a></p>

  <i class="fa fa-link"></i><p>Experimental scenes: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FFixationNetScenes&ga=1" rel="nofollow" target="_blank">experimental senes</a></p>

  <i class="fa fa-link"></i><p>Supplementary materials: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FFixationNet%5FSupplementalMaterial%2Ezip&parent=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments&ga=1" rel="nofollow" target="_blank">supplementary materials</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu21_fixationnet" style="display:none;">@article{hu21_fixationnet,
	title={FixationNet: Forecasting eye fixations in task-oriented virtual environments},
	author={Hu, Zhiming and Bulling, Andreas and Li, Sheng and Wang, Guoping},
	journal={IEEE Transactions on Visualization and Computer Graphics},
	volume={27},
	number={5},
	pages={2681--2690},
	year={2021},
	publisher={IEEE}}
</div>
</ol>


<ol class="bibliography"><li>
<div id="hu20_gaze" class="margin-bottom-30">
<a href="./hu20_gaze.html">
<img class="thumbnail" src="./hu20_gaze/image/thumb.png" title="Gaze Analysis and Prediction in Virtual Reality" alt="Gaze Analysis and Prediction in Virtual Reality">
</a>
<p class="pub_title">Gaze Analysis and Prediction in Virtual Reality</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">Proceedings of the 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), 2020: 543-544.
</span>
</p>
<p class="pub_tags">        
<span class="button bg_greydark"><a id="pub_abstract_sh_hu20_gaze" class="pub_show" onclick="pub_showhide(&#39;hu20_gaze&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
<span class="button bg_greydark"><a id="pub_links_sh_hu20_gaze" class="pub_show" onclick="pub_showhide(&#39;hu20_gaze&#39;,&#39;pub_links&#39;)">Links</a></span>
<span class="button bg_greydark"><a id="pub_bibtex_sh_hu20_gaze" class="pub_show" onclick="pub_showhide(&#39;hu20_gaze&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
<span class="button bg_greydark margin-left-10"><a href="./hu20_gaze.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu20_gaze" style="display:none;">
In virtual reality (VR) systems, users’ gaze information has gained importance in recent years. 
It can be applied to many aspects, including VR content design, eye-movement based interaction, gaze-contingent rendering, etc. 
In this context, it becomes increasingly important to understand users’ gaze behaviors in virtual reality and to predict users’ gaze positions. 
This paper presents research in gaze behavior analysis and gaze position prediction in virtual reality. 
Specifically, this paper focuses on static virtual scenes and dynamic virtual scenes under free-viewing conditions. 
Users’ gaze data in virtual scenes are collected and statistical analysis is performed on the recorded data. 
The analysis reveals that users’ gaze positions are correlated with their head rotation velocities and the salient regions of the content. 
In dynamic scenes, users’ gaze positions also have strong correlations with the positions of dynamic objects. 
A data-driven eye-head coordination model is proposed for realtime gaze prediction in static scenes and a CNN-based model is derived for predicting gaze positions in dynamic scenes.
</div>
<div class="pub_links bg_grey" id="pub_links_hu20_gaze" style="display:none;">
<div class="pub_links">            
	<svg class="svg-inline--fa fa-fingerprint fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="fingerprint" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M256.12 245.96c-13.25 0-24 10.74-24 24 1.14 72.25-8.14 141.9-27.7 211.55-2.73 9.72 2.15 30.49 23.12 30.49 10.48 0 20.11-6.92 23.09-17.52 13.53-47.91 31.04-125.41 29.48-224.52.01-13.25-10.73-24-23.99-24zm-.86-81.73C194 164.16 151.25 211.3 152.1 265.32c.75 47.94-3.75 95.91-13.37 142.55-2.69 12.98 5.67 25.69 18.64 28.36 13.05 2.67 25.67-5.66 28.36-18.64 10.34-50.09 15.17-101.58 14.37-153.02-.41-25.95 19.92-52.49 54.45-52.34 31.31.47 57.15 25.34 57.62 55.47.77 48.05-2.81 96.33-10.61 143.55-2.17 13.06 6.69 25.42 19.76 27.58 19.97 3.33 26.81-15.1 27.58-19.77 8.28-50.03 12.06-101.21 11.27-152.11-.88-55.8-47.94-101.88-104.91-102.72zm-110.69-19.78c-10.3-8.34-25.37-6.8-33.76 3.48-25.62 31.5-39.39 71.28-38.75 112 .59 37.58-2.47 75.27-9.11 112.05-2.34 13.05 6.31 25.53 19.36 27.89 20.11 3.5 27.07-14.81 27.89-19.36 7.19-39.84 10.5-80.66 9.86-121.33-.47-29.88 9.2-57.88 28-80.97 8.35-10.28 6.79-25.39-3.49-33.76zm109.47-62.33c-15.41-.41-30.87 1.44-45.78 4.97-12.89 3.06-20.87 15.98-17.83 28.89 3.06 12.89 16 20.83 28.89 17.83 11.05-2.61 22.47-3.77 34-3.69 75.43 1.13 137.73 61.5 138.88 134.58.59 37.88-1.28 76.11-5.58 113.63-1.5 13.17 7.95 25.08 21.11 26.58 16.72 1.95 25.51-11.88 26.58-21.11a929.06 929.06 0 0 0 5.89-119.85c-1.56-98.75-85.07-180.33-186.16-181.83zm252.07 121.45c-2.86-12.92-15.51-21.2-28.61-18.27-12.94 2.86-21.12 15.66-18.26 28.61 4.71 21.41 4.91 37.41 4.7 61.6-.11 13.27 10.55 24.09 23.8 24.2h.2c13.17 0 23.89-10.61 24-23.8.18-22.18.4-44.11-5.83-72.34zm-40.12-90.72C417.29 43.46 337.6 1.29 252.81.02 183.02-.82 118.47 24.91 70.46 72.94 24.09 119.37-.9 181.04.14 246.65l-.12 21.47c-.39 13.25 10.03 24.31 23.28 24.69.23.02.48.02.72.02 12.92 0 23.59-10.3 23.97-23.3l.16-23.64c-.83-52.5 19.16-101.86 56.28-139 38.76-38.8 91.34-59.67 147.68-58.86 69.45 1.03 134.73 35.56 174.62 92.39 7.61 10.86 22.56 13.45 33.42 5.86 10.84-7.62 13.46-22.59 5.84-33.43z"></path></svg><p>Doi: <a class="a-text-ext" href="https://doi.org/10.1109/vrw50115.2020.00123" rel="nofollow" target="_blank">doi</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu20_gaze/pdf/hu20_gaze.pdf">paper.pdf</a></p>                
  <i class="fa fa-file-powerpoint"></i>&nbsp; Slides: <a class="pub_list" href="./hu20_gaze/ppt/hu20_gaze.pdf">slides.pdf</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu20_gaze" style="display:none;">@inproceedings{hu20_gaze, 
	author={Hu, Zhiming},
	title = {Gaze Analysis and Prediction in Virtual Reality}, 
	booktitle={Proceedings of the 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops},
	year = {2020},
	pages={543--544},
	organization={IEEE}} 
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu20_dgaze" class="margin-bottom-30">
<a href="./hu20_dgaze.html">
<img class="thumbnail" src="./hu20_dgaze/image/thumb.png" title="DGaze: CNN-Based Gaze Prediction in Dynamic Scenes" alt="DGaze: CNN-Based Gaze Prediction in Dynamic Scenes">
</a>	
<p class="pub_title">DGaze: CNN-Based Gaze Prediction in Dynamic Scenes</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>, 
Sheng Li,
<a class="a-int" href="https://cong-yi.github.io/">Congyi Zhang</a>,
Kangrui Yi, Guoping Wang,
<a class="a-int" href="https://www.cs.umd.edu/people/dmanocha">Dinesh Manocha</a>
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics (TVCG, IEEE VR 2020 Journal-track), 2020, 26(5): 1902-1911.
</span>
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu20_dgaze" class="pub_show" onclick="pub_showhide(&#39;hu20_dgaze&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu20_dgaze" class="pub_show" onclick="pub_showhide(&#39;hu20_dgaze&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu20_dgaze" class="pub_show" onclick="pub_showhide(&#39;hu20_dgaze&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu20_dgaze.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu20_dgaze" style="display:none;">
We conduct novel analyses of users' gaze behaviors in dynamic virtual scenes and, based on our analyses, we present a novel CNN-based model called DGaze for gaze prediction in HMD-based applications. 
We first collect 43 users' eye tracking data in 5 dynamic scenes under free-viewing conditions. 
Next, we perform statistical analysis of our data and observe that dynamic object positions, head rotation velocities, and salient regions are correlated with users' gaze positions. 
Based on our analysis, we present a CNN-based model (DGaze) that combines object position sequence, head velocity sequence, and saliency features to predict users' gaze positions. 
Our model can be applied to predict not only realtime gaze positions but also gaze positions in the near future and can achieve better performance than prior method. 
In terms of realtime prediction, DGaze achieves a 22.0% improvement over prior method in dynamic scenes and obtains an improvement of 9.5% in static scenes, based on using the angular distance as the evaluation metric. 
We also propose a variant of our model called DGaze_ET that can be used to predict future gaze positions with higher precision by combining accurate past gaze data gathered using an eye tracker. 
We further analyze our CNN architecture and verify the effectiveness of each component in our model. 
We apply DGaze to gaze-contingent rendering and a game, and also present the evaluation results from a user study.
</div>
<div class="pub_links bg_grey" id="pub_links_hu20_dgaze" style="display:none;">
<div class="pub_links">
  <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2020.2973473" rel="nofollow" target="_blank">doi</a></p>
            
            
  <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./hu20_dgaze/pdf/hu20_dgaze.pdf">paper.pdf</a></p>

  <i class="fa fa-link"></i><p>Code: <a class="a-text-ext" href="https://github.com/CraneHzm/DGaze" rel="nofollow" target="_blank">code</a></p>            

  <i class="fa fa-link"></i><p>Dataset: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FDGazeDataset&ga=1" rel="nofollow" target="_blank">dataset</a></p>

  <i class="fa fa-file-pdf"></i>&nbsp;Slides: <a class="pub_list" href="./hu20_dgaze/ppt/hu20_dgaze.pdf">slides.pdf</a></p>

  <i class="fa fa-link"></i><p>Experimental scenes: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FDGazeScenes&ga=1" rel="nofollow" target="_blank">experimental scenes</a></p>

  <i class="fa fa-link"></i><p>Supplementary materials: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FDGaze%5FSupplementalMaterial%2Ezip&parent=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments&ga=1" rel="nofollow" target="_blank">supplementary materials</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu20_dgaze" style="display:none;">@article{hu20_dgaze,
	title={DGaze: CNN-Based Gaze Prediction in Dynamic Scenes},
	author={Hu, Zhiming and Li, Sheng and Zhang, Congyi and Yi, Kangrui and Wang, Guoping and Manocha, Dinesh},
	journal={IEEE Transactions on Visualization and Computer Graphics},
	volume={26},
	number={5},
	pages={1902--1911},
	year={2020},
	publisher={IEEE}}
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu20_temporal" class="margin-bottom-30">
<a href="./hu20_temporal.html">
<img class="thumbnail" src="./hu20_temporal/image/thumb.png" title="Temporal continuity of visual attention for future gaze prediction in immersive virtual reality" alt="Temporal continuity of visual attention for future gaze prediction in immersive virtual reality">
</a>	
<p class="pub_title">Temporal continuity of visual attention for future gaze prediction in immersive virtual reality</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
Sheng Li, Meng Gai
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">Virtual Reality & Intelligent Hardware (VRIH), 2020, 2(2): 142-152.
</span>
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu20_temporal" class="pub_show" onclick="pub_showhide(&#39;hu20_temporal&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu20_temporal" class="pub_show" onclick="pub_showhide(&#39;hu20_temporal&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu20_temporal" class="pub_show" onclick="pub_showhide(&#39;hu20_temporal&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu20_temporal.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu20_temporal" style="display:none;">
<b>Background</b> Eye tracking technology is receiving increased attention in the field of virtual reality. 
Specifically, future gaze prediction is crucial in pre-computation for many applications such as gaze-contingent rendering, advertisement placement, and content-based design. 
To explore future gaze prediction, it is necessary to analyze the temporal continuity of visual attention in immersive virtual reality. 
<b>Methods</b> In this paper, the concept of temporal continuity of visual attention is presented. 
Subsequently, an autocorrelation function method is proposed to evaluate the temporal continuity. 
Thereafter, the temporal continuity is analyzed in both free-viewing and task-oriented conditions. 
<b>Results</b> Specifically, in free-viewing conditions, the analysis of a free-viewing gaze dataset indicates that the temporal continuity performs well only within a short time interval. 
A task-oriented game scene condition was created and conducted to collect users' gaze data. 
An analysis of the collected gaze data finds the temporal continuity has a similar performance with that of the free-viewing conditions. 
Temporal continuity can be applied to future gaze prediction and if it is good, users' current gaze positions can be directly utilized to predict their gaze positions in the future. 
<b>Conclusions</b> The current gaze's future prediction performances are further evaluated in both free-viewing and task-oriented conditions and discover that the current gaze can be efficiently applied to the task of short-term future gaze prediction. 
The task of long-term gaze prediction still remains to be explored.
</div>
<div class="pub_links bg_grey" id="pub_links_hu20_temporal" style="display:none;">
<div class="pub_links">
	<svg class="svg-inline--fa fa-fingerprint fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="fingerprint" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M256.12 245.96c-13.25 0-24 10.74-24 24 1.14 72.25-8.14 141.9-27.7 211.55-2.73 9.72 2.15 30.49 23.12 30.49 10.48 0 20.11-6.92 23.09-17.52 13.53-47.91 31.04-125.41 29.48-224.52.01-13.25-10.73-24-23.99-24zm-.86-81.73C194 164.16 151.25 211.3 152.1 265.32c.75 47.94-3.75 95.91-13.37 142.55-2.69 12.98 5.67 25.69 18.64 28.36 13.05 2.67 25.67-5.66 28.36-18.64 10.34-50.09 15.17-101.58 14.37-153.02-.41-25.95 19.92-52.49 54.45-52.34 31.31.47 57.15 25.34 57.62 55.47.77 48.05-2.81 96.33-10.61 143.55-2.17 13.06 6.69 25.42 19.76 27.58 19.97 3.33 26.81-15.1 27.58-19.77 8.28-50.03 12.06-101.21 11.27-152.11-.88-55.8-47.94-101.88-104.91-102.72zm-110.69-19.78c-10.3-8.34-25.37-6.8-33.76 3.48-25.62 31.5-39.39 71.28-38.75 112 .59 37.58-2.47 75.27-9.11 112.05-2.34 13.05 6.31 25.53 19.36 27.89 20.11 3.5 27.07-14.81 27.89-19.36 7.19-39.84 10.5-80.66 9.86-121.33-.47-29.88 9.2-57.88 28-80.97 8.35-10.28 6.79-25.39-3.49-33.76zm109.47-62.33c-15.41-.41-30.87 1.44-45.78 4.97-12.89 3.06-20.87 15.98-17.83 28.89 3.06 12.89 16 20.83 28.89 17.83 11.05-2.61 22.47-3.77 34-3.69 75.43 1.13 137.73 61.5 138.88 134.58.59 37.88-1.28 76.11-5.58 113.63-1.5 13.17 7.95 25.08 21.11 26.58 16.72 1.95 25.51-11.88 26.58-21.11a929.06 929.06 0 0 0 5.89-119.85c-1.56-98.75-85.07-180.33-186.16-181.83zm252.07 121.45c-2.86-12.92-15.51-21.2-28.61-18.27-12.94 2.86-21.12 15.66-18.26 28.61 4.71 21.41 4.91 37.41 4.7 61.6-.11 13.27 10.55 24.09 23.8 24.2h.2c13.17 0 23.89-10.61 24-23.8.18-22.18.4-44.11-5.83-72.34zm-40.12-90.72C417.29 43.46 337.6 1.29 252.81.02 183.02-.82 118.47 24.91 70.46 72.94 24.09 119.37-.9 181.04.14 246.65l-.12 21.47c-.39 13.25 10.03 24.31 23.28 24.69.23.02.48.02.72.02 12.92 0 23.59-10.3 23.97-23.3l.16-23.64c-.83-52.5 19.16-101.86 56.28-139 38.76-38.8 91.34-59.67 147.68-58.86 69.45 1.03 134.73 35.56 174.62 92.39 7.61 10.86 22.56 13.45 33.42 5.86 10.84-7.62 13.46-22.59 5.84-33.43z"></path></svg><!-- <i class="fa fa-fingerprint"></i> --><p>Doi: <a class="a-text-ext" href="https://doi.org/10.1016/j.vrih.2020.01.002" rel="nofollow" target="_blank">doi</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><!-- <i class="fa fa-file-pdf"></i> --><p>Paper: <a class="pub_list" href="./hu20_temporal/pdf/hu20_temporal.pdf">paper.pdf</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu20_temporal" style="display:none;">@article{hu20_temporal,
	title={Temporal continuity of visual attention for future gaze prediction in immersive virtual reality},
	author={Hu, Zhiming and Li, Sheng and Gai, Meng},
	journal={Virtual Reality and Intelligent Hardware},
	volume={2},
	number={2},
	pages={142--152},
	year={2020},
	publisher={Elsevier}}
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu19_sgaze" class="margin-bottom-30">
<a href="./hu19_sgaze.html">
<img class="thumbnail" src="./hu19_sgaze/image/thumb.png" title="SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction" alt="SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction">
</a>	
<p class="pub_title">SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
<a class="a-int" href="https://cong-yi.github.io/">Congyi Zhang</a>,
Sheng Li, Guoping Wang,
<a class="a-int" href="https://www.cs.umd.edu/people/dmanocha">Dinesh Manocha</a>
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics (TVCG, IEEE VR 2019 Journal-track), 2019, 25(5): 2002-2010.
</span>
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu19_sgaze" class="pub_show" onclick="pub_showhide(&#39;hu19_sgaze&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu19_sgaze" class="pub_show" onclick="pub_showhide(&#39;hu19_sgaze&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu19_sgaze" class="pub_show" onclick="pub_showhide(&#39;hu19_sgaze&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu19_sgaze.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu19_sgaze" style="display:none;">
We present a novel, data-driven eye-head coordination model that can be used for realtime gaze prediction for immersive HMD-based applications without any external hardware or eye tracker. 
Our model (SGaze) is computed by generating a large dataset that corresponds to different users navigating in virtual worlds with different lighting conditions. 
We perform statistical analysis on the recorded data and observe a linear correlation between gaze positions and head rotation angular velocities. 
We also find that there exists a latency between eye movements and head movements. 
SGaze can work as a software-based realtime gaze predictor and we formulate a time related function between head movement and eye movement and use that for realtime gaze position prediction. 
We demonstrate the benefits of SGaze for gaze-contingent rendering and evaluate the results with a user study.
</div>
<div class="pub_links bg_grey" id="pub_links_hu19_sgaze" style="display:none;">
<div class="pub_links">
  <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2019.2899187" rel="nofollow" target="_blank">doi</a></p>
            
            
  <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./hu19_sgaze/pdf/hu19_sgaze.pdf">paper.pdf</a></p>

  <i class="fa fa-database"></i>&nbsp;&nbsp;Dataset: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FSGaze%5FDataset&ga=1">dataset</a></p>

  <i class="fa fa-code"></i>&nbsp;Code: <a class="a-text-ext" href="https://github.com/CraneHzm/SGaze">code</a></p>

  <i class="fa fa-link"></i><p>Supplementary materials: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FSGaze%5FSupplementalMaterial%2Ezip&parent=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments&ga=1" rel="nofollow" target="_blank">supplementary materials</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu19_sgaze" style="display:none;">@article{hu19_sgaze,
	title={SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction},
	author={Hu, Zhiming and Zhang, Congyi and Li, Sheng and Wang, Guoping and Manocha, Dinesh},
	journal={IEEE Transactions on Visualization and Computer Graphics},
	volume={25},
	number={5},
	pages={2002--2010},
	year={2019},
	publisher={IEEE}}
</div>
</ol>


<hr>
<h3>Personal Blogs</h3>
All blogs are written in Chinese.
<ul>
<li> <a class="a-text-ext" href= "https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483828&idx=1&sn=d1ca10341d3e6f1a3141db56f252af80&chksm=ebbedf77dcc95661aefd8927569374a341301c18290f9d9c814d95720b907ce84f228b66b130&scene=132&exptype=timeline_recommend_article_extendread_samebiz#wechat_redirect" title=""> 理性与偏见</a>, 2023</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483824&idx=1&sn=5f2b32fca7e147844de53301ac197f61&chksm=ebbedf73dcc956654ac7fb23046e7731568bd92f513495110f0fe37a708fe443324e86b190b8&token=781182399&lang=zh_CN#rd" title=""> 聊一聊“卷”和“同辈压力”
</a>, 2023</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483818&idx=1&sn=8fc606f7a13ee74c3fbea2c0399d5cbd&chksm=ebbedf69dcc9567fc80a8edb0ee86509c1d3aeba6ab13f065023ac2c5078a5c4b7e80f171ce5&token=781182399&lang=zh_CN#rd" title=""> 从博士到博士后</a>, 2022</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483762&idx=1&sn=11b9a80092adfc95abb31755b6e91d4a&chksm=ebbedfb1dcc956a7a3fa8d427978280816689802f28f6e0492fdde811bf8c0663a12afa96127&token=781182399&lang=zh_CN#rd" title=""> 写在博士生涯的最后</a>, 2022</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483752&idx=1&sn=9eea860e76a73516e5ee694e909c9142&chksm=ebbedfabdcc956bd473ce16ec52f29235d62334192a583358f7c330d544038a3926512256b26&token=781182399&lang=zh_CN#rd" title=""> 学了三个专业之后</a>, 2021</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483736&idx=1&sn=a6f827edf5156e82bcb571198372772b&chksm=ebbedf9bdcc9568d8e026b752d0e1f140c5803f67f9b94b74a879c0207371e366e0840a6a599&token=781182399&lang=zh_CN#rd" title=""> Research，还真是有趣呢~</a>, 2020</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483720&idx=1&sn=267485316bb6990035348ea618b6ec26&chksm=ebbedf8bdcc9569d14cbf7ddf665e887151e25128b96a099b692f0bd4240fe9dac3d3d0b2a0c&token=781182399&lang=zh_CN#rd" title=""> 科研路上的一颗糖</a>, 2019</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483701&idx=1&sn=bf87eda79bb28dcf6680196a18e14c24&chksm=ebbedff6dcc956e0e7921946011b0a2acfc34e357889d97e5b62aa6cfbc0743e31c09c285173&token=781182399&lang=zh_CN#rd" title=""> 博士第一年：有发堪学直须学</a>, 2018</li>
</ul>

</dl>
</dl>
</dl>
</dl>
</dl>
</div>
</div>

</div>

<!-- footer -->
<div id="footer" class="footer-v1">
<div class="copyright custom-copyright">
<div class="container">
<div class="row">
<div class="col-md-6">
<p>
<span class="custom-copyright-container">
	Last modified: 02/07/2024
</span>
</p>
</div>
</div>
</div>
</div>
</div>

</body></html>