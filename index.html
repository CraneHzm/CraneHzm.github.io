<!DOCTYPE html>
<html lang="en" class="fontawesome-i2svg-active fontawesome-i2svg-complete"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">        
<title>Zhiming (Jimmy) Hu - Homepage</title>        
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" media="all" href="./index/css/main.css">
<link rel="stylesheet" media="all" href="./index/css/owl.carousel.min.css">
<link rel="stylesheet" media="all" href="./index/css/owl.theme.default.min.css">
<link rel="stylesheet" media="all" href="./index/css/jquery-ui.min.css">
<script src="./index/js/jquery-3.4.1.min.js"></script>
<script src="./index/js/jquery-ui.min.js"></script>
<script src="./index/js/fontawesome-5.11.2.js"></script>
<script src="./index/js/main.js"></script>
<script src="./index/js/owl.carousel.min.js"></script>
<script src="./index/js/rot13.js"></script>
</head>

<body class="header header-location">
<div class="wrapper">
<!-- header -->
<div id="unstickyheader" class="unstickyheader">
<div class="topbar">
<div class="container">
<div class="header-image-container">
	<!--<a href=""><img class="header-image" src="./index/" alt=""></a>-->
</div>
<div class="header-image-container-right">
	<!--a href=""><img class="header-image" src="./index/image/" alt=""></a>-->
</div>
</div>
</div>
</div>
</div>

<!-- content -->
<div class="content">
<div class="section margin-top-20 margin-bottom-20">
<div class="container">

<div class="person">
<img class="personpic" title="Zhiming Hu" src="./index/image/hu.png" alt="Zhiming Hu">

<div class="personinfo">
<h3>Zhiming (Jimmy) Hu</h3>

<p><a class="a-text-ext" href="https://eecs.pku.edu.cn/" title="School of Electronic Engineering and Computer Science">School of Electronic Engineering and Computer Science</a>, <a class="a-text-ext" href="https://www.pku.edu.cn/" target="_blank" title="Peking University" rel="nofollow">Peking University</a></p>
<p>
	<svg class="svg-inline--fa fa-envelope fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="envelope" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg><!-- <i class="fa fa-envelope"></i> --><span>jimmyhu (at) pku.edu.cn, cranehzm (at) gmail.com</span>
	<svg class="svg-inline--fa fa-phone fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="phone" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M493.4 24.6l-104-24c-11.3-2.6-22.9 3.3-27.5 13.9l-48 112c-4.2 9.8-1.4 21.3 6.9 28l60.6 49.6c-36 76.7-98.9 140.5-177.2 177.2l-49.6-60.6c-6.8-8.3-18.2-11.1-28-6.9l-112 48C3.9 366.5-2 378.1.6 389.4l24 104C27.1 504.2 36.7 512 48 512c256.1 0 464-207.5 464-464 0-11.2-7.7-20.9-18.6-23.4z"></path></svg><!-- <i class="fa fa-phone"></i> --><span>+86 010 6276 5819, <a class="a-text-ext" href="https://scholar.google.com/citations?user=OLB_xBEAAAAJ" title="Google Scholar">Google Scholar</a>, <a class="a-text-ext" href="https://github.com/cranehzm" title="Github">Github</a>, <a class="a-text-ext" href="https://mp.weixin.qq.com/s/7uPr0SxNhLA58K97osda-Q" title="WeChat Official Account">WeChat Official Account</a></span>
	<svg class="svg-inline--fa fa-map-marker-alt fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="map-marker-alt" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M172.268 501.67C26.97 291.031 0 269.413 0 192 0 85.961 85.961 0 192 0s192 85.961 192 192c0 77.413-26.97 99.031-172.268 309.67-9.535 13.774-29.93 13.773-39.464 0zM192 272c44.183 0 80-35.817 80-80s-35.817-80-80-80-80 35.817-80 80 35.817 80 80 80z"></path></svg><!-- <i class="fa fa-map-marker-alt"></i> --><span><a href="https://www.openstreetmap.org/relation/8737487" rel="nofollow" target="_blank" class="a-text-ext">No.5 YiHeYuan Road, Haidian District, Beijing 100871, China</a></span>
	<svg class="svg-inline--fa fa-door-open fa-w-20" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="door-open" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" data-fa-i2svg=""><path fill="currentColor" d="M624 448h-80V113.45C544 86.19 522.47 64 496 64H384v64h96v384h144c8.84 0 16-7.16 16-16v-32c0-8.84-7.16-16-16-16zM312.24 1.01l-192 49.74C105.99 54.44 96 67.7 96 82.92V448H16c-8.84 0-16 7.16-16 16v32c0 8.84 7.16 16 16 16h336V33.18c0-21.58-19.56-37.41-39.76-32.17zM264 288c-13.25 0-24-14.33-24-32s10.75-32 24-32 24 14.33 24 32-10.75 32-24 32z"></path></svg><!-- <i class="fa fa-door-open"></i> --><span>Peking University, Science Building No.1, Room 1322N</span>
</p>
</div>
</div>
<div class="clearfix"></div>

<hr>
<h3>Biography</h3>

<p>Zhiming Hu is a PhD student that majors in Computer Science in <b>Peking University</b> since September 2017. 
   He received his Bachelor's degree in Optical Engineering from <b>Beijing Institute of Technology</b> in 2017. 
   His research interest includes virtual reality, visual attention, human-computer interaction, and eye tracking.
   He served as a reviewer for IEEE VR, ISMAR, CAD & CG, IMWUT, UbiComp, and ICCV.
</p>
<p> <a class="a-text-ext" href="./Resume-Chinese.html" title="Resume-Chinese">Resume - Chinese</a>, <!--<a class="a-text-ext" href="./Resume-English.html" title="Resume-English">Resume-English</a>,!--><a class="a-text-ext" href="./CurriculumVitae/hu_CV.pdf" title="Curriculum Vitae">Curriculum Vitae - English</a>
</p>

<hr>
<h3>Honors, awards, and scholarships</h3>
<ul>
<li> TVCG Best Journal Nominees Award, IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2021)</li>
<li> CSC (China Scholarship Council) Scholarship, 2020</li>
<li> Second-Class Scholarship of Peking University, 2020</li>
<li> Merit Student Prize in Peking University, 2020</li>
<li> Chancellor's Scholarship, 2020</li>
<li> Leo KoGuan Scholarship, 2019</li>
<li> Hongcai Scholarship, 2019</li>
<li> Merit Student Prize in Peking University, 2019</li>
<li> Hongcai Scholarship, 2017</li>
<li> Leader Scholarship, 2017</li>
<li> Outstanding Graduates Prize in Beijing Institute of Technology, 2017</li>
<li> National Scholarship, 2016</li>
<li> Honorable Mention of Mathematical Contest In Modeling, 2016</li>
<li> Merit Student Prize in Beijing Institute of Technology, 2016</li>
<li> National Encouragement Scholarship, 2015</li>
<li> Merit Student Prize in Beijing Institute of Technology, 2015</li>
<li> Third Prize of The National College Students Composition Competition, 2015</li>
<li> Pacemaker to Merit Student Prize in Beijing Institute of Technology, 2014</li>
<li> Third Prize of Beijing Humanistic Knowledge Competition, 2014</li>
<li> National Scholarship, 2014</li>
</ul>



<hr>
<h3>Publications</h3>


<!--paper-->            
<ol class="bibliography"><li>
<div id="hu21_Eye" class="margin-bottom-30">
<a href="./EyeFixation.html">
<img class="thumbnail" src="./index/image/EyeFixation.png" title="Eye Fixation Forecasting in Task-Oriented Virtual Reality" alt="Eye Fixation Forecasting in Task-Oriented Virtual Reality">
</a>
<p class="pub_title">Eye Fixation Forecasting in Task-Oriented Virtual Reality</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">Proc. IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),
</span>
<span class="pub_additional_pages">pp. 707-708, </span>
<span class="pub_additional_year">2021</span>.
</p>
<p class="pub_tags">        
<span class="button bg_greydark"><a id="pub_abstract_sh_hu21_Eye" class="pub_show" onclick="pub_showhide(&#39;hu21_Eye&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
<span class="button bg_greydark"><a id="pub_links_sh_hu21_Eye" class="pub_show" onclick="pub_showhide(&#39;hu21_Eye&#39;,&#39;pub_links&#39;)">Links</a></span>
<span class="button bg_greydark"><a id="pub_bibtex_sh_hu21_Eye" class="pub_show" onclick="pub_showhide(&#39;hu21_Eye&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
<span class="button bg_greydark margin-left-10"><a href="./EyeFixation.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu21_Eye" style="display:none;">
In immersive virtual reality (VR), users' visual attention is crucial for many important applications, including VR content design, gaze-based interaction, and gaze-contingent rendering. 
Especially, information on users' future eye fixations is key for intelligent user interfaces and has significant relevance for many areas, such as visual attention enhancement, dynamic event triggering, and human-computer interaction. 
However, previous works typically focused on free-viewing conditions and paid less attention to task-oriented attention. 
This paper aims at forecasting users' eye fixations in task-oriented virtual reality. 
To this end, a VR eye tracking dataset that corresponds to different users performing a visual search task in immersive virtual environments is built. 
A comprehensive analysis of users' eye fixations is performed based on the collected data. 
The analysis reveals that eye fixations are correlated with users' historical gaze positions, task-related objects, saliency information of the VR content, and head rotation velocities. 
Based on this analysis, a novel learning-based model is proposed to forecast users' eye fixations in the near future in immersive virtual environments.</div>
<div class="pub_links bg_grey" id="pub_links_hu21_Eye" style="display:none;">
<div class="pub_links">            
	<svg class="svg-inline--fa fa-fingerprint fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="fingerprint" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M256.12 245.96c-13.25 0-24 10.74-24 24 1.14 72.25-8.14 141.9-27.7 211.55-2.73 9.72 2.15 30.49 23.12 30.49 10.48 0 20.11-6.92 23.09-17.52 13.53-47.91 31.04-125.41 29.48-224.52.01-13.25-10.73-24-23.99-24zm-.86-81.73C194 164.16 151.25 211.3 152.1 265.32c.75 47.94-3.75 95.91-13.37 142.55-2.69 12.98 5.67 25.69 18.64 28.36 13.05 2.67 25.67-5.66 28.36-18.64 10.34-50.09 15.17-101.58 14.37-153.02-.41-25.95 19.92-52.49 54.45-52.34 31.31.47 57.15 25.34 57.62 55.47.77 48.05-2.81 96.33-10.61 143.55-2.17 13.06 6.69 25.42 19.76 27.58 19.97 3.33 26.81-15.1 27.58-19.77 8.28-50.03 12.06-101.21 11.27-152.11-.88-55.8-47.94-101.88-104.91-102.72zm-110.69-19.78c-10.3-8.34-25.37-6.8-33.76 3.48-25.62 31.5-39.39 71.28-38.75 112 .59 37.58-2.47 75.27-9.11 112.05-2.34 13.05 6.31 25.53 19.36 27.89 20.11 3.5 27.07-14.81 27.89-19.36 7.19-39.84 10.5-80.66 9.86-121.33-.47-29.88 9.2-57.88 28-80.97 8.35-10.28 6.79-25.39-3.49-33.76zm109.47-62.33c-15.41-.41-30.87 1.44-45.78 4.97-12.89 3.06-20.87 15.98-17.83 28.89 3.06 12.89 16 20.83 28.89 17.83 11.05-2.61 22.47-3.77 34-3.69 75.43 1.13 137.73 61.5 138.88 134.58.59 37.88-1.28 76.11-5.58 113.63-1.5 13.17 7.95 25.08 21.11 26.58 16.72 1.95 25.51-11.88 26.58-21.11a929.06 929.06 0 0 0 5.89-119.85c-1.56-98.75-85.07-180.33-186.16-181.83zm252.07 121.45c-2.86-12.92-15.51-21.2-28.61-18.27-12.94 2.86-21.12 15.66-18.26 28.61 4.71 21.41 4.91 37.41 4.7 61.6-.11 13.27 10.55 24.09 23.8 24.2h.2c13.17 0 23.89-10.61 24-23.8.18-22.18.4-44.11-5.83-72.34zm-40.12-90.72C417.29 43.46 337.6 1.29 252.81.02 183.02-.82 118.47 24.91 70.46 72.94 24.09 119.37-.9 181.04.14 246.65l-.12 21.47c-.39 13.25 10.03 24.31 23.28 24.69.23.02.48.02.72.02 12.92 0 23.59-10.3 23.97-23.3l.16-23.64c-.83-52.5 19.16-101.86 56.28-139 38.76-38.8 91.34-59.67 147.68-58.86 69.45 1.03 134.73 35.56 174.62 92.39 7.61 10.86 22.56 13.45 33.42 5.86 10.84-7.62 13.46-22.59 5.84-33.43z"></path></svg><p>doi: <a class="a-text-ext" href="https://doi.org/10.1109/VRW52623.2021.00236" rel="nofollow" target="_blank">10.1109/VRW52623.2021.00236</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><!-- <i class="fa fa-file-pdf"></i> --><p>Paper: <a class="pub_list" href="./EyeFixation/pdf/hu21_Eye.pdf">hu21_Eye.pdf</a></p>                
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu21_Eye" style="display:none;">
@inproceedings{hu21_Eye,
  title = {Eye Fixation Forecasting in Task-Oriented Virtual Reality},
  author = {Hu, Zhiming},
  year = {2021},
  booktitle = {2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)},
  doi = {10.1109/VRW52623.2021.00236},
  pages = {707-708}  
}
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu21_FixationNet" class="margin-bottom-30">
<a href="./FixationNet.html">
<img class="thumbnail" src="./index/image/FixationNet.png" title="FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments" alt="FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments">
</a>
<p class="pub_title">FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
Andreas Bulling, Sheng Li, Guoping Wang
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics,
</span>
<span class="pub_additional_pages">pp. 2681-2690, </span>
<span class="pub_additional_year">2021</span>.
</p>
<p class="pub_tags">        
<span class="button bg_greydark"><a id="pub_abstract_sh_hu21_FixationNet" class="pub_show" onclick="pub_showhide(&#39;hu21_FixationNet&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
<span class="button bg_greydark"><a id="pub_links_sh_hu21_FixationNet" class="pub_show" onclick="pub_showhide(&#39;hu21_FixationNet&#39;,&#39;pub_links&#39;)">Links</a></span>
<span class="button bg_greydark"><a id="pub_bibtex_sh_hu21_FixationNet" class="pub_show" onclick="pub_showhide(&#39;hu21_FixationNet&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
<span class="button bg_greydark margin-left-10"><a href="./FixationNet.html">Project</a></span>
<span class="pub_award"><svg class="svg-inline--fa fa-award fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="award" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M97.12 362.63c-8.69-8.69-4.16-6.24-25.12-11.85-9.51-2.55-17.87-7.45-25.43-13.32L1.2 448.7c-4.39 10.77 3.81 22.47 15.43 22.03l52.69-2.01L105.56 507c8 8.44 22.04 5.81 26.43-4.96l52.05-127.62c-10.84 6.04-22.87 9.58-35.31 9.58-19.5 0-37.82-7.59-51.61-21.37zM382.8 448.7l-45.37-111.24c-7.56 5.88-15.92 10.77-25.43 13.32-21.07 5.64-16.45 3.18-25.12 11.85-13.79 13.78-32.12 21.37-51.62 21.37-12.44 0-24.47-3.55-35.31-9.58L252 502.04c4.39 10.77 18.44 13.4 26.43 4.96l36.25-38.28 52.69 2.01c11.62.44 19.82-11.27 15.43-22.03zM263 340c15.28-15.55 17.03-14.21 38.79-20.14 13.89-3.79 24.75-14.84 28.47-28.98 7.48-28.4 5.54-24.97 25.95-45.75 10.17-10.35 14.14-25.44 10.42-39.58-7.47-28.38-7.48-24.42 0-52.83 3.72-14.14-.25-29.23-10.42-39.58-20.41-20.78-18.47-17.36-25.95-45.75-3.72-14.14-14.58-25.19-28.47-28.98-27.88-7.61-24.52-5.62-44.95-26.41-10.17-10.35-25-14.4-38.89-10.61-27.87 7.6-23.98 7.61-51.9 0-13.89-3.79-28.72.25-38.89 10.61-20.41 20.78-17.05 18.8-44.94 26.41-13.89 3.79-24.75 14.84-28.47 28.98-7.47 28.39-5.54 24.97-25.95 45.75-10.17 10.35-14.15 25.44-10.42 39.58 7.47 28.36 7.48 24.4 0 52.82-3.72 14.14.25 29.23 10.42 39.59 20.41 20.78 18.47 17.35 25.95 45.75 3.72 14.14 14.58 25.19 28.47 28.98C104.6 325.96 106.27 325 121 340c13.23 13.47 33.84 15.88 49.74 5.82a39.676 39.676 0 0 1 42.53 0c15.89 10.06 36.5 7.65 49.73-5.82zM97.66 175.96c0-53.03 42.24-96.02 94.34-96.02s94.34 42.99 94.34 96.02-42.24 96.02-94.34 96.02-94.34-42.99-94.34-96.02z"></path></svg> TVCG Best Journal Nominees Award</span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu21_FixationNet" style="display:none;">
Human visual attention in immersive virtual reality (VR) is key for many important applications, such as content design, gaze-contingent rendering, or gaze-based interaction.
However, prior works typically focused on free-viewing conditions that have limited relevance for practical applications.
We first collect eye tracking data of 27 participants performing a visual search task in four immersive VR environments.
Based on this dataset, we provide a comprehensive analysis of the collected data and reveal correlations between users' eye fixations and other factors, i.e. users' historical gaze positions, task-related objects, saliency information of the VR content, and users' head rotation velocities.
Based on this analysis, we propose <i>FixationNet</i> -- a novel learning-based model to forecast users' eye fixations in the near future in VR.
We evaluate the performance of our model for free-viewing and task-oriented settings and show that it outperforms the state of the art by a large margin of 19.8% (from a mean error of 2.93&deg to 2.35&deg) in free-viewing and of 15.1% (from 2.05&deg to 1.74&deg) in task-oriented situations.
As such, our work provides new insights into task-oriented attention in virtual environments and guides future work on this important topic in VR research.</div>
<div class="pub_links bg_grey" id="pub_links_hu21_FixationNet" style="display:none;">
<div class="pub_links">            
	<svg class="svg-inline--fa fa-fingerprint fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="fingerprint" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M256.12 245.96c-13.25 0-24 10.74-24 24 1.14 72.25-8.14 141.9-27.7 211.55-2.73 9.72 2.15 30.49 23.12 30.49 10.48 0 20.11-6.92 23.09-17.52 13.53-47.91 31.04-125.41 29.48-224.52.01-13.25-10.73-24-23.99-24zm-.86-81.73C194 164.16 151.25 211.3 152.1 265.32c.75 47.94-3.75 95.91-13.37 142.55-2.69 12.98 5.67 25.69 18.64 28.36 13.05 2.67 25.67-5.66 28.36-18.64 10.34-50.09 15.17-101.58 14.37-153.02-.41-25.95 19.92-52.49 54.45-52.34 31.31.47 57.15 25.34 57.62 55.47.77 48.05-2.81 96.33-10.61 143.55-2.17 13.06 6.69 25.42 19.76 27.58 19.97 3.33 26.81-15.1 27.58-19.77 8.28-50.03 12.06-101.21 11.27-152.11-.88-55.8-47.94-101.88-104.91-102.72zm-110.69-19.78c-10.3-8.34-25.37-6.8-33.76 3.48-25.62 31.5-39.39 71.28-38.75 112 .59 37.58-2.47 75.27-9.11 112.05-2.34 13.05 6.31 25.53 19.36 27.89 20.11 3.5 27.07-14.81 27.89-19.36 7.19-39.84 10.5-80.66 9.86-121.33-.47-29.88 9.2-57.88 28-80.97 8.35-10.28 6.79-25.39-3.49-33.76zm109.47-62.33c-15.41-.41-30.87 1.44-45.78 4.97-12.89 3.06-20.87 15.98-17.83 28.89 3.06 12.89 16 20.83 28.89 17.83 11.05-2.61 22.47-3.77 34-3.69 75.43 1.13 137.73 61.5 138.88 134.58.59 37.88-1.28 76.11-5.58 113.63-1.5 13.17 7.95 25.08 21.11 26.58 16.72 1.95 25.51-11.88 26.58-21.11a929.06 929.06 0 0 0 5.89-119.85c-1.56-98.75-85.07-180.33-186.16-181.83zm252.07 121.45c-2.86-12.92-15.51-21.2-28.61-18.27-12.94 2.86-21.12 15.66-18.26 28.61 4.71 21.41 4.91 37.41 4.7 61.6-.11 13.27 10.55 24.09 23.8 24.2h.2c13.17 0 23.89-10.61 24-23.8.18-22.18.4-44.11-5.83-72.34zm-40.12-90.72C417.29 43.46 337.6 1.29 252.81.02 183.02-.82 118.47 24.91 70.46 72.94 24.09 119.37-.9 181.04.14 246.65l-.12 21.47c-.39 13.25 10.03 24.31 23.28 24.69.23.02.48.02.72.02 12.92 0 23.59-10.3 23.97-23.3l.16-23.64c-.83-52.5 19.16-101.86 56.28-139 38.76-38.8 91.34-59.67 147.68-58.86 69.45 1.03 134.73 35.56 174.62 92.39 7.61 10.86 22.56 13.45 33.42 5.86 10.84-7.62 13.46-22.59 5.84-33.43z"></path></svg><p>doi: <a class="a-text-ext" href="https://doi.org/10.1109/TVCG.2021.3067779" rel="nofollow" target="_blank">10.1109/TVCG.2021.3067779</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./FixationNet/pdf/hu21_FixationNet.pdf">hu21_FixationNet.pdf</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu21_FixationNet" style="display:none;">
@article{hu21_FixationNet,
  title = {FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments},
  author = {Hu, Zhiming and Bulling, Andreas and Li, Sheng and Wang, Guoping},
  year = {2021},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  doi = {10.1109/TVCG.2021.3067779},
  pages = {2681-2690}
}
</div>
</ol>


<!--paper-->            
<ol class="bibliography"><li>
<div id="hu20_Gaze" class="margin-bottom-30">
<a href="./GazeAnalysis.html">
<img class="thumbnail" src="./index/image/GazeAnalysis.png" title="Gaze Analysis and Prediction in Virtual Reality" alt="Gaze Analysis and Prediction in Virtual Reality">
</a>
<p class="pub_title">Gaze Analysis and Prediction in Virtual Reality</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">Proc. IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),
</span>
<span class="pub_additional_pages">pp. 543-544, </span>        
<span class="pub_additional_year">2020</span>.
</p>
<p class="pub_tags">        
<span class="button bg_greydark"><a id="pub_abstract_sh_hu20_Gaze" class="pub_show" onclick="pub_showhide(&#39;hu20_Gaze&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
<span class="button bg_greydark"><a id="pub_links_sh_hu20_Gaze" class="pub_show" onclick="pub_showhide(&#39;hu20_Gaze&#39;,&#39;pub_links&#39;)">Links</a></span>
<span class="button bg_greydark"><a id="pub_bibtex_sh_hu20_Gaze" class="pub_show" onclick="pub_showhide(&#39;hu20_Gaze&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
<span class="button bg_greydark margin-left-10"><a href="./GazeAnalysis.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu20_Gaze" style="display:none;">
In virtual reality (VR) systems, users’ gaze information has gained importance in recent years. 
It can be applied to many aspects, including VR content design, eye-movement based interaction, gaze-contingent rendering, etc. 
In this context, it becomes increasingly important to understand users’ gaze behaviors in virtual reality and to predict users’ gaze positions. 
This paper presents research in gaze behavior analysis and gaze position prediction in virtual reality. 
Specifically, this paper focuses on static virtual scenes and dynamic virtual scenes under free-viewing conditions. 
Users’ gaze data in virtual scenes are collected and statistical analysis is performed on the recorded data. 
The analysis reveals that users’ gaze positions are correlated with their head rotation velocities and the salient regions of the content. 
In dynamic scenes, users’ gaze positions also have strong correlations with the positions of dynamic objects. 
A data-driven eye-head coordination model is proposed for realtime gaze prediction in static scenes and a CNN-based model is derived for predicting gaze positions in dynamic scenes.
</div>
<div class="pub_links bg_grey" id="pub_links_hu20_Gaze" style="display:none;">
<div class="pub_links">            
	<svg class="svg-inline--fa fa-fingerprint fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="fingerprint" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M256.12 245.96c-13.25 0-24 10.74-24 24 1.14 72.25-8.14 141.9-27.7 211.55-2.73 9.72 2.15 30.49 23.12 30.49 10.48 0 20.11-6.92 23.09-17.52 13.53-47.91 31.04-125.41 29.48-224.52.01-13.25-10.73-24-23.99-24zm-.86-81.73C194 164.16 151.25 211.3 152.1 265.32c.75 47.94-3.75 95.91-13.37 142.55-2.69 12.98 5.67 25.69 18.64 28.36 13.05 2.67 25.67-5.66 28.36-18.64 10.34-50.09 15.17-101.58 14.37-153.02-.41-25.95 19.92-52.49 54.45-52.34 31.31.47 57.15 25.34 57.62 55.47.77 48.05-2.81 96.33-10.61 143.55-2.17 13.06 6.69 25.42 19.76 27.58 19.97 3.33 26.81-15.1 27.58-19.77 8.28-50.03 12.06-101.21 11.27-152.11-.88-55.8-47.94-101.88-104.91-102.72zm-110.69-19.78c-10.3-8.34-25.37-6.8-33.76 3.48-25.62 31.5-39.39 71.28-38.75 112 .59 37.58-2.47 75.27-9.11 112.05-2.34 13.05 6.31 25.53 19.36 27.89 20.11 3.5 27.07-14.81 27.89-19.36 7.19-39.84 10.5-80.66 9.86-121.33-.47-29.88 9.2-57.88 28-80.97 8.35-10.28 6.79-25.39-3.49-33.76zm109.47-62.33c-15.41-.41-30.87 1.44-45.78 4.97-12.89 3.06-20.87 15.98-17.83 28.89 3.06 12.89 16 20.83 28.89 17.83 11.05-2.61 22.47-3.77 34-3.69 75.43 1.13 137.73 61.5 138.88 134.58.59 37.88-1.28 76.11-5.58 113.63-1.5 13.17 7.95 25.08 21.11 26.58 16.72 1.95 25.51-11.88 26.58-21.11a929.06 929.06 0 0 0 5.89-119.85c-1.56-98.75-85.07-180.33-186.16-181.83zm252.07 121.45c-2.86-12.92-15.51-21.2-28.61-18.27-12.94 2.86-21.12 15.66-18.26 28.61 4.71 21.41 4.91 37.41 4.7 61.6-.11 13.27 10.55 24.09 23.8 24.2h.2c13.17 0 23.89-10.61 24-23.8.18-22.18.4-44.11-5.83-72.34zm-40.12-90.72C417.29 43.46 337.6 1.29 252.81.02 183.02-.82 118.47 24.91 70.46 72.94 24.09 119.37-.9 181.04.14 246.65l-.12 21.47c-.39 13.25 10.03 24.31 23.28 24.69.23.02.48.02.72.02 12.92 0 23.59-10.3 23.97-23.3l.16-23.64c-.83-52.5 19.16-101.86 56.28-139 38.76-38.8 91.34-59.67 147.68-58.86 69.45 1.03 134.73 35.56 174.62 92.39 7.61 10.86 22.56 13.45 33.42 5.86 10.84-7.62 13.46-22.59 5.84-33.43z"></path></svg><!-- <i class="fa fa-fingerprint"></i> --><p>doi: <a class="a-text-ext" href="https://doi.org/10.1109/vrw50115.2020.00123" rel="nofollow" target="_blank">10.1109/VRW50115.2020.00123</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><!-- <i class="fa fa-file-pdf"></i> --><p>Paper: <a class="pub_list" href="./GazeAnalysis/pdf/hu20_Gaze.pdf">hu20_Gaze.pdf</a></p>                
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu20_Gaze" style="display:none;">
@inproceedings{hu20_Gaze,
  title = {Gaze Analysis and Prediction in Virtual Reality},
  author = {Hu, Zhiming},
  year = {2020},
  booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)},
  doi = {10.1109/VRW50115.2020.00123},
  pages = {543-544}
}
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu20_DGaze" class="margin-bottom-30">
<a href="./DGaze.html">
<img class="thumbnail" src="./index/image/DGaze.png" title="DGaze: CNN-Based Gaze Prediction in Dynamic Scenes" alt="DGaze: CNN-Based Gaze Prediction in Dynamic Scenes">
</a>	
<p class="pub_title">DGaze: CNN-Based Gaze Prediction in Dynamic Scenes</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>, 
Sheng Li, Congyi Zhang, Kangrui Yi, Guoping Wang, Dinesh Manocha
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics,
</span>
<span class="pub_additional_pages">pp. 1902-1911, </span>
<span class="pub_additional_year">2020</span>.
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu20_DGaze" class="pub_show" onclick="pub_showhide(&#39;hu20_DGaze&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu20_DGaze" class="pub_show" onclick="pub_showhide(&#39;hu20_DGaze&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu20_DGaze" class="pub_show" onclick="pub_showhide(&#39;hu20_DGaze&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./DGaze.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu20_DGaze" style="display:none;">
We conduct novel analyses of users' gaze behaviors in dynamic virtual scenes and, based on our analyses, we present a novel CNN-based model called DGaze for gaze prediction in HMD-based applications. 
We first collect 43 users' eye tracking data in 5 dynamic scenes under free-viewing conditions. 
Next, we perform statistical analysis of our data and observe that dynamic object positions, head rotation velocities, and salient regions are correlated with users' gaze positions. 
Based on our analysis, we present a CNN-based model (DGaze) that combines object position sequence, head velocity sequence, and saliency features to predict users' gaze positions. 
Our model can be applied to predict not only realtime gaze positions but also gaze positions in the near future and can achieve better performance than prior method. 
In terms of realtime prediction, DGaze achieves a 22.0% improvement over prior method in dynamic scenes and obtains an improvement of 9.5% in static scenes, based on using the angular distance as the evaluation metric. 
We also propose a variant of our model called DGaze_ET that can be used to predict future gaze positions with higher precision by combining accurate past gaze data gathered using an eye tracker. 
We further analyze our CNN architecture and verify the effectiveness of each component in our model. 
We apply DGaze to gaze-contingent rendering and a game, and also present the evaluation results from a user study.
</div>
<div class="pub_links bg_grey" id="pub_links_hu20_DGaze" style="display:none;">
<div class="pub_links">
	<svg class="svg-inline--fa fa-fingerprint fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="fingerprint" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M256.12 245.96c-13.25 0-24 10.74-24 24 1.14 72.25-8.14 141.9-27.7 211.55-2.73 9.72 2.15 30.49 23.12 30.49 10.48 0 20.11-6.92 23.09-17.52 13.53-47.91 31.04-125.41 29.48-224.52.01-13.25-10.73-24-23.99-24zm-.86-81.73C194 164.16 151.25 211.3 152.1 265.32c.75 47.94-3.75 95.91-13.37 142.55-2.69 12.98 5.67 25.69 18.64 28.36 13.05 2.67 25.67-5.66 28.36-18.64 10.34-50.09 15.17-101.58 14.37-153.02-.41-25.95 19.92-52.49 54.45-52.34 31.31.47 57.15 25.34 57.62 55.47.77 48.05-2.81 96.33-10.61 143.55-2.17 13.06 6.69 25.42 19.76 27.58 19.97 3.33 26.81-15.1 27.58-19.77 8.28-50.03 12.06-101.21 11.27-152.11-.88-55.8-47.94-101.88-104.91-102.72zm-110.69-19.78c-10.3-8.34-25.37-6.8-33.76 3.48-25.62 31.5-39.39 71.28-38.75 112 .59 37.58-2.47 75.27-9.11 112.05-2.34 13.05 6.31 25.53 19.36 27.89 20.11 3.5 27.07-14.81 27.89-19.36 7.19-39.84 10.5-80.66 9.86-121.33-.47-29.88 9.2-57.88 28-80.97 8.35-10.28 6.79-25.39-3.49-33.76zm109.47-62.33c-15.41-.41-30.87 1.44-45.78 4.97-12.89 3.06-20.87 15.98-17.83 28.89 3.06 12.89 16 20.83 28.89 17.83 11.05-2.61 22.47-3.77 34-3.69 75.43 1.13 137.73 61.5 138.88 134.58.59 37.88-1.28 76.11-5.58 113.63-1.5 13.17 7.95 25.08 21.11 26.58 16.72 1.95 25.51-11.88 26.58-21.11a929.06 929.06 0 0 0 5.89-119.85c-1.56-98.75-85.07-180.33-186.16-181.83zm252.07 121.45c-2.86-12.92-15.51-21.2-28.61-18.27-12.94 2.86-21.12 15.66-18.26 28.61 4.71 21.41 4.91 37.41 4.7 61.6-.11 13.27 10.55 24.09 23.8 24.2h.2c13.17 0 23.89-10.61 24-23.8.18-22.18.4-44.11-5.83-72.34zm-40.12-90.72C417.29 43.46 337.6 1.29 252.81.02 183.02-.82 118.47 24.91 70.46 72.94 24.09 119.37-.9 181.04.14 246.65l-.12 21.47c-.39 13.25 10.03 24.31 23.28 24.69.23.02.48.02.72.02 12.92 0 23.59-10.3 23.97-23.3l.16-23.64c-.83-52.5 19.16-101.86 56.28-139 38.76-38.8 91.34-59.67 147.68-58.86 69.45 1.03 134.73 35.56 174.62 92.39 7.61 10.86 22.56 13.45 33.42 5.86 10.84-7.62 13.46-22.59 5.84-33.43z"></path></svg><!-- <i class="fa fa-fingerprint"></i> --><p>doi: <a class="a-text-ext" href="https://doi.org/10.1109/TVCG.2020.2973473" rel="nofollow" target="_blank">10.1109/TVCG.2020.2973473</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><!-- <i class="fa fa-file-pdf"></i> --><p>Paper: <a class="pub_list" href="./DGaze/pdf/hu20_DGaze.pdf">hu20_DGaze.pdf</a></p>                
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu20_DGaze" style="display:none;">
@article{hu20_DGaze,
  title = {DGaze: CNN-Based Gaze Prediction in Dynamic Scenes},
  author = {Hu, Zhiming and Li, Sheng and Zhang, Congyi and Yi, Kangrui and Wang, Guoping and Manocha, Dinesh},
  year = {2020},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  doi = {10.1109/TVCG.2020.2973473},
  pages = {1902-1911}
}
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu20_Temporal" class="margin-bottom-30">
<a href="./TemporalContinuity.html">
<img class="thumbnail" src="./index/image/TemporalContinuity.png" title="Temporal continuity of visual attention for future gaze prediction in immersive virtual reality" alt="Temporal continuity of visual attention for future gaze prediction in immersive virtual reality">
</a>	
<p class="pub_title">Temporal continuity of visual attention for future gaze prediction in immersive virtual reality</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
Sheng Li, Meng Gai
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">Virtual Reality & Intelligent Hardware,
</span>
<span class="pub_additional_pages">pp. 142-152, </span>
<span class="pub_additional_year">2020</span>.
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu20_Temporal" class="pub_show" onclick="pub_showhide(&#39;hu20_Temporal&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu20_Temporal" class="pub_show" onclick="pub_showhide(&#39;hu20_Temporal&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu20_Temporal" class="pub_show" onclick="pub_showhide(&#39;hu20_Temporal&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./TemporalContinuity.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu20_Temporal" style="display:none;">
<b>Background</b> Eye tracking technology is receiving increased attention in the field of virtual reality. 
Specifically, future gaze prediction is crucial in pre-computation for many applications such as gaze-contingent rendering, advertisement placement, and content-based design. 
To explore future gaze prediction, it is necessary to analyze the temporal continuity of visual attention in immersive virtual reality. 
<b>Methods</b> In this paper, the concept of temporal continuity of visual attention is presented. 
Subsequently, an autocorrelation function method is proposed to evaluate the temporal continuity. 
Thereafter, the temporal continuity is analyzed in both free-viewing and task-oriented conditions. 
<b>Results</b> Specifically, in free-viewing conditions, the analysis of a free-viewing gaze dataset indicates that the temporal continuity performs well only within a short time interval. 
A task-oriented game scene condition was created and conducted to collect users' gaze data. 
An analysis of the collected gaze data finds the temporal continuity has a similar performance with that of the free-viewing conditions. 
Temporal continuity can be applied to future gaze prediction and if it is good, users' current gaze positions can be directly utilized to predict their gaze positions in the future. 
<b>Conclusions</b> The current gaze's future prediction performances are further evaluated in both free-viewing and task-oriented conditions and discover that the current gaze can be efficiently applied to the task of short-term future gaze prediction. 
The task of long-term gaze prediction still remains to be explored.
</div>
<div class="pub_links bg_grey" id="pub_links_hu20_Temporal" style="display:none;">
<div class="pub_links">
	<svg class="svg-inline--fa fa-fingerprint fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="fingerprint" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M256.12 245.96c-13.25 0-24 10.74-24 24 1.14 72.25-8.14 141.9-27.7 211.55-2.73 9.72 2.15 30.49 23.12 30.49 10.48 0 20.11-6.92 23.09-17.52 13.53-47.91 31.04-125.41 29.48-224.52.01-13.25-10.73-24-23.99-24zm-.86-81.73C194 164.16 151.25 211.3 152.1 265.32c.75 47.94-3.75 95.91-13.37 142.55-2.69 12.98 5.67 25.69 18.64 28.36 13.05 2.67 25.67-5.66 28.36-18.64 10.34-50.09 15.17-101.58 14.37-153.02-.41-25.95 19.92-52.49 54.45-52.34 31.31.47 57.15 25.34 57.62 55.47.77 48.05-2.81 96.33-10.61 143.55-2.17 13.06 6.69 25.42 19.76 27.58 19.97 3.33 26.81-15.1 27.58-19.77 8.28-50.03 12.06-101.21 11.27-152.11-.88-55.8-47.94-101.88-104.91-102.72zm-110.69-19.78c-10.3-8.34-25.37-6.8-33.76 3.48-25.62 31.5-39.39 71.28-38.75 112 .59 37.58-2.47 75.27-9.11 112.05-2.34 13.05 6.31 25.53 19.36 27.89 20.11 3.5 27.07-14.81 27.89-19.36 7.19-39.84 10.5-80.66 9.86-121.33-.47-29.88 9.2-57.88 28-80.97 8.35-10.28 6.79-25.39-3.49-33.76zm109.47-62.33c-15.41-.41-30.87 1.44-45.78 4.97-12.89 3.06-20.87 15.98-17.83 28.89 3.06 12.89 16 20.83 28.89 17.83 11.05-2.61 22.47-3.77 34-3.69 75.43 1.13 137.73 61.5 138.88 134.58.59 37.88-1.28 76.11-5.58 113.63-1.5 13.17 7.95 25.08 21.11 26.58 16.72 1.95 25.51-11.88 26.58-21.11a929.06 929.06 0 0 0 5.89-119.85c-1.56-98.75-85.07-180.33-186.16-181.83zm252.07 121.45c-2.86-12.92-15.51-21.2-28.61-18.27-12.94 2.86-21.12 15.66-18.26 28.61 4.71 21.41 4.91 37.41 4.7 61.6-.11 13.27 10.55 24.09 23.8 24.2h.2c13.17 0 23.89-10.61 24-23.8.18-22.18.4-44.11-5.83-72.34zm-40.12-90.72C417.29 43.46 337.6 1.29 252.81.02 183.02-.82 118.47 24.91 70.46 72.94 24.09 119.37-.9 181.04.14 246.65l-.12 21.47c-.39 13.25 10.03 24.31 23.28 24.69.23.02.48.02.72.02 12.92 0 23.59-10.3 23.97-23.3l.16-23.64c-.83-52.5 19.16-101.86 56.28-139 38.76-38.8 91.34-59.67 147.68-58.86 69.45 1.03 134.73 35.56 174.62 92.39 7.61 10.86 22.56 13.45 33.42 5.86 10.84-7.62 13.46-22.59 5.84-33.43z"></path></svg><!-- <i class="fa fa-fingerprint"></i> --><p>doi: <a class="a-text-ext" href="https://doi.org/10.1016/j.vrih.2020.01.002" rel="nofollow" target="_blank">10.1016/j.vrih.2020.01.002</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><!-- <i class="fa fa-file-pdf"></i> --><p>Paper: <a class="pub_list" href="./VRIH/pdf/hu20_Temporal.pdf">hu20_Temporal.pdf</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu20_Temporal" style="display:none;">
@article{hu20_Temporal,
  title = {Temporal continuity of visual attention for future gaze prediction in immersive virtual reality},
  author = {Hu, Zhiming and Li, Sheng and Gai, Meng},
  year = {2020},
  journal = {Virtual Reality & Intelligent Hardware},
  doi = {10.1016/j.vrih.2020.01.002},
  pages = {142-152}
}
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu19_SGaze" class="margin-bottom-30">
<a href="./SGaze.html">
<img class="thumbnail" src="./index/image/SGaze.png" title="SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction" alt="SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction">
</a>	
<p class="pub_title">SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
Congyi Zhang, Sheng Li, Guoping Wang, Dinesh Manocha
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics,
</span>
<span class="pub_additional_pages">pp. 2002-2010, </span>
<span class="pub_additional_year">2019</span>.
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu19_SGaze" class="pub_show" onclick="pub_showhide(&#39;hu19_SGaze&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu19_SGaze" class="pub_show" onclick="pub_showhide(&#39;hu19_SGaze&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu19_SGaze" class="pub_show" onclick="pub_showhide(&#39;hu19_SGaze&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./SGaze.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu19_SGaze" style="display:none;">
We present a novel, data-driven eye-head coordination model that can be used for realtime gaze prediction for immersive HMD-based applications without any external hardware or eye tracker. 
Our model (SGaze) is computed by generating a large dataset that corresponds to different users navigating in virtual worlds with different lighting conditions. 
We perform statistical analysis on the recorded data and observe a linear correlation between gaze positions and head rotation angular velocities. 
We also find that there exists a latency between eye movements and head movements. 
SGaze can work as a software-based realtime gaze predictor and we formulate a time related function between head movement and eye movement and use that for realtime gaze position prediction. 
We demonstrate the benefits of SGaze for gaze-contingent rendering and evaluate the results with a user study.
</div>
<div class="pub_links bg_grey" id="pub_links_hu19_SGaze" style="display:none;">
<div class="pub_links">
	<svg class="svg-inline--fa fa-fingerprint fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="fingerprint" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M256.12 245.96c-13.25 0-24 10.74-24 24 1.14 72.25-8.14 141.9-27.7 211.55-2.73 9.72 2.15 30.49 23.12 30.49 10.48 0 20.11-6.92 23.09-17.52 13.53-47.91 31.04-125.41 29.48-224.52.01-13.25-10.73-24-23.99-24zm-.86-81.73C194 164.16 151.25 211.3 152.1 265.32c.75 47.94-3.75 95.91-13.37 142.55-2.69 12.98 5.67 25.69 18.64 28.36 13.05 2.67 25.67-5.66 28.36-18.64 10.34-50.09 15.17-101.58 14.37-153.02-.41-25.95 19.92-52.49 54.45-52.34 31.31.47 57.15 25.34 57.62 55.47.77 48.05-2.81 96.33-10.61 143.55-2.17 13.06 6.69 25.42 19.76 27.58 19.97 3.33 26.81-15.1 27.58-19.77 8.28-50.03 12.06-101.21 11.27-152.11-.88-55.8-47.94-101.88-104.91-102.72zm-110.69-19.78c-10.3-8.34-25.37-6.8-33.76 3.48-25.62 31.5-39.39 71.28-38.75 112 .59 37.58-2.47 75.27-9.11 112.05-2.34 13.05 6.31 25.53 19.36 27.89 20.11 3.5 27.07-14.81 27.89-19.36 7.19-39.84 10.5-80.66 9.86-121.33-.47-29.88 9.2-57.88 28-80.97 8.35-10.28 6.79-25.39-3.49-33.76zm109.47-62.33c-15.41-.41-30.87 1.44-45.78 4.97-12.89 3.06-20.87 15.98-17.83 28.89 3.06 12.89 16 20.83 28.89 17.83 11.05-2.61 22.47-3.77 34-3.69 75.43 1.13 137.73 61.5 138.88 134.58.59 37.88-1.28 76.11-5.58 113.63-1.5 13.17 7.95 25.08 21.11 26.58 16.72 1.95 25.51-11.88 26.58-21.11a929.06 929.06 0 0 0 5.89-119.85c-1.56-98.75-85.07-180.33-186.16-181.83zm252.07 121.45c-2.86-12.92-15.51-21.2-28.61-18.27-12.94 2.86-21.12 15.66-18.26 28.61 4.71 21.41 4.91 37.41 4.7 61.6-.11 13.27 10.55 24.09 23.8 24.2h.2c13.17 0 23.89-10.61 24-23.8.18-22.18.4-44.11-5.83-72.34zm-40.12-90.72C417.29 43.46 337.6 1.29 252.81.02 183.02-.82 118.47 24.91 70.46 72.94 24.09 119.37-.9 181.04.14 246.65l-.12 21.47c-.39 13.25 10.03 24.31 23.28 24.69.23.02.48.02.72.02 12.92 0 23.59-10.3 23.97-23.3l.16-23.64c-.83-52.5 19.16-101.86 56.28-139 38.76-38.8 91.34-59.67 147.68-58.86 69.45 1.03 134.73 35.56 174.62 92.39 7.61 10.86 22.56 13.45 33.42 5.86 10.84-7.62 13.46-22.59 5.84-33.43z"></path></svg><!-- <i class="fa fa-fingerprint"></i> --><p>doi: <a class="a-text-ext" href="https://doi.org/10.1109/TVCG.2019.2899187" rel="nofollow" target="_blank">10.1109/TVCG.2019.2899187</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><!-- <i class="fa fa-file-pdf"></i> --><p>Paper: <a class="pub_list" href="./SGaze/pdf/hu19_SGaze.pdf">hu19_SGaze.pdf</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu19_SGaze" style="display:none;">
@article{hu19_SGaze,
  title = {SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction},
  author = {Hu, Zhiming and Zhang, Congyi and Li, Sheng and Wang, Guoping and Manocha, Dinesh},
  year = {2019},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  doi = {10.1109/TVCG.2019.2899187},
  pages = {2002-2010}
}
</div>
</ol>



</dl>
</dl>
</dl>
</dl>
</dl>
</div>
</div>

</div>

<!-- footer -->
<div id="footer" class="footer-v1">
<div class="copyright custom-copyright">
<div class="container">
<div class="row">
<div class="col-md-6">
<p>
<span class="custom-copyright-container">
	Last modified: 2021/04/19
</span>
</p>
</div>
</div>
</div>
</div>
</div>

</body></html>