<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="Eye Fixation Forecasting in Task-Oriented Virtual Reality">
<meta name="author" content="Zhiming Hu">
<title>Eye Fixation Forecasting in Task-Oriented Virtual Reality</title>
<!-- Bootstrap core CSS -->
<link href="./EyeFixation/css/bootstrap.min.css" rel="stylesheet">
<!-- Custom styles for this template -->
<link href="./EyeFixation/css/offcanvas.css" rel="stylesheet">
</head>
	
	
<body>
<div class="container">
<div class="jumbotron">
<h2>Eye Fixation Forecasting in Task-Oriented Virtual Reality</h2>
<p class="abstract">Eye Fixation Analysis and Forecasting in Task-Oriented Virtual Reality</p>
<p iclass="authors"><a href="https://cranehzm.github.io/">Zhiming Hu</a> </p>
<p><a class="btn btn-primary" href="./EyeFixation/pdf/hu21_Eye.pdf">PDF</a> <a class="btn btn-primary" href="./EyeFixation/ppt/hu21_Eye.pdf">PPT</a> </p> 
</div>    


<hr>
<div>
<h3>Abstract</h3>
<p>
In immersive virtual reality (VR), users' visual attention is crucial for many important applications, including VR content design, gaze-based interaction, and gaze-contingent rendering. 
Especially, information on users' future eye fixations is key for intelligent user interfaces and has significant relevance for many areas, such as visual attention enhancement, dynamic event triggering, and human-computer interaction. 
However, previous works typically focused on free-viewing conditions and paid less attention to task-oriented attention. 
This paper aims at forecasting users' eye fixations in task-oriented virtual reality. 
To this end, a VR eye tracking dataset that corresponds to different users performing a visual search task in immersive virtual environments is built. 
A comprehensive analysis of users' eye fixations is performed based on the collected data. 
The analysis reveals that eye fixations are correlated with users' historical gaze positions, task-related objects, saliency information of the VR content, and head rotation velocities. 
Based on this analysis, a novel learning-based model is proposed to forecast users' eye fixations in the near future in immersive virtual environments. </p>
</div>
	
	
<div class="section">
<h3>Presentation Video</h3>
<object type='application/x-shockwave-flash' style='width:1024px; height:608px;' data='https://www.youtube.com/v/esvgn1Hl2BI'>
<param name='movie' value='https://www.youtube.com/v/esvgn1Hl2BI' />
</object>
</div>

		
<div class="section">
<h3>Related Work</h3>
<hr>
<p>Our related work:</p>
<p><a href="https://cranehzm.github.io/EHTask.html">EHTask: Recognizing User Tasks from Eye and Head Movements in Immersive Virtual Reality</a></p>
<p><a href="https://cranehzm.github.io/UserTask.html">Research progress of user task prediction and algorithm analysis (in Chinese)</a></p>
<p><a href="https://cranehzm.github.io/FixationNet.html">FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments</a></p>
<p><a href="https://cranehzm.github.io/GazeAnalysis.html">Gaze Analysis and Prediction in Virtual Reality</a></p>
<p><a href="https://cranehzm.github.io/DGaze.html">DGaze: CNN-Based Gaze Prediction in Dynamic Scenes</a></p>
<p><a href="https://cranehzm.github.io/TemporalContinuity.html">Temporal Continuity of Visual Attention for Future Gaze Prediction in Immersive Virtual Reality</a></p>    
<p><a href="https://cranehzm.github.io/SGaze.html">SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction</a></p>
</div>

	
<h3>Bibtex</h3>
<hr>
<div class="bibtexsection">
@inproceedings{hu21_Eye,
  title = {Eye Fixation Forecasting in Task-Oriented Virtual Reality},
  author = {Hu, Zhiming},
  year = {2021},
  booktitle = {2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)},
  doi = {10.1109/VRW52623.2021.00236},
  pages = {707-708}
}
</div>
		
    
<hr>
<footer>
<p>Send feedback and questions to <a href="https://cranehzm.github.io/">Zhiming Hu</a>.</p>
<p>Thanks to Vincent Sitzmann for his website template. Â© 2017</p>
</footer>


</div><!--/.container-->
</body></html>