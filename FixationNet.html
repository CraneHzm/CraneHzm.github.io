<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments">
<meta name="author" content="Zhiming Hu">
<title>FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments</title>
<!-- Bootstrap core CSS -->
<link href="./FixationNet/css/bootstrap.min.css" rel="stylesheet">
<!-- Custom styles for this template -->
<link href="./FixationNet/css/offcanvas.css" rel="stylesheet">
</head>


<body><div class="container"><div class="jumbotron">
<h2>FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments</h2>
<p class="abstract">Human Eye Fixation Analysis and Forecasting in Task-Oriented Virtual Environments</p>
<p iclass="authors"><a href="https://cranehzm.github.io/">Zhiming Hu</a>, Andreas Bulling, Sheng Li, Guoping Wang</p>
<!--<p><a class="btn btn-primary" href="https://pkueducn-my.sharepoint.com/:f:/g/personal/jimmyhu_pku_edu_cn/EkycED96pMBEuMOyoDTNDYsB58PIX8gms3fRyao1v4CKxA?e=MTx9YN">Dataset</a> <a class="btn btn-primary" href="./DGaze/pdf/hu20_DGaze.pdf">PDF</a> <a class="btn btn-primary" href="./DGaze/ppt/ppt.pdf">PPT</a> <a class="btn btn-primary" href="https://github.com/CraneHzm/DGaze">Code</a> <a class="btn btn-primary" href="https://pkueducn-my.sharepoint.com/:u:/g/personal/jimmyhu_pku_edu_cn/EZ6HlNw0lVJFkEu753DaPk8BDf4uE_nVKyyJJAeYUcKJ1w?e=S7sg5n">Supplemental Material</a></p>-->
</div>

<!--teaser image-->
<img src="./FixationNet/image/teaser.png" style="width:100%; margin-right:-20px; margin-top:-10px;">

<hr>
<div><h3>Abstract</h3><p>
Human visual attention in immersive virtual reality (VR) is key for many important applications, such as content design, gaze-contingent rendering, or gaze-based interaction.
However, prior works typically focused on free-viewing conditions that have limited relevance for practical applications.
We first collect eye tracking data of 27 participants performing a visual search task in four immersive VR environments.
Based on this dataset, we provide a comprehensive analysis of the collected data and reveal correlations between users' eye fixations and other factors, i.e. users' historical gaze positions, task-related objects, saliency information of the VR content, and users' head rotation velocities.
Based on this analysis, we propose <i>FixationNet</i> -- a novel learning-based model to forecast users' eye fixations in the near future in VR.
We evaluate the performance of our model for free-viewing and task-oriented settings and show that it outperforms the state of the art by a large margin of 19.8% (from a mean error of 2.93&deg to 2.35&deg) in free-viewing and of 15.1% (from 2.05&deg to 1.74&deg) in task-oriented situations.
As such, our work provides new insights into task-oriented attention in virtual environments and guides future work on this important topic in VR research.</p>
</div>


<!--<div class="section">
<h3>Paper Video</h3>
<object type='application/x-shockwave-flash' style='width:1024px; height:608px;' data='https://www.youtube.com/v/Ul1TZBgXXik'>
<param name='movie' value='https://www.youtube.com/v/Ul1TZBgXXik'/>
</object></div>-->


<!--<div class="section">
<h3>Presentation Video</h3>
<object type='application/x-shockwave-flash' style='width:1024px; height:608px;' data='https://www.youtube.com/v/RQKNGtL_89M'>
<param name='movie' value='https://www.youtube.com/v/RQKNGtL_89M' />
</object>
</div>-->


<div class="section">
<h3>Related work</h3>
<hr>
<p>Our related work on gaze analysis and prediction in virtual reality:</p>
<p><a href="https://cranehzm.github.io/DC.html">Gaze Analysis and Prediction in Virtual Reality</a></p>
<p><a href="https://cranehzm.github.io/DGaze.html">DGaze: CNN-Based Gaze Prediction in Dynamic Scenes</a></p>
<p><a href="https://cranehzm.github.io/VRIH.html">Temporal Continuity of Visual Attention for Future Gaze Prediction in Immersive Virtual Reality</a></p>    
<p><a href="https://cranehzm.github.io/SGaze.html">SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction</a></p>
</div>


<h3>Bibtex</h3>
<hr>
<div class="bibtexsection">
@article{hu21_FixationNet,
  title = {FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments},
  author = {Hu, Zhiming and Bulling, Andreas and Li, Sheng and Wang, Guoping},
  year = {2021},
  journal={IEEE Transactions on Visualization and Computer Graphics}
}
</div>


<hr>
<footer>
<p>Send feedback and questions to <a href="https://cranehzm.github.io/">Zhiming Hu</a>.</p>
<p>Thanks to Vincent Sitzmann for his website template. Â© 2017</p>
</footer>


</div><!--/.container-->
</body></html>