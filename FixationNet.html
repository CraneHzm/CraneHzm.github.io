<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments">
<meta name="author" content="Zhiming Hu">
<title>FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments</title>
<!-- Bootstrap core CSS -->
<link href="./FixationNet/css/bootstrap.min.css" rel="stylesheet">
<!-- Custom styles for this template -->
<link href="./FixationNet/css/offcanvas.css" rel="stylesheet">
</head>


<body><div class="container"><div class="jumbotron">
<h2>FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments</h2>
<p style="color:#F0AD4E">TVCG Best Journal Nominees Award</p>
<p class="abstract">Human Eye Fixation Analysis and Forecasting in Task-Oriented Virtual Environments</p>
<p iclass="authors"><a href="https://cranehzm.github.io/">Zhiming Hu</a>, Andreas Bulling, Sheng Li, Guoping Wang</p>
<p><a class="btn btn-primary" href="https://pkueducn-my.sharepoint.com/:f:/g/personal/jimmyhu_pku_edu_cn/EvK78OysBK9GgD1Xlr4Hx9EBZ58JTMpL3j-pPi28_-Ok-Q?e=3Vvqb0">Dataset</a> 
   <a class="btn btn-primary" href="./FixationNet/pdf/hu21_FixationNet.pdf">PDF</a> 
   <a class="btn btn-primary" href="./FixationNet/ppt/hu21_FixationNet.pdf">PPT</a> 
   <a class="btn btn-primary" href="https://github.com/CraneHzm/FixationNet">Code</a> 
   <a class="btn btn-primary" href="https://pkueducn-my.sharepoint.com/:f:/g/personal/jimmyhu_pku_edu_cn/EjqQDzC2-7lIm69dtW69pvQB-1ZipBZ2zlm6ezMcwwkR3w?e=lhafEx">Experimental Scenes</a>
   <a class="btn btn-primary" href="https://pkueducn-my.sharepoint.com/:u:/g/personal/jimmyhu_pku_edu_cn/ES-d5SJ3PSNJo3G86evVHpgBCgyNwe20RMcogS2B2XAggg?e=vxdNEe">Supplemental Material</a></p>
</div>

<!--teaser image-->
<img src="./FixationNet/image/teaser.png" style="width:100%; margin-right:-20px; margin-top:-10px;">

<hr>
<div><h3>Abstract</h3><p>
Human visual attention in immersive virtual reality (VR) is key for many important applications, such as content design, gaze-contingent rendering, or gaze-based interaction.
However, prior works typically focused on free-viewing conditions that have limited relevance for practical applications.
We first collect eye tracking data of 27 participants performing a visual search task in four immersive VR environments.
Based on this dataset, we provide a comprehensive analysis of the collected data and reveal correlations between users' eye fixations and other factors, i.e. users' historical gaze positions, task-related objects, saliency information of the VR content, and users' head rotation velocities.
Based on this analysis, we propose <i>FixationNet</i> -- a novel learning-based model to forecast users' eye fixations in the near future in VR.
We evaluate the performance of our model for free-viewing and task-oriented settings and show that it outperforms the state of the art by a large margin of 19.8% (from a mean error of 2.93&deg to 2.35&deg) in free-viewing and of 15.1% (from 2.05&deg to 1.74&deg) in task-oriented situations.
As such, our work provides new insights into task-oriented attention in virtual environments and guides future work on this important topic in VR research.</p>
</div>


<div class="section">
<h3>Paper Video</h3>
<object type='application/x-shockwave-flash' style='width:1024px; height:608px;' data='https://www.youtube.com/v/ephYjXvUyGo'>
<param name='movie' value='https://www.youtube.com/v/ephYjXvUyGo'/>
</object></div>


<div class="section">
<h3>Presentation Video</h3>
<object type='application/x-shockwave-flash' style='width:1024px; height:608px;' data='https://www.youtube.com/v/kQGqXLJwM54'>
<param name='movie' value='https://www.youtube.com/v/kQGqXLJwM54' />
</object>
</div>


<div class="section">
<h3>Related Work</h3>
<hr>
<p>Our related work:</p>
<p><a href="https://cranehzm.github.io/EHTask.html">EHTask: Recognizing User Tasks from Eye and Head Movements in Immersive Virtual Reality</a></p>
<p><a href="https://cranehzm.github.io/UserTask.html">Research progress of user task prediction and algorithm analysis (in Chinese)</a></p>
<p><a href="https://cranehzm.github.io/EyeFixation.html">Eye Fixation Forecasting in Task-Oriented Virtual Reality</a></p>
<p><a href="https://cranehzm.github.io/GazeAnalysis.html">Gaze Analysis and Prediction in Virtual Reality</a></p>
<p><a href="https://cranehzm.github.io/DGaze.html">DGaze: CNN-Based Gaze Prediction in Dynamic Scenes</a></p>
<p><a href="https://cranehzm.github.io/TemporalContinuity.html">Temporal Continuity of Visual Attention for Future Gaze Prediction in Immersive Virtual Reality</a></p>    
<p><a href="https://cranehzm.github.io/SGaze.html">SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction</a></p>
</div>


<h3>Bibtex</h3>
<hr>
<div class="bibtexsection">
@article{hu21_FixationNet,
  title = {FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments},
  author = {Hu, Zhiming and Bulling, Andreas and Li, Sheng and Wang, Guoping},
  year = {2021},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  doi = {10.1109/TVCG.2021.3067779},
  pages = {2681-2690}
}
</div>


<hr>
<footer>
<p>Send feedback and questions to <a href="https://cranehzm.github.io/">Zhiming Hu</a>.</p>
<p>Thanks to Vincent Sitzmann for his website template. Â© 2017</p>
</footer>


</div><!--/.container-->
</body></html>