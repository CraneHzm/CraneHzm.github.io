<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="DGaze: CNN-Based Gaze Prediction in Dynamic Scenes">
<meta name="author" content="Zhiming Hu">
<title>DGaze: CNN-Based Gaze Prediction in Dynamic Scenes</title>
<!-- Bootstrap core CSS -->
<link href="./DGaze/css/bootstrap.min.css" rel="stylesheet">
<!-- Custom styles for this template -->
<link href="./DGaze/css/offcanvas.css" rel="stylesheet">
</head>

	
<body>
<div class="container">
<div class="jumbotron">
<h2>DGaze: CNN-Based Gaze Prediction in Dynamic Scenes</h2>
<p class="abstract">Gaze Analysis and Prediction in Dynamic Virtual Scenes</p>
<p iclass="authors"><a href="https://cranehzm.github.io/">Zhiming Hu</a>, Sheng Li, Congyi Zhang, Kangrui Yi, Guoping Wang, and Dinesh Manocha </p>
<p><a class="btn btn-primary" href="https://chinapku-my.sharepoint.com/:f:/g/personal/1701111311_pku_edu_cn/Eotz_w-ZvT1FiSXY7SJCoCkBtdmeEwZG2APqqARB4NGSNw?e=aMF3Gp">Dataset</a> 
   <a class="btn btn-primary" href="./DGaze/pdf/hu20_DGaze.pdf">PDF</a> 
   <a class="btn btn-primary" href="./DGaze/ppt/hu20_DGaze.pdf">PPT</a> 
   <a class="btn btn-primary" href="https://github.com/CraneHzm/DGaze">Code</a> 
   <a class="btn btn-primary" href="https://chinapku-my.sharepoint.com/:f:/g/personal/1701111311_pku_edu_cn/EjntedyAOqdOj4Wt5KDCOb8BdZG6qNll7DPAq8WswkGJCQ?e=dgHwGm">Experimental Scenes</a>
   <a class="btn btn-primary" href="https://chinapku-my.sharepoint.com/:u:/g/personal/1701111311_pku_edu_cn/EXHt844HL9JJplZxxosnYM4BKuM24mNQj4YrWROd6N6lOQ?e=mbFU3T">Supplemental Material</a> </p> 
</div>    


<!--teaser image-->	
<img src="./DGaze/image/teaser.png" style="width:100%; margin-right:-20px; margin-top:-10px;">


<hr>
<div>
<h3>Abstract</h3>
<p>
We conduct novel analyses of users' gaze behaviors in dynamic virtual scenes and, based on our analyses, we present a novel CNN-based model called DGaze for gaze prediction in HMD-based applications. 
We first collect 43 users' eye tracking data in 5 dynamic scenes under free-viewing conditions. 
Next, we perform statistical analysis of our data and observe that dynamic object positions, head rotation velocities, and salient regions are correlated with users' gaze positions. 
Based on our analysis, we present a CNN-based model (DGaze) that combines object position sequence, head velocity sequence, and saliency features to predict users' gaze positions. 
Our model can be applied to predict not only realtime gaze positions but also gaze positions in the near future and can achieve better performance than prior method. 
In terms of realtime prediction, DGaze achieves a 22.0% improvement over prior method in dynamic scenes and obtains an improvement of 9.5% in static scenes, based on using the angular distance as the evaluation metric. 
We also propose a variant of our model called DGaze_ET that can be used to predict future gaze positions with higher precision by combining accurate past gaze data gathered using an eye tracker. 
We further analyze our CNN architecture and verify the effectiveness of each component in our model. 
We apply DGaze to gaze-contingent rendering and a game, and also present the evaluation results from a user study.
</p>
</div>
	
	
<div class="section">
<h3>Paper Video</h3>
<object type='application/x-shockwave-flash' style='width:1024px; height:608px;' data='https://www.youtube.com/v/Ul1TZBgXXik'>
<param name='movie' value='https://www.youtube.com/v/Ul1TZBgXXik' />
</object>
</div>


<div class="section">
<h3>Presentation Video</h3>
<object type='application/x-shockwave-flash' style='width:1024px; height:608px;' data='https://www.youtube.com/v/RQKNGtL_89M'>
<param name='movie' value='https://www.youtube.com/v/RQKNGtL_89M' />
</object>
</div>	

		
<div class="section">
<h3>Related Work</h3>
<hr>
<p>Our related work:</p>
<p><a href="https://cranehzm.github.io/EHTask.html">EHTask: Recognizing User Tasks from Eye and Head Movements in Immersive Virtual Reality</a></p>
<p><a href="https://cranehzm.github.io/UserTask.html">Research progress of user task prediction and algorithm analysis (in Chinese)</a></p>
<p><a href="https://cranehzm.github.io/EyeFixation.html">Eye Fixation Forecasting in Task-Oriented Virtual Reality</a></p>
<p><a href="https://cranehzm.github.io/FixationNet.html">FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments</a></p>
<p><a href="https://cranehzm.github.io/GazeAnalysis.html">Gaze Analysis and Prediction in Virtual Reality</a></p>
<p><a href="https://cranehzm.github.io/TemporalContinuity.html">Temporal Continuity of Visual Attention for Future Gaze Prediction in Immersive Virtual Reality</a></p>    
<p><a href="https://cranehzm.github.io/SGaze.html">SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction</a></p>
</div>

	
<h3>Bibtex</h3>
<hr>
<div class="bibtexsection">
@article{hu20_DGaze,
  title={DGaze: CNN-Based Gaze Prediction in Dynamic Scenes},
  author={Hu, Zhiming and Li, Sheng and Zhang, Congyi and Yi, Kangrui and Wang, Guoping and Manocha, Dinesh},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  volume={26},
  number={5},
  pages={1902--1911},
  year={2020},
  publisher={IEEE}
}
</div>
		
    
<hr>
<footer>
<p>Send feedback and questions to <a href="https://cranehzm.github.io/">Zhiming Hu</a>.</p>
<p>Thanks to Vincent Sitzmann for his website template. Â© 2017</p>
</footer>


</div><!--/.container-->
</body></html>